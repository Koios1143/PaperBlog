const nt="ENTRIES",V="KEYS",T="VALUES",F="";class D{set;_type;_path;constructor(t,s){const n=t._tree,u=Array.from(n.keys());this.set=t,this._type=s,this._path=u.length>0?[{node:n,keys:u}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===F)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==F).join("")}value(){return E(this._path).node.get(F)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ut=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const u=t.length+1,o=u+s,i=new Uint8Array(o*u).fill(s+1);for(let r=0;r<u;++r)i[r]=r;for(let r=1;r<o;++r)i[r*u]=r;return R(e,t,s,n,i,1,u,""),n},R=(e,t,s,n,u,o,i,r)=>{const d=o*i;t:for(const l of e.keys())if(l===F){const a=u[d-1];a<=s&&n.set(r,[e.get(l),a])}else{let a=o;for(let h=0;h<l.length;++h,++a){const m=l[h],p=i*a,f=p-i;let c=u[p];const g=Math.max(0,a-s-1),_=Math.min(i-1,a+s);for(let y=g;y<_;++y){const b=m!==t[y],z=u[f+y]+ +b,A=u[f+y+1]+1,w=u[p+y]+1,L=u[p+y+1]=Math.min(z,A,w);L<c&&(c=L)}if(c>s)continue t}R(e.get(l),t,s,n,u,a,i,r+l)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[u,o]=M(n);for(const i of u.keys())if(i!==F&&i.startsWith(o)){const r=new Map;return r.set(i.slice(o.length),u.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ut(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(F):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(F)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(F,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(F,s(n.get(F))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let u=n.get(F);return u===void 0&&n.set(F,u=s()),u}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,u]of t)s.set(n,u);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==F&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==F&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const o of e.keys())if(o!==F&&t[n]===o[0]){const i=Math.min(s-n,o.length);let r=1;for(;r<i&&t[n+r]===o[r];)++r;const d=e.get(o);if(r===o.length)e=d;else{const l=new Map;l.set(o.slice(r),d),e.set(t.slice(n,n+r),l),e.delete(o),e=l}n+=r;continue t}const u=new Map;return e.set(t.slice(n),u),u}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(F),s.size===0)W(n);else if(s.size===1){const[u,o]=s.entries().next().value;q(n,u,o)}}},W=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,u]=t.entries().next().value;n!==F&&q(e.slice(0,-1),n,u)}},q=(e,t,s)=>{if(e.length===0)return;const[n,u]=M(e);n.set(u+t,s),n.delete(u)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",$="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,ht=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:u,terms:o,match:i}=t.get(s);n.score=n.score+u,n.match=Object.assign(n.match,i),P(n.terms,o)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const u=e.get(n);if(u==null)continue;const{score:o,terms:i,match:r}=t.get(n);P(u.terms,i),s.set(n,{score:u.score+o,terms:u.terms,match:Object.assign(u.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,u,o)=>{const{k:i,b:r,d}=o;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/u)))},ft=e=>(t,s,n)=>{const u=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,o=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:u,prefix:o}},H=(e,t,s,n)=>{for(const u of Object.keys(e._fieldIds))if(e._fieldIds[u]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${u}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const u=e._index.fetch(n,ht),o=u.get(t);o==null||o.get(s)==null?H(e,s,t,n):o.get(s)<=1?o.size<=1?u.delete(t):o.delete(s):o.set(s,o.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:$,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},yt={..._t,...U},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(dt[s])||new Map},B=(e,t,s,n,u,o,i,r,d=new Map)=>{if(u==null)return d;for(const l of Object.keys(o)){const a=o[l],h=e._fieldIds[l],m=u.get(h);if(m==null)continue;let p=m.size;const f=e._avgFieldLength[h];for(const c of m.keys()){if(!e._documentIds.has(c)){gt(e,h,c,s),p-=1;continue}const g=i?i(e._documentIds.get(c),s,e._storedFields.get(c)):1;if(!g)continue;const _=m.get(c),y=e._fieldLength.get(c)[h],b=at(_,p,e._documentCount,y,f,r),z=n*a*g*b,A=d.get(c);if(A){A.score+=z,lt(A.terms,t);const w=G(A.match,s);w?w.push(l):A.match[s]=[l]}else d.set(c,{score:z,terms:[t],match:{[s]:[l]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},u=(n.fields||e._options.fields).reduce((c,g)=>({...c,[g]:G(n.boost,g)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:l,prefix:a}={...J.weights,...i},h=e._index.get(t.term),m=B(e,t.term,t.term,1,h,u,o,d);let p,f;if(t.prefix&&(p=e._index.atPrefix(t.term)),t.fuzzy){const c=t.fuzzy===!0?.2:t.fuzzy,g=c<1?Math.min(r,Math.round(t.term.length*c)):c;g&&(f=e._index.fuzzyGet(t.term,g))}if(p)for(const[c,g]of p){const _=c.length-t.term.length;if(!_)continue;f?.delete(c);const y=a*c.length/(c.length+.3*_);B(e,t.term,c,y,g,u,o,d,m)}if(f)for(const c of f.keys()){const[g,_]=f.get(c);if(!_)continue;const y=l*c.length/(c.length+_);B(e,t.term,c,y,g,u,o,d,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(m=>X(e,m,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:u,searchOptions:o}=e._options,i={tokenize:n,processTerm:u,...o,...s},{tokenize:r,processTerm:d}=i,l=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>At(e,a,i));return Y(l,i.combineWith)},K=(e,t,s={})=>{const n=X(e,t,s),u=[];for(const[o,{score:i,terms:r,match:d}]of n){const l=r.length,a={id:e._documentIds.get(o),score:i*l,terms:Object.keys(d),match:d};Object.assign(a,e._storedFields.get(o)),(s.filter==null||s.filter(a))&&u.push(a)}return u.sort(N),u},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:o,terms:i}of K(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=o,d.count+=1):n.set(r,{score:o,terms:i,count:1})}const u=[];for(const[o,{score:i,terms:r,count:d}]of n)u.push({suggestion:o,terms:r,score:i/d});return u.sort(N),u};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const u={};for(const[o,i]of n)u[o]=Object.fromEntries(i);t.push([s,u])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:u,fieldLength:o,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:l},a)=>{if(l!==1&&l!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=k(n),h._idToShortId=new Map,h._fieldIds=u,h._fieldLength=k(o),h._avgFieldLength=i,h._storedFields=k(r),h._dirtCount=d||0,h._index=new C;for(const[m,p]of h._documentIds)h._idToShortId.set(p,m);for(const[m,p]of e){const f=new Map;for(const c of Object.keys(p)){let g=p[c];l===1&&(g=g.ds),f.set(parseInt(c,10),k(g))}h._index.set(m,f)}return h},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),u=[];let o=0,i=0;const r=(l,a=!1)=>{let h="";i===0?h=l.length>20?`… ${l.slice(-20)}`:l:a?h=l.length+i>100?`${l.slice(0,100-i)}… `:l:h=l.length>20?`${l.slice(0,20)} … ${l.slice(-20)}`:l,h&&u.push(h),i+=h.length,a||(u.push(["mark",t]),i+=t.length,i>=100&&u.push(" …"))};let d=s.indexOf(n,o);if(d===-1)return null;for(;d>=0;){const l=d+n.length;if(r(e.slice(o,d)),o=l,i>100)break;d=s.indexOf(n,o)}return i<100&&r(e.slice(o),!0),u},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),kt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),et=(e,t,s={})=>{const n={};return K(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(u=>{const{id:o,terms:i,score:r}=u,d=o.includes("@"),l=o.includes("#"),[a,h]=o.split(/[#@]/),m=i.sort((f,c)=>f.length-c.length).filter((f,c)=>i.slice(c+1).every(g=>!g.includes(f))),{contents:p}=n[a]??={title:"",contents:[]};if(d)p.push([{type:"customField",key:a,index:h,display:m.map(f=>u.c.map(c=>j(c,f))).flat().filter(f=>f!==null)},r]);else{const f=m.map(c=>j(u.h,c)).filter(c=>c!==null);if(f.length&&p.push([{type:l?"heading":"title",key:a,...l&&{anchor:h},display:f},r]),"t"in u)for(const c of u.t){const g=m.map(_=>j(c,_)).filter(_=>_!==null);g.length&&p.push([{type:"text",key:a,...l&&{anchor:h},display:g},r])}}}),Q(n).sort(([,u],[,o])=>"max"==="total"?xt(u,o):kt(u,o)).map(([u,{title:o,contents:i}])=>{if(!o){const r=it(t,u);r&&(o=r.h)}return{title:o,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>Ct(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/\":{\"documentCount\":136,\"nextId\":136,\"documentIds\":{\"0\":\"v-184f4da6\",\"1\":\"v-3caeec67\",\"2\":\"v-3caeec67#basic-information\",\"3\":\"v-3caeec67#問題描述\",\"4\":\"v-3caeec67#related-works\",\"5\":\"v-3caeec67#never-give-up\",\"6\":\"v-3caeec67#intrinsic-reward\",\"7\":\"v-3caeec67#uvfa\",\"8\":\"v-3caeec67#rl-loss\",\"9\":\"v-3caeec67#ngu-agent\",\"10\":\"v-3caeec67#ngu-的問題\",\"11\":\"v-3caeec67#methodology\",\"12\":\"v-3caeec67#state-action-value-function-parameterization\",\"13\":\"v-3caeec67#adaptive-exploration-over-a-family-of-policies-bandit\",\"14\":\"v-3caeec67#upper-confidence-bound-algorithm-ucb\",\"15\":\"v-3caeec67#sliding-window-ucb\",\"16\":\"v-3caeec67#simplified-sliding-window-ucb\",\"17\":\"v-3caeec67#backprop-through-time-window-size\",\"18\":\"v-3caeec67#high-level-architecture\",\"19\":\"v-3caeec67#actors\",\"20\":\"v-3caeec67#results\",\"21\":\"v-3caeec67#settings\",\"22\":\"v-3caeec67#state-action-value-function-parameterization-1\",\"23\":\"v-3caeec67#backprop-through-time-window-size-1\",\"24\":\"v-3caeec67#adaptive-exploration\",\"25\":\"v-3caeec67#summary\",\"26\":\"v-3caeec67#discussion\",\"27\":\"v-3caeec67#contribution\",\"28\":\"v-3caeec67#值得一看的文章們\",\"29\":\"v-3caeec67@0\",\"30\":\"v-3caeec67@1\",\"31\":\"v-c0336012\",\"32\":\"v-c0336012#basic-information\",\"33\":\"v-c0336012#what-is-domain-adaption\",\"34\":\"v-c0336012#問題描述\",\"35\":\"v-c0336012#related-works\",\"36\":\"v-c0336012#domain-alignment\",\"37\":\"v-c0336012#pseudo-labelling-or-self-training\",\"38\":\"v-c0336012#mixing\",\"39\":\"v-c0336012#methodology\",\"40\":\"v-c0336012#naive-mixing-to-uda\",\"41\":\"v-c0336012#domain-adaption-via-corss-domain-mixed-sampling-dacs\",\"42\":\"v-c0336012#results\",\"43\":\"v-c0336012#實驗設定\",\"44\":\"v-c0336012#dataset\",\"45\":\"v-c0336012#cityscapes\",\"46\":\"v-c0336012#gta5\",\"47\":\"v-c0336012#synthia\",\"48\":\"v-c0336012#gta5-cityscapes\",\"49\":\"v-c0336012#synthia-cityscapes\",\"50\":\"v-c0336012#some-issues-about-evaluation\",\"51\":\"v-c0336012#contribution\",\"52\":\"v-c0336012#值得一看的文章們\",\"53\":\"v-c0336012@0\",\"54\":\"v-c0336012@1\",\"55\":\"v-6fdb6976\",\"56\":\"v-6fdb6976#basic-information\",\"57\":\"v-6fdb6976#問題描述\",\"58\":\"v-6fdb6976#related-works\",\"59\":\"v-6fdb6976#methodology\",\"60\":\"v-6fdb6976#self-training-for-uda\",\"61\":\"v-6fdb6976#daformer-network-architecture\",\"62\":\"v-6fdb6976#rare-class-sampling-rcs\",\"63\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd\",\"64\":\"v-6fdb6976#learning-rate-warmup-for-uda\",\"65\":\"v-6fdb6976#results\",\"66\":\"v-6fdb6976#實驗設定\",\"67\":\"v-6fdb6976#summary\",\"68\":\"v-6fdb6976#learning-rate-warmup\",\"69\":\"v-6fdb6976#rare-class-sampling-rcs-1\",\"70\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd-1\",\"71\":\"v-6fdb6976#daformer-decoder\",\"72\":\"v-6fdb6976#contribution\",\"73\":\"v-6fdb6976#值得一看的文章們\",\"74\":\"v-6fdb6976@0\",\"75\":\"v-6fdb6976@1\",\"76\":\"v-32d63a0d\",\"77\":\"v-32d63a0d#basic-information\",\"78\":\"v-32d63a0d#問題描述\",\"79\":\"v-32d63a0d#related-works\",\"80\":\"v-32d63a0d#q-networks\",\"81\":\"v-32d63a0d#td-gammon\",\"82\":\"v-32d63a0d#收斂性相關研究\",\"83\":\"v-32d63a0d#nfq\",\"84\":\"v-32d63a0d#methodology\",\"85\":\"v-32d63a0d#results\",\"86\":\"v-32d63a0d#實驗設定\",\"87\":\"v-32d63a0d#評估方式\",\"88\":\"v-32d63a0d#比較基準\",\"89\":\"v-32d63a0d#contribution\",\"90\":\"v-32d63a0d@0\",\"91\":\"v-32d63a0d@1\",\"92\":\"v-5b18c8c4\",\"93\":\"v-5b18c8c4#basic-information\",\"94\":\"v-5b18c8c4#問題描述\",\"95\":\"v-5b18c8c4#related-works\",\"96\":\"v-5b18c8c4#parameter-space-noise-for-exploration\",\"97\":\"v-5b18c8c4#dqn\",\"98\":\"v-5b18c8c4#double-dqn\",\"99\":\"v-5b18c8c4#dueling-dqn\",\"100\":\"v-5b18c8c4#a3c\",\"101\":\"v-5b18c8c4#methodology\",\"102\":\"v-5b18c8c4#基本想法\",\"103\":\"v-5b18c8c4#減少產-random-number-時間\",\"104\":\"v-5b18c8c4#dqn-dueling-dqn\",\"105\":\"v-5b18c8c4#distributed-a3c\",\"106\":\"v-5b18c8c4#results\",\"107\":\"v-5b18c8c4#experiments\",\"108\":\"v-5b18c8c4#analysis\",\"109\":\"v-5b18c8c4#contribution\",\"110\":\"v-5b18c8c4#值得一看的文章們\",\"111\":\"v-5b18c8c4@0\",\"112\":\"v-5b18c8c4@1\",\"113\":\"v-0fd9e004\",\"114\":\"v-0fd9e004#basic-information\",\"115\":\"v-0fd9e004#問題描述\",\"116\":\"v-0fd9e004#related-works\",\"117\":\"v-0fd9e004#methodology\",\"118\":\"v-0fd9e004#preliminary\",\"119\":\"v-0fd9e004#target\",\"120\":\"v-0fd9e004#prototypical-pseudo-label-denoising\",\"121\":\"v-0fd9e004#權重計算\",\"122\":\"v-0fd9e004#prototype-計算\",\"123\":\"v-0fd9e004#loss-計算\",\"124\":\"v-0fd9e004#structure-learning-by-enforcing-consistency\",\"125\":\"v-0fd9e004#distillation-to-self-supervised-model\",\"126\":\"v-0fd9e004#整體流程\",\"127\":\"v-0fd9e004#results\",\"128\":\"v-0fd9e004#實驗設定\",\"129\":\"v-0fd9e004#gta5-cityscapes\",\"130\":\"v-0fd9e004#synthia-cityscapes\",\"131\":\"v-0fd9e004#contribution\",\"132\":\"v-0fd9e004#值得一看的文章們\",\"133\":\"v-0fd9e004@0\",\"134\":\"v-0fd9e004@1\",\"135\":\"v-e1e3da16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2],\"1\":[6],\"2\":[2,13],\"3\":[1,115],\"4\":[2],\"5\":[3,18],\"6\":[2,75],\"7\":[1,46],\"8\":[2,114],\"9\":[2,41],\"10\":[2,58],\"11\":[1],\"12\":[5,132],\"13\":[9,91],\"14\":[6,83],\"15\":[3,56],\"16\":[4,52],\"17\":[5,30],\"18\":[3,13],\"19\":[1,80],\"20\":[1],\"21\":[1,56],\"22\":[5,137],\"23\":[5,39],\"24\":[2,144],\"25\":[1,63],\"26\":[1],\"27\":[1,22],\"28\":[1,41],\"29\":[null,null,1],\"30\":[null,null,5],\"31\":[8],\"32\":[2,19],\"33\":[4,49],\"34\":[1,49],\"35\":[2],\"36\":[2,77],\"37\":[6,108],\"38\":[1,66],\"39\":[1],\"40\":[4,54],\"41\":[9,67],\"42\":[1],\"43\":[1,37],\"44\":[1,15],\"45\":[1,13],\"46\":[1,19],\"47\":[1,28],\"48\":[3,48],\"49\":[3,39],\"50\":[4,52],\"51\":[1,19],\"52\":[1,52],\"53\":[null,null,1],\"54\":[null,null,7],\"55\":[12],\"56\":[2,24],\"57\":[1,49],\"58\":[2,10],\"59\":[1],\"60\":[4,149],\"61\":[3,167],\"62\":[5,60],\"63\":[7,112],\"64\":[5,13],\"65\":[1],\"66\":[1,42],\"67\":[1,45],\"68\":[3,22],\"69\":[5,48],\"70\":[7,63],\"71\":[2,43],\"72\":[1,22],\"73\":[1,20],\"74\":[null,null,1],\"75\":[null,null,7],\"76\":[6],\"77\":[2,16],\"78\":[1,60],\"79\":[2,16],\"80\":[2,85],\"81\":[2,20],\"82\":[1,32],\"83\":[1,25],\"84\":[1,94],\"85\":[1],\"86\":[1,52],\"87\":[1,50],\"88\":[1,49],\"89\":[1,19],\"90\":[null,null,1],\"91\":[null,null,5],\"92\":[4],\"93\":[2,13],\"94\":[1,40],\"95\":[2,19],\"96\":[5,78],\"97\":[1,70],\"98\":[2,43],\"99\":[2,92],\"100\":[1,129],\"101\":[1],\"102\":[1,86],\"103\":[4,48],\"104\":[2,55],\"105\":[2,71],\"106\":[1],\"107\":[1,73],\"108\":[1,61],\"109\":[1,12],\"110\":[1,49],\"111\":[null,null,1],\"112\":[null,null,5],\"113\":[13],\"114\":[2,22],\"115\":[1,83],\"116\":[2,10],\"117\":[1],\"118\":[1,33],\"119\":[1,75],\"120\":[4,86],\"121\":[1,43],\"122\":[2,46],\"123\":[2,60],\"124\":[5,117],\"125\":[5,45],\"126\":[1,52],\"127\":[1],\"128\":[1,35],\"129\":[4,37],\"130\":[4,11],\"131\":[1,19],\"132\":[1,37],\"133\":[null,null,1],\"134\":[null,null,7],\"135\":[1]},\"averageFieldLength\":[2.5459986463885187,53.08775675083696,0.5803418221569123],\"storedFields\":{\"0\":{\"h\":\"About us\"},\"1\":{\"h\":\"Agent57: Outperforming the Atari Human Benchmark\"},\"2\":{\"h\":\"Basic Information\",\"t\":[\"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, et al. @ Google DeepMind\",\"2020 ICML\"]},\"3\":{\"h\":\"問題描述\",\"t\":[\"在 RL 當中，Atari games 是一個相當重要的 benchmark。過去的 RL 模型已經能夠在大多的 atari games 當中獲得相當不錯的 performance，例如 MuZero、R2D2，分別在 57 個遊戲當中有 51 和 52 個遊戲是 outperform 人類的。不過可惜的是，在剩下的遊戲當中這些 SoTA 就通常完全沒辦法學習。\",\"Info\",\"稍微翻了一下 MuZero 以及 R2D2 兩篇 paper 的結果，分別是這些遊戲 performance 不太好。\",\"MuZero \",\"montezuma revenge, pitfall, private eye, skiing, solaris, venture\",\"R2D2 \",\"montezuma revenge, pitfall, private eye, skiing, solaris\",\"那麼，剩下這些遊戲有怎樣的共通點呢？\",\"skiing 和 solaris 這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到 reward，在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響。\",\"Skiing game on Atari 2600. Video from TheLimeyDragon\",\"以 Skiing 這款遊戲來說，玩家要操作角色滑雪，途中要盡可能快速通過指定數量的 gates。每忽略一個 gate 就會多 5 秒的 penalty。Reward 會一直到遊戲的最後依照最後通過的時間決定。\",\"剩下的四款遊戲則是因為環境太大，又有不少的 negative reward，需要相當大量的探索之後才能得到 positive reward。\",\"Pitfall game on Atari 2600. Video from The No Swear Gamer\",\"以 Ptifall 這款遊戲來說，玩家要操作主角在 20 分鐘的時間探索 255 個遊戲場景，去找到藏在地圖當中的寶藏。過程中有許多陷阱，找到寶藏可以加分，最後分數越多越好。\",\"從這些觀察當中可以得到兩個待改善的地方\",\"long-term credit assignment 如何決定哪些 action 應該要給 positive 或是 negative reward\",\"exploration 如何讓 agent 能夠盡可能去正確探索環境 \",\"之所以說\\\"正確\\\"，是因為即便是在很多 negative reward 的地方，也需要嘗試越過那些障礙，也許才有機會遇到 positive reward。\",\"這一篇 paper 希望改善這兩個對 RL 相當重要的問題，也提出了一個可以在所有 57 Atari games 都 outperform 人類的 RL 模型。\"]},\"4\":{\"h\":\"Related Works\"},\"5\":{\"h\":\"Never Give Up\",\"t\":[\"Never Give Up(NGU) 目的也是希望能夠讓 RL agent 能夠在上述 hard-exploration 的環境當中有更好的成效。具體來說 NGU 包含了幾個重要的部分。\",\"Intrinsic Reward\",\"UVFA\",\"RL Loss\",\"NGU Agent\"]},\"6\":{\"h\":\"Intrinsic Reward\",\"t\":[\"在 Intrinsic Reward 的部分目的也是希望能夠促使 agent 多多探索，他們將 reward 分成了兩個部分，分別是 per-episode noveltyrtepisodic​ 以及 life-long noveltyαt​。這兩者分別會讓 agent 鼓勵去探索那些在 episode 當中、在整個訓練過程當中沒有踏足過的狀態。而整體 intrinsic Reward 如下。\",\"rti​=rtepisodic​⋅min{max{αt​,1},L}(L=5)\",\"min 和 max 只是用來限制 life-long novelty 的範圍，避免太大或是太小。\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"而整體的 reward 依照過去 curiosity-driven exploration 的研究，設定如下。\",\"rtβi​​=rte​+βi​rti​\",\"rte​ 是 Extrinsic Reward，在 RL 當中就是環境給予的 reward\",\"rti​ 是 Intrinsic Reward，也就是前面定義的 reward\",\"βi​ 用來調整兩種 reward 的影響程度\",\"不同的環境下需要的 exploration 以及 exploitation 是不同的。當 β 比較大的時候，intrinsic reward 會使得 agent 比較傾向去試試看那些不熟的 state，反之則會去走那些比較熟悉的。\"]},\"7\":{\"h\":\"UVFA\",\"t\":[\"NGU 接下來用 Universal Value Function Approximator, UVFA 去近似 action value function Q。\",\"Q(st​,at​,βi​)=E[rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+…∣st​,at​,βi​]\",\"針對不同的 βi​，NGU 會選擇不同的 γ。\",\"βi​ 大，傾向 exploration，不需要看太遠，γ 選小一些\",\"βi​ 小，傾向 exploitation，需要看遠一些，γ 選大一些\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"左邊是 β 選擇的分布，右邊是 γ 的分布。\"]},\"8\":{\"h\":\"RL Loss\",\"t\":[\"既然有 NN 去逼近，那也就會有 Loss。NGU 計算 Loss 的方式是採用 Transformed Retrace Double Q-learning Loss。\",\"Retrace 是一個可以用來評估或是用在 control 上的 RL 演算法。在這邊我們在意的是評估的部分，Retrace 可以幫助我們去評估如果我們 follow policy μ，在目標的 policy π 的 action value function Qπ 可以拿到多少 Reward。\",\"首先定義從 policy μ 當中取得的 trajectories τ\",\"τ=(xt​,at​,rt​,xt+1​)t∈N​\",\"考慮有限的 sampled sequences，定義 Retrace operator\",\"T^Q(xt​,at​)=Q(xt​,at​)+s=t∑t+k−1​γs−t(i=t+1∏s​ci​)δs​\",\"其中\",\"δt​cs​​=rt​+γa∈A∑​π(a∣xt+1​)Q(xt+1​,a)−Q(xt​,at​)=λmin(1,μ(as​∣xs​)π(as​∣xs​)​)​\",\"實際上訓練的 NN 會有兩個，就跟 DQN 一樣，一個是 target network，一個是 online network。Target network 就可以透過 Retrace operation 去得到目標 yt​^​\",\"yt​^​=T^Q(xt​,at​;θ−)\",\"θ− 是 target network 的 parameter。\",\"有了目標，也就能夠得到 Loss\",\"L(xt​,at​,θ)=(Q(xt​,at​,θ)−yt​^​)2\",\"Tips\",\"上面提及的是單純的 Retrace Double Q-learning Loss，實際上還會為了讓 NN 更好學習，改成 Transformed 版本。\",\"ThQ(x,a)=Eμ​[h(h−1(Q(x,a))+t≥0∑​γt(s=1∏t​cs​)δth​)]\",\"其中\",\"δth​=rt​+γa∈A∑​π(a∣xt+1​)h−1(Q(xt+1​,a)−h−1Q(xt​,at​))\",\"∀z∈R,h(z)∀z∈R,h−1(z)​=sgn(z)(∣z∣+1​−1)+ϵz=sgn(z)((2ϵ1+4ϵ(∣z∣+1+ϵ)​−1​)−1)​\",\"但數學有點太難，我還沒有理解這一段做了什麼。\"]},\"9\":{\"h\":\"NGU Agent\",\"t\":[\"NGU 基本上使用了 R2D2，只不過輸入上會丟\",\"Action at−1​\",\"Extrinsic Reward rt−1e​\",\"Intrinsic Reward rt−1i​\",\"βi​\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"NGU 採用分散式學習，有許多的 actor 使用不同的 βi​ 取得不同的 experience 丟在 replay buffer，然後再讓 learner 使用 experience 去更新參數學習。\",\"最後只需要設定 β=0，就可以得到單純 exploitation 的模型當成最後的結果。\"]},\"10\":{\"h\":\"NGU 的問題\",\"t\":[\"實作上 NGU 有時會很不穩定、難以收斂，尤其當 rti​ 和 rte​ 的大小、分布相當不同時 \",\"Agent57 的作者認為是因為 NGU 只用了一個 NN 去學習導致\",\"不是那麼地 general \",\"解決了一些 hard-exploration 的問題，卻在一些簡單的問題做得很差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"每種 policy(不同 βi​ 的選擇) sample 的 experience 數量相同 \",\"有些 policy 對於學習是並沒有幫助的，但是卻跟其他人有同樣的影響力\",\"有些環境需要更多的 exploration，有些則不需要\",\"無法好好處理 long-term credit assignment 問題 \",\"例如在 skiiing 以及 solaris 就做得頗差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\"]},\"11\":{\"h\":\"Methodology\"},\"12\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"Agent57 首先針對 State-Action Value Function 拆開來，用兩個 NN 分別去針對 Extrinsic 以及 Intrinsic Reward 處理。\",\"Q(x,a,j;θ)=Q(x,a,j;θe)+βj​Q(x,a,j;θi)\",\"x: state\",\"a: action\",\"j: 表示使用的是哪一個 policy 的 one-hot vector\",\"θe: 近似 Extrinsic Reward re 的 NN\",\"θi: 近似 Intrinsic Reward ri 的 NN\",\"θ: θe∪θi\",\"兩個 Q-Network 都會接收同樣的 state 和 action，並且也是 follow 相同的 policy π。\",\"π(x)=arga∈Amax​Q(x,a,j;θ)\",\"兩個模型都是使用 Transformed Retrace Loss，跟 NGU 是一樣的，不過在計算 Loss 時 reward 的部分是分別給 re 和 ri。\",\"細節上，因為是一次更新 B 個 batch，每個 batch sample 的 sequence 大小為 H，因此 Loss 會有兩組總和。\",\"L(D,θ,θ−,π,μ,r,h)=b=0∑B−1​s=t∑t+H−1​(Q(xsb​,asb​;θ)−T^r,hμ,π​Q(xsb​,asb​;θ−))2\",\"D 表示從 μ sample 出來的 trajectories\",\"θ 為 online network 的參數\",\"θ− 為 target network 的參數\",\"π 為目標 policy\",\"μ 為當前 policy\",\"r 表示 reward，上面的差異就是這裡傳入的分別是 re 和 ri\",\"h 為 Transformed Retrace Operator 的 h\",\"xsb​ 是在 batch b、時間 s 的 state\",\"asb​ 是在 batch b、時間 s 的 action\",\"於是 Agent57 的模型變成底下的樣子。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"Note: 雖然兩個模型都會把 intrinsic 以及 extrinsic reward 輸入進去，但 Loss 在計算上分別都只會拿自己的。\",\"Tips\",\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子，也就是說這種做法的正確性是被確保的(無論是否有使用 Transformed 的版本)。\",\"不過實際上訓練時因為拆開來訓練，能夠使模型更好去學習各自的 reward，以達到更好的訓練成效。\",\"透過拆開訓練，解決了 NGU 不穩定、難以收斂的問題。\"]},\"13\":{\"h\":\"Adaptive Exploration over a Family of Policies (Bandit)\",\"t\":[\"「每種 policy sample 的 experience 數量相同」這個問題 Agent57 透過加上 Meta-controller 來解決。\",\"Tips\",\"如果每個 actor 都能夠學習什麼時候該 exploit、什麼時候該 explore，選擇出現傾向，不同 policy 就有不同重要程度了\",\"舉一個例子來說，NGU 會把每個 actor 都當成是工廠生產出來的機器人，每一個 actor 一開始都是一樣的。\",\"接下來依照你的需求不同，你分別把這幾個 actor 加上不同的偏好，有些傾向 exploration，有些傾向 exploitation。\",\"這些 actor 就會去環境當中互動，蒐集一些 experience 給你學習。\",\"另一方面，Agent57 的 actor 天生就有一些自己的偏好，有人天生愛探險，有人天生愛保險。\",\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的 reward。\",\"最後你一樣可以透過這些 actor 蒐集的 experience 去學習。但是 policy 不會被固定下來，具有更高的靈活性。\",\"照著這樣的想法，Agent57 讓每個 actor 前面都加上一組 Meta-controller，在每一個 episode 開始之前，透過它決定接下來要使用的 (βj​,γj​)。此外，Meta-controller 也會依據得到的 reward 去調整選擇不同 j 的機率。\",\"如此一來，每個 actor 就會因為 Meta-controller 的存在，產生出選擇 policy 的傾向，進而使得整體訓練採用的 experience 中 policy 的比例改變。\",\"Warning\",\"細節上，每個 actor 選擇 action 都是採用 ϵl​-greedy，其中的 l 表示不同的 actor。亦即，不同 actor 採用不同的 ϵ 大小，也因為如此，Meta-controller 是每個 actor 各有一個。\"]},\"14\":{\"h\":\"Upper Confidence Bound Algorithm (UCB)\",\"t\":[\"Agent57 把 Meta-controller 簡單設計成一個 Multi-Arm Bandit (MAB) 問題，也就是說我現在面前有 N 個 action {0,…,N−1} 可以選擇，在時間 k 你選擇 Ak​，目標是在整個 horizon K 當中你可以得到最好的 return，也就是讓底下的期望值最大化。\",\"Eπ​[k=0∑K−1​Rk​(Ak​)]\",\"過去對於 MAB 在 reward 的分布是固定的狀況下會使用 UCB 來解決它。基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界，把這個上界當成是它預期的 return，選擇其中最大的當成這次的選擇。\",\"未知/嘗試次數少的選擇 (不確定性高，要傾向 exploration) \",\"平均 Return 低 ➡️ UCB 高 ➡️ 探索機率高\",\"平均 Return 高 ➡️ UCB 更高 ➡️ 探索機率更高\",\"已知/嘗試次數多的選擇 (不確定性低，要傾向 exploitation) \",\"平均 Return 低 ➡️ UCB 低 ➡️ 嘗試機率低\",\"平均 Return 高 ➡️ UCB 高 ➡️ 嘗試機率高\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a)+βNk−1​(a)log(k−1)​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a)μ^​k​(a)​=m=0∑k−1​1{Am​=a}​=Nk​(a)1​m=0∑k−1​Rk​(a)1{Am​=a}​​\",\"也就是說\",\"Nk​(a) 用來表示一個 action a 至今被嘗試的次數\",\"μ^​k​(a) 用來表示一個 action a 至今平均的 Return\",\"從式子當中也可以觀察到，確實它會傾向讓 平均 Return 高 或是 嘗試次數少 的選項有更高機率被選擇到。\"]},\"15\":{\"h\":\"Sliding-Window UCB\",\"t\":[\"然而，如果 reward 的分布會變動的話，單純的 UCB 並不會是一個好的選項，因為過去的經驗即便在現實狀況改變仍然有大影響力。而隨著 agent 更新、行為模式改變，reward 的分布也會變動。\",\"這裡的經驗指的是一個 action 採取的次數以及得到的 Return 平均 (Nk​(a) 和 μ^​k​(a))。\",\"因此 Sliding-Window UCB 加上了一個 window length τ∈N∗ 來限制要考慮多久之前的經驗。\",\"τ 的選擇應遠比 K 小。\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)log(min(k−1,τ))​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a,τ)μ^​k​(a,τ)​=m=max(0,k−τ)∑k−1​1{Am​=a}​=Nk​(a,τ)1​m=max(0,k−τ)∑k−1​Rk​(a)1{Am​=a}​​\",\"僅僅是加上 τ 而已，剩餘的都是相同的。\"]},\"16\":{\"h\":\"Simplified Sliding-Window UCB\",\"t\":[\"最後，Agent57 對 Sliding-Window UCB 做了兩個小修正\",\"log 對於結果並不會有影響，可以移除\",\"多加上 ϵ-greedy\",\"Ak​=⎩⎨⎧​kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)1​​Yk​​∀0≤k≤N−1∀N≤k≤K−1,Uk​≥ϵUCB​∀N≤k≤K−1,Uk​<ϵUCB​​\",\"其中\",\"ϵUCB​ 是一個 hyperparameter\",\"Uk​ 是一個 [0,1] 之間均勻分布的隨機值\",\"Yk​ 是一個 {0,…,N−1} 之間均勻分布的隨機 action\",\"Tips\",\"透過 Bandit，每個 actor 能夠調整自己的 (γ,β)，解決了 NGU「不是那麼地 general」、「每種 policy sample 的 experience 數量相同」這兩個問題。\"]},\"17\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"原先 R2D2 在 Replay buffer 的設計是採用 trace length 80 搭配 replay period 40，作者在實驗當中發現如果採用 trace length 160 搭配 replay period 80，也就是 long trace 的話，對於 long-term credit assignment 的問題似乎能夠得到改善。\",\"Tips\",\"透過 long trace 解決了 NGU「無法好好處理 long-term credit assignment」的問題。\"]},\"18\":{\"h\":\"High-level architecture\",\"t\":[\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"19\":{\"h\":\"Actors\",\"t\":[\"每個 episode 開始前，透過各自的 Meta-Controller 選擇出一組 (γj​,βj​)\",\"透過上一個 trajectory (xt​,rt−1e​,rt−1i​,at−1​,ht−1​) 估計當前 state-action value Q(xt​,⋅,j,θl​)\",\"透過 ϵl​-greedy 選擇 action\",\"計算 intrinsic reward rti​\",\"環境中取得 observation xt+1​, extrinsic reward rte​\",\"若已經又經過 400 個 frames，更新模型參數\",\"重複 2 直到 episode 結束\",\"將 trajectories 交給 replay buffer\",\"ϵl​ 的選擇根據 Dan Horgan, John Quan, David Budden, et al. (2018) 如下\",\"ϵl​=ϵ1+αL−11​\",\"其他部分基本上都跟 NGU 相同。總之，Actors 去跟環境互動，取得 experience 之後交給 replay buffer，Learner 會從 replay buffer 當中 sample 一些 experience 學習，然後繼續跟環境互動。\"]},\"20\":{\"h\":\"Results\"},\"21\":{\"h\":\"Settings\",\"t\":[\"Agent57 在 γ 的分布上有做了一點調整，範圍變成 [0.99,0.9999]，具體來說如下圖\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"其他 Hyperparameter 的設定詳閱論文的 Appendix G，這裡就不贅述。\",\"對於每個實驗的 Agent 都另外加上一個 Evaluator 去紀錄訓練過程當中的 undiscounted episode returns。\",\"此外，他們並不是採用 Human Normalized Scores (HNS)，而是 Capped Human Normalized Scores (CHNS)，這個測量標準比較強調那些 HNS 比較差的結果，也限制了數值範圍，因此會比較能夠好好評估 general performance。\",\"CHNS=max{min{HNS,1},0}\",\"其中\",\"HNS=Humanscore​−Randomscore​Agentscore​−Randomscore​​\"]},\"22\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"我們透過 intrinsic 以及 extrinsic 拆開來解決 NGU 的缺陷，這裡要來實驗這一個做法實際上帶來多少影響。\",\"作者建構一個簡單的 15×15 Gridworld random coin。在每個 episode 開始之前他們把一個 agent 以及一個 coin 隨機地放在地圖上的任意格子。Agent 能夠上、下、左、右移動，並且每個 episode 最多 200 個 steps。當 Agent 走到 coin 會得到 reward 1，然後結束這個 episode。\",\"接著作者比較 NGU 以及 NGU 加上 separate network 的做法。如同前面提及 βj​ 如果選擇較大，由於 intrinsic reward 有較大的影響，agent 會偏向 exploration，反之則是 exploitation。細節上，βj​ 的設定會透過 β 來調整整體 βj​ 的大小。\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"作者比較兩個模型在不同 β 的大小下，各自最傾向 exploration (βj​=maxj​βj​) 以及最傾向 exploitation (βj​=0) 的設定取得的 Extrinsic Reward。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"x 軸表示 β (注意並不是 βj​)\",\"y 軸表示 extrinsic reward\",\"紫色圓點表示 βj​=0，最傾向 exploitation 的狀況\",\"綠色圓點表示 βj​=maxj​βj​，最傾向 exploration 的狀況\",\"從結果可以發現到 NGU 在不同 β 的設定下會大程度影響到最終 exploitation 的結果，即便這個環境設定是相當簡單的，最終 Return 的趨勢仍然是隨著 β 越大變得越小。\",\"另一方面，加上了 separate network 的狀況下 exploitation 的 return 基本上都相當接近 1.0，也就是說能夠順利到達 coin 所在的位置。\",\"在 exploration 的部分也可以發現到兩者的發展方向會稍有不同。但整體來說兩者都能在最後趨近於 0.0。\",\"由此可見，當 β 提升，由於 intrinsic reward 與 extrinsic reward 的大小相差越來越懸殊，導致 NGU 並無法好好只透過一個 NN 去學習，進而影響到結果，較不具有彈性。相對的，增加 separate network 確實能夠帶來相當好的效益。\",\"此外，作者也發現如果把 Agent57 的 separate network 移除，performance 會掉 20% 以上，可見 separate network 的重要性。\",\"作者也發現到 separate network 在最傾向 exploration 的模型會盡可能避開 coin，反之會走出最短路。\",\"Tips\",\"值得一提的是，這個結果如果在取得 coin 之後仍然不會停止的話就不會出現。\"]},\"23\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"在 trace length 以及對應的 replay period 有多少影響呢？\",\"作者將 R2D2 以及 Agent57 分別用 small trace 以及 long trace 來比較，作者認為在這兩者都有一個共通點：Long trace 會導致訓練前期較為緩慢，但最後能取得更好的 performance。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"在 10 個比較難的遊戲當中測試的結果\",\"尤其在 Solaris 這一款遊戲，可以看到比較明顯的結果。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"24\":{\"h\":\"Adaptive Exploration\",\"t\":[\"最後是針對 Meta-Controller 的實驗。作者將 R2D2+sep. network 以及 NGU+sep. network 拿來比較加上 Meta-Conroller 以及沒有的狀況。\",\"在 10 個比較困難的遊戲當中，可以發現到加上 Meta-Controller(圖片中以 bandit 表示)後可以得到更好的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"此外，從上面的圖片中也可以觀察到這樣的 improvement 在 NGU 當中是小許多的。可以認為 separate networks 跟 meta-controller 之間有一些重疊的 benifit\",\"另一方面，有了 meta-controller 之後，即便 discount factor γ 異常地大(如 γ=0.9999)，模型還是能夠順利學習。在下表當中可以看到 high gamma 的 R2D2，在搭配了 meta-controller 之後得到的成效在 10 款比較困難的遊戲當中有些甚至是能夠比 Average Human 還要強。\",\"Games\",\"R2D2(Retrace) high gamma\",\"Average Human\",\"beam rider\",\"349971.96 ± 5595.38\",\"16926.50\",\"freeway\",\"32.84 ± 0.06\",\"29.60\",\"montezuma revenge\",\"1664.89 ± 1177.26\",\"4753.30\",\"pitfall\",\"0.00 ± 0.00\",\"6463.70\",\"pong\",\"21.00 ± 0.00\",\"14.60\",\"private eye\",\"22480.31 ± 10362.99\",\"69571.30\",\"skiing\",\"-4596.26 ± 601.04\",\"-4336.90\",\"solaris\",\"14814.76 ± 11361.16\",\"12326.70\",\"surround\",\"10.00 ± 0.00\",\"6.50\",\"venture\",\"1774.89 ± 83.79\",\"1187.50\",\"因此作者認為 meta-controller 提供了更大的普遍性，即便在參數比較異常的狀況下仍然能有很不錯的學習成果。\",\"最後，作者也觀察了在幾款遊戲訓練過程中當中 Meta-Controller 在每個 bandit 選擇中最大的 return 分別落在哪個 bandit，可以發現到不同的遊戲會有不同的偏好。從這裡也可以了解到實際上讓每個 actor 自己調整 policy、適應不同的環境，實際上是有幫助的。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"25\":{\"h\":\"Summary\",\"t\":[\"最後比較 R2D2、NGU、Agent57、MuZero 在所有 Atari games 的優劣，可以發現到 MuZero 雖然在 uncapped mean 有最好的結果，但是在 capped mean 卻是最差的。顯示了 MuZero 在限定幾款遊戲有特別出色的成效，但並不 general。\",\"同時也可以看到 Agent57 有最大的 Capped Mean 100，亦即 Agent57 能夠在所有的 Atari games 當中獲得比人類平均還要好的成果，除了展現驚人的成果以外，也說明了 Agent57 的普遍性。\",\"同時也能在 R2D2 與 R2D2 bandit 的比較當中明顯看到在所有的成績都有所提升，再次說明了 Meta-Controller 帶來的效益。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"最後，Agent57 透過 separate networks、Meta-Controller、long trace 解決了 NGU 的四個缺陷，最終在所有的 Atari games 當中都獲得了超過人類的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"26\":{\"h\":\"Discussion\"},\"27\":{\"h\":\"Contribution\",\"t\":[\"提出透過 separate networks 解決訓練不穩定、難以收斂的問題\",\"提出 Meta-Controller 來讓每個 actor 自適應不同環境，使模型具有更好的普遍性，並且不同 policy 得以有不同程度的影響\",\"第一個能夠在所有 Atari games 都獲得比 Average Human 更好的成效\"]},\"28\":{\"h\":\"值得一看的文章們\",\"t\":[\"Agent57: Outperforming the human Atari benchmark\",\"Recurrent Neural Networks in Reinforcement Learning\",\"MAB - UCB <> TS 基本概念\",\"Safe and efficient off-policy reinforcement learning.\",\"Never Give Up: Learning Directed Exploration Strategies\",\"Adapting Behaviour for Learning Progress\",\"Agent57\",\"Distributed Prioritized Experience Replay\",\"Recurrent experience replay in distributed reinforcement learning\"]},\"29\":{\"c\":[\"Note\"]},\"30\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICML\"]},\"31\":{\"h\":\"DACS: Domain Adaptation via Cross-domain Mixed Sampling\"},\"32\":{\"h\":\"Basic Information\",\"t\":[\"2020 Release\",\"2021 WACV(Winter Conference on Applications of Computer Vision)\",\"Chalmers University of Technology(查爾摩斯理工大學)與 Volvo Cars 共同發表\"]},\"33\":{\"h\":\"What is Domain Adaption\",\"t\":[\"Image from Medium\",\"所謂的 Domain 就是用來描述一群資料他們的分布狀況。\",\"Domain Adaption 的目標是把兩個不同分佈的 Domain (Source Domain 以及 Target Domain) 投射到同一個平面上，使得同類型的資料會相近，反之則相遠。\",\"舉一個在 CV 上的例子。如果我們想要訓練一個模型去做自駕車的街景物件偵測，很多時候我們並不會直接去蒐集真實的資料，像是直接有一台車會去蒐集真實街景資料，這樣所需要的成本會過大。時常我們會訓練在合成資料上(synethic data)，然後再應用在真實的世界當中。\",\"Image from Medium\",\"不過這種情況下一個直覺的問題是，在 虛擬世界(Source Domain) 上也許我們能夠對各種物件去做標記 label，但是對於真實世界(Target Domain)往往會有許多我們沒有的 label、環境與虛擬世界有差距，這種差距被描述為 Domain Shift。當兩個 Domain 相差過大，Domain Shift 過高，就會導致單純在 Source Domain 上訓練的模型難以直接 apply 到 Target Domain 上。\",\"因此，Domain Adaption 想解決的就是盡可能地將 Domain Shift 降低，讓我們得以用較低的成本在虛擬環境中訓練模型，然後應用在真實的環境當中。\"]},\"34\":{\"h\":\"問題描述\",\"t\":[\"近年來透過 CNN 處理 semantic segmentation(影像分割) 的模型雖然有許多，也獲得不錯的成果，不過如果遇到新的 domain，往往就會 work 不太好，尤其是從 synethic data 轉變到 real data 上的時候。\",\"問題在於不同的 domain，各自的 domain distribution 會不同。只訓練在 source domain 的模型對於 target domain 的狀況缺乏認知，導致預測失準。\",\"Info\",\"這就像是同理心，因為缺乏對他人的理解，擅自用自己的思維解讀，就會導致互相的不理解。\",\"Image from Liang-ChiehChen et al. (2015)\",\"可以發現單純用 CNN 就可以得到相當好的影像分割結果。\",\"Image from Yiheng Zhang et al. (2018)\",\"直接把訓練在虛擬環境的模型應用在真實環境，結果相當糟糕。\"]},\"35\":{\"h\":\"Related Works\"},\"36\":{\"h\":\"Domain Alignment\",\"t\":[\"透過 adversarial learning (對抗式學習) 去拉近 source domain 以及 target domain。\",\"我們可以想成現在 Segmentation Network 就是 GAN 的 Generator，然後會有一個 Discriminator 去判別現在給我的究竟是 source domain 還是 target domain 的預測結果。\",\"Image from Yi-Hsuan Tsai et al. (2018)\",\"兩個 Domain 中各取圖片，經過相同的 Segmentation Network，將產出的 semantic maps 做對抗式學習\",\"Info\",\"依照 alignment 的不同，可以分成 pixel level, feature map level, semantic level 等不同的做法。\",\"這樣的做法之所以可行，是源自於即便 domain 不同，在 semantic maps 上的 spatial layout 以及 local context 通常並不會差太多。\",\"DACS 的做法之所以能夠成功，也有部分是源自於這樣的相似性帶來的好處。\",\"Tips\",\"同樣以自駕車的例子來說，即便 synethic data 和 real data 的 domain 有相當大的差異，不過像是馬路、汽車、行人都還是會跟地板黏在一起，其他像是路燈、號誌、天空之類的就通常會像是在半空中。這類的 spatial layout 就相當地雷同。\",\"Image from Yi-Hsuan Tsai et al. (2018)\"]},\"37\":{\"h\":\"pseudo labelling (or self-training)\",\"t\":[\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"最初是為了解決 半監督式學習(Semi-Supervise Learning, SSL) 而被提出的。\",\"Info\",\"所謂的半監督式學習也就是說 target domain 的資料上只有一些 labeled data，其他絕大多都是 unlabeled data，這種狀況下訓練模型就被稱為半監督式學習。\",\"而半監督式學習困難的點在於雖然對於 Target Domain 有部分的認知，但是並不全面。\",\"一個簡單的方法是想辦法給這些 unlabeled data 一些 pseudo label。那我們就可以用 supervise learning 的方法解決了。\",\"舉例來說，先在 labeled data 上訓練一個模型，透過這個模型我們就有辦法給 unlabeled data 做 prediction，而 prediction 的結果就當作是他的 pseudo label，就可以再拿去 fine-tune model 了。\",\"Image from Sylwia Majchrowska et al. (2021)\",\"但主要的問題來自於 Domain Shift，畢竟 Source Domain 和 Target Domain 還是存在差異的，並不是所有的 Target Data 都能夠透過 Source Data 去轉移出來。\",\"尤其在 Unsupervised Domain Adaption(UDA) 來說是相當大的問題，在 UDA 當中通常 Domain Shift 都會特別大。\",\"Info\",\"所謂的 UDA 也就是說我們對於 Target Domain 的資料不存在任何 label。換句話說，我們對於 Target Domain 缺乏 label 上的認知。\",\"對於 UDA 來說由於缺乏對於 Target Domain 的認識，一個常見的問題是產出的結果通常會傾向去預測結果為常見的 class。\",\"Info\",\"對陌生人的認識，往往先從貼標籤開始。\",\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的 class，如果出現道路或甚至機車，有可能就被誤判成人行道。或是汽車比卡車更常見，導致卡車時常被預測成汽車。\",\"Image from Yang Zou et al (2018)\",\"看 column 4，只有 pseudo labeling 的例子\",\"Info\",\"雖然已經有 paper 提出如 CBST 的方法來降低這種問題，但在邊界上往往還是難以有好的結果。\"]},\"38\":{\"h\":\"Mixing\",\"t\":[\"Mixing 基本上就是從 training image 拿出兩張，透過一些方式混在一起，產生一個新的 training image。最初被用於把 unlabeled image 混合成新的圖片，是一種 data augumentation 的技巧。\",\"像是 Mixup 這種 data augumentation 方法也是屬於 Mixing 的一種。\",\"DACS 當中使用的是 ClassMix 這種 Mixing 方法。\",\"具體來說，ClassMix 的步驟\",\"把兩個圖片 (A,B) 先轉成 semantic map (SA​,SB​)\",\"把 SA​ 其中一半的 classes 對應的 semantic map 做出一個 binary mask (M)\",\"把 mask M apply 在 A 上，跟 B 合成出 XA​。\",\"把 mask M apply 在 SA​ 上，跟 SB​ 合成出 XA​ 對應的 semantic map YA​\",\"Image from Viktor Olsson et al. (2020)\",\"這樣的做法有趣的是能夠將 semantic segmentation 在邊界上往往會出現誤差的問題解決。\",\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清。但透過剪貼則可以造成不同環境的突兀感，進而解決這個問題。因此這時候 pseudo labelling 就能夠比較好發揮作用。\",\"Image from Viktor Olsson et al. (2020)\"]},\"39\":{\"h\":\"Methodology\"},\"40\":{\"h\":\"Naive Mixing to UDA\",\"t\":[\"最 Naive 的做法就是照著 ClassMix 的方法，將 unlebelled dataset Mixing 成新的 dataset，把 labelled dataset 以及 mixed dataset 拿去訓練。\",\"Info\",\"在 UDA 當中，unlabelled dataset 就是 target domain dataset。\",\"Image from Wilhelm Tranheden at al.\",\"但是這種做法實際上效果很糟糕。像是 sidewalk 被預測成 road，rider 被預測成 person 之類的，許多的 class 都被其他 class 覆蓋。這樣的問題只在 target domain 上會發生，這跟前面提到只使用 pseudo labelling to UDA 會造成的問題是吻合的。\",\"Image from Wilhelm Tranheden at al.\",\"單純的 Naive Mixing 往往在邊界上會有許多誤判的 class\",\"Tips\",\"這種相似的 class 相鄰而導致的誤判被稱為 class conflation\"]},\"41\":{\"h\":\"Domain Adaption via Corss-domain mixed Sampling (DACS)\",\"t\":[\"DACS 的核心做法是不單只是跟 Target Domain 去 mixing，而是將 Source 跟 Target 一起 Mix。如此一來， Target Domain 以及 Source Domain 的關聯性就能被連結起來，降低 Domain Shift。\",\"Image from Wilhelm Tranheden at al.\",\"詳細的步驟具體來說\",\"從 Source Domain (DS​) 取出圖片與 lebel (XS​,YS​)\",\"從 Target Domain (DT​) 取出圖片 XT​\",\"透過 segmentation network fθ​ 取得 XT​ 的 pseudo label YT​^​\",\"將 (XS​,YS​),(XT​,YT​^​) 經過 ClassMix 得到 (XM​,YM​)\",\"把 (XS​,YS​),(XM​,YM​) 拿去訓練。\",\"Image from Wilhelm Tranheden at al.\",\"在 Loss 的設計上也相當直覺，就是希望 XS​ 的預測結果要接近 YS​，XM​ 的結果要接近 YM​。\",\"H: Cross-Entropy\",\"λ: 調整 Mixing 部分的影響程度\",\"L(θ)=E[H(fθ​(XS​),YS​)+λH(fθ​(XM​),YM​)]\"]},\"42\":{\"h\":\"Results\"},\"43\":{\"h\":\"實驗設定\",\"t\":[\"在 segmentation network 的設定上參考了許多過去的研究，選擇採用 DeepLab v2 搭配 ResNet101 作為 backbone。\",\"ResNet101 是 pretrained on ImageNet 跟 MSCOCO。而 Hyperparameter 的設定基本上跟 Yi-Hsuan Tsai et al. (2018) 一樣。\",\"在 Mixing 的方法上雖然任何 based on binary mask 的 Mixing 都可以使用，不過這裡最主要都是使用 ClassMix。\"]},\"44\":{\"h\":\"Dataset\",\"t\":[\"在 synthetic-to-real 有一些常見的 benchmarks。\",\"GTA5 -> Cityscapes\",\"SYNTHIA -> Cityscapes\",\"GTA5 以及 SYNTHIA 都是虛擬世界當中的影像，而 Cityscapes 則是現實世界當中的影像。\"]},\"45\":{\"h\":\"Cityscapes\",\"t\":[\"照片是在城市當中開車拍下的各種照片\",\"Image from Marius Cordts et al. (2016)\",\"2975 training images\",\"19 classes\"]},\"46\":{\"h\":\"GTA5\",\"t\":[\"照片是在 GTA5 下拍攝的\",\"Image from Stephan R. Richter et al.\",\"24966 synthetic training images\",\"19 classes \",\"可對應到 Cityscapes 的 classes\"]},\"47\":{\"h\":\"SYNTHIA\",\"t\":[\"照片是在 Unity 建構的 virtual city 下拍攝\",\"Image from GermanRos et al. (2016)\",\"9400 synthetic training images\",\"16(or 13) classes \",\"都會對到 Cityscapes 的 classes\",\"13 個 classes 的版本是少了 Wall, Fence, Pole\"]},\"48\":{\"h\":\"GTA5 -> Cityscapes\",\"t\":[\"Image from Wilhelm Tranheden at al.\",\"其他的 Model 都是 DeepLab-v2，他們選擇其中 Performance 最好的，但 Backbone 並不一定要是 ResNet 101\",\"Image from Wilhelm Tranheden at al.\",\"Source 是只有使用 source domain 去 train 的模型\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，只對簡單的 class 像是 Road, Build, Veg, Sky, Person, Car 這些普遍做得不錯的 class 有還不錯的 Performance\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 有點偏以及 Train 真的很糟\"]},\"49\":{\"h\":\"SYNTHIA -> Cityscapes\",\"t\":[\"考慮到 SYNTHIA 有些 paper 使用 16 個 classes，有些是 13 個 class 的版本，所以在數據上 mIoU 有兩列分別表示 13 個平均跟 16 個的平均。\",\"Image from Wilhelm Tranheden at al.\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，甚至對 Road 的 Performance 都不太好\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 頗偏\"]},\"50\":{\"h\":\"Some issues about evaluation\",\"t\":[\"他們認為在其他的 paper 有不少人最後給的結果之所以那麼好看是因為\",\"Cityscapes 並沒有 testset\",\"他們選擇用 validation set 判斷要不要 early stop，這個 validation set 也跟最後評估的 set 是一樣的\",\"針對 validation set 挑選 hyperparameters (?)\",\"所以他們認為這樣不太公平，畢竟在 Validation set 做得很棒不能直接表達在整體會表達很棒。 他們也試著用相同的手段訓練模型，然後拿到了\",\"GTA5 \",\"Baseline: 35.68% (+2.83%)\",\"DACS: 53.84% (+1.7%) (BEST)\",\"SYNTHIA \",\"DACS (13 classes): 55.98% (+1.17%) (1.02% to BEST)\",\"DACS (16 classes): 49.10% (+0.76%) (0.7% to BEST)\"]},\"51\":{\"h\":\"Contribution\",\"t\":[\"Apply SSL method on ClassMix to UDA\",\"Introduce a simple framework with high-performance\",\"Beat SOTA in GTA5 to Cityscape\"]},\"52\":{\"h\":\"值得一看的文章們\",\"t\":[\"【Day 24】半監督式學習（Semi-supervised Learning）（上）\",\"【Day 25】半監督式學習（Semi-supervised Learning）（下）\",\"Notes on “DACS: Domain Adaptation via Cross-domain Mixed Sampling”\",\"物件偵測的領域自適應 (Domain Adaptation)\",\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"Domain Adaptation in Computer Vision: Everything You Need to Know\",\"Semi-supervised semantic segmentation needs strong, varied perturbations\",\"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\",\"Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training\",\"Learning to Adapt Structured Output Space for Semantic Segmentation\"]},\"53\":{\"c\":[\"Note\"]},\"54\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"WACV\"]},\"55\":{\"h\":\"DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\"},\"56\":{\"h\":\"Basic Information\",\"t\":[\"Lukas Hoyer, Dengxin Dai, Luc Van Gool @ ETH Zurich & MPI for Informatics\",\"2022 CVPR\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"與過去的方法相比， DAFormer 在 UDA semantic segmentation 領域當中做出了劃時代的貢獻。\"]},\"57\":{\"h\":\"問題描述\",\"t\":[\"如同過去看過的 UDA 問題描述，這一篇同樣也是先說明了 semantic segmentation 在 UDA 上的重要性。由於標記 semantic segmentation labels 的成本過高，以致於開始將研究的方向轉向如 weak-supervised 或是 semi-supervised learning，最終則是 unsuvervised learning 的 UDA。\",\"在這一篇論文當中主要探討的是過去 UDA 的模型都是採用如 DeepLab 搭配 ResNet 或是 VGG 等架構，但是這些架構在 semantic segmentation 領域都已經是過時的產物，有許多新的架構可以得到更高的 mIoU。作者懷疑會不會其實我們應該要試著採用更好的 backbone 去訓練，可以得到更好的結果。\",\"不過直覺上，如果我們用更加強大的 backbone，那麼就會有更高的機會在 source domain 上 overfitting，因此這一篇 paper 的目標是在改採用更佳的 backbone 的同時，避免 overfitting 的問題。\"]},\"58\":{\"h\":\"Related Works\",\"t\":[\"Sematic Image Segmentation\",\"Unsupervised Domain Adaptation (UDA)\",\"Transformer\",\"Self-training\"]},\"59\":{\"h\":\"Methodology\"},\"60\":{\"h\":\"Self training for UDA\",\"t\":[\"一開始我們一樣先看一下這一篇論文當中會用到的 Notation 以及他對於 self training 的描述。這裡已經預設包含了 Knowledge Distillation。\",\"gθ​ 表示 student model\",\"hϕ​ 表示 teacher model\",\"NS​ 表示 Source Domain 的資料數量\",\"NT​ 表示 Target Domain 的資料數量\",\"XS​={xS(i)​}i=1NS​​ 表示 Source Domain 的資料\",\"XT​={xT(i)​}i=1NT​​ 表示 Target Domain 的資料\",\"YS​={yS(i)​}i=1NS​​ 表示 Source Domain 對應的 labels\",\"YT​={yT(i)​}i=1NT​​ 表示 Target Domain 對應的 labels，在 UDA 預設是不會知道的\",\"H,W 分別表示圖片的高寬\",\"YS​,YT​ 都具有 C 個共通的 classes\",\"最 Naive 的方法是把套上 Categorical Cross Entropy Loss (CCE Loss) 期待預測的 label 跟目標相同。\",\"LS(i)​=−j=1∑H×W​c=1∑C​yS(i,j,c)​loggθ​(xS(i)​)(j,c)\",\"然而這種方法的 performance 以及一般性都並不是很理想。Self training 的方法會使用 pseudo labelling，透過產生假想的 label 去學習。於是 pseudo label 就不是單純的 one-hot，而是包含了機率的概念，我們會選其中最大的當成是最後的答案 pT(i,j,c)​。\",\"pT(i,j,c)​=[c=argmaxc′​hϕ​(xT(i)​)(j,c′)]\",\"此外，我們也可以去定義當前 pseudo label 信心度的標準 qT(i)​。也就是說，會期待預測出來的 label 至少信心度要超過 τ，這樣的結果有多少。\",\"qT(i)​=H⋅W∑j=1H×W​[maxc′​hϕ​(xT(i)​)(j,c′)>τ]​\",\"Info\",\"這裡的 [⋅] 是 Iverson Bracket，只是單純符合條件給 1，否則給 0 的符號。\",\"[P]={10​ifPistrueotherwise​\",\"有了評斷信心水平的標準，就可以結合起來形成新的 Loss。\",\"LT(i)​=−j=1∑H×W​c=1∑C​qT(i)​pT(i,j,c)​loggθ​(xT(i)​)(j,c)\",\"也就是說我們會期待產生出來的 pseudo label 除了越準確越好，也會期待其信心水平也要是高的。\",\"Pseudo label 的產生方式可以是 offline 也可以是 online，這裡考慮到 online 的實作比較簡單，所以採用這個方法。與 ProDA 相同，根據過去的研究，這裡會採用 Exponential Moving Average (EMA) 去更新 teacher model。\",\"ϕt+1​←αϕt​+(1−α)θt​\",\"此外，student model 的訓練上也是使用 augumented data。包含了 DAFormer、Color Jitter、Gaussian Blur、ClassMix。\"]},\"61\":{\"h\":\"DAFormer Network Architecture\",\"t\":[\"首先，針對 backbone network 過於老舊的部分作者先透過一些實驗去尋找好的架構，他們後來發現 Transformer based model 會有更好的 mIoU。這裡選用的 Transformer 是 SegFormer。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Src-Only: 只訓練在 GTA dataset\",\"UDA: 使用 GTA dataset 作為 source domain 加上 UDA 方法 adapt Cityscape dataset\",\"Oracle: 直接使用 supervised learning 訓練 Cityscape dataset\",\"上述的三者分數都是以 Cityscape dataset 去評估取得 Rel 用來比較 UDA 在 Oracle 的 scale 下有多強。\",\"Rel=OracleUDA​\",\"可以發現到 SegFormer 的表現都比起其他架構來得好許多，並且有趣的是 DeepLabV3+ 並沒有得到比 DeepLabV2 更好的表現。\",\":::spoiler 更多關於模型選擇的實驗\",\"由於 backbone 實際上包含了 Encoder 以及 Decoder 兩個部分，作者進一步去分析究竟是哪一個部分使最後得到好的結果。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"可以發現到當 Encoder 不採用 MiT-B5 這種包含了 Transformer 的 encoder，得出的 performance 會有大量的下降，也就是說，Transformer 在這裡能夠提供更好的幫助。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"進一步去研究不同大小的 Encoder 會有怎樣的影響，可以發現到通常越大的模型能夠提供更好的效益。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"使用 Transformer based encoder 有另一個有趣的好處是，他可以很好地把不同的 classes 分開，即便這些 classes 有許多相像的地方。圖中圈起來的是各種交通工具，可以發現到 MiT-B5 可以有更好的 feature separation。\",\"此外，Transformer 當中包含的 self-attention 與傳統的 CNN 不同，即便在 testing 階段能夠動態地依據當下的輸入資料的相似性來產生對應的Affinity-map，再依據得到的Affinity-map做出預測。\",\"中文敘述參考 [論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\":::\",\"於是，根據實驗的結果我們選擇使用 SegFormer 做為新的 backbone。\",\"不過過去使用 Transformer based backbone 解決 semantic segmentation 通常會有個通病是在 decoder 的部分只能取得 local information。於是作者嘗試修改 decoder 的部分，把 encoder 給出不同 level 的 feature maps 處理成相同 channels 數量以及大小，再使用不同的 dilation rates 去處理。如下圖所示。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Training Strategies for UDA\",\"這一篇 paper 最主要的貢獻，就是解決了使用更好的 backbone 同時避免 overfitting source domain 的方法，具體而言有三個部分：Rare Class Sampling(RCS), Thing-Class ImageNet Feature Distance(FD), Learning Rate Warmup for UDA。以下就分別說明這三個部分的作法。\"]},\"62\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"作者在實驗的過程當中發現到 DAFormer 在 Rare Classes 的 performance 在不同 random seed 的情況下有很大的不同。作者認為這是因為若這些 rare classes 在訓練後期才出現，模型很可能已經被 common classes 干擾形成 bias，以致於難以 re-learn。\",\"於是，對於這些 rare classes，我們就希望讓他在訓練過程當中出現的頻率可以更高，也就有更高的機會可以學更多次、更早看到它。\",\"定義一個 source domain class c 出現的頻率 fc​ 如下。\",\"fc​=NS​⋅H⋅W∑i=1NS​​∑j=1H×W​[yS(i,j,c)​]​\",\"而一個 class c 被 sample 到的機率 P(c) 就可以用 softmax with temperature 去定義如下。\",\"P(c)=∑c′=1C​e1−fc′​/Te(1−fc​)/T​\",\"也就是說，我們會盡可能讓出現頻率越低的 class 有較高的機會被 sample 到。\"]},\"63\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"通常在 UDA 的 backbone 所使用的 semantic segmentation network 都會使用 ImageNet pretrained models 去初始化權重。我們理想上會預期那些 ImageNet 當中有包含的 classes 理應因此得到較好的結果。\",\"然而，如 train 和 bus 這兩個 classes 卻反而往往得到很糟糕的結果。並且透過觀察訓練過程作者發現到，其實在訓練初期其實是能夠辨別這些 classes 的，但卻隨著訓練過程慢慢地變糟。\",\"作者認為這是好的 features 都被 Loss function LS​ 搞壞所導致。\",\"因此，作者把這些 \\\"bottleneck features\\\" 拿出來，希望他們在 ImageNet 的 feature 以及訓練模型的 feature 之間的距離可以拉近，避免模型\\\"忘記\\\"這些 features。\",\"不過也考慮到 ImageNet 幾乎都是訓練在 Thing-Class 上，Stuff-Class 如 road, sky 就基本上沒有。因此這裡的拉近只會針對 Thing-Classes Cthings​ 處理。\",\"定義 xS(i)​ 的第 j 個 pixel 的 Feature Distance d(i,j) 如下。\",\"d(i,j)=∥FImageNet​(xS(i)​)(j)−Fθ​(xS(i)​)(j)∥2​\",\"定義 Mask Mthings(i,j)​ 如下。\",\"Mthings(i,j)​=c′=1∑C​yS,small(i,j,c′)​⋅[c′∈Cthings​]\",\"這裡的 yS,small(i,j,c′)​ 只是為了 downsample size，採用了 Average Pooling。\",\"yS,small(i,j,c′)​=[AvgPool(ySc​,H/HF​,W/WF​)>r]\",\"如此一來就能在 Loss 上多加上一項去 regularize。\",\"LFD(i)​=∑j​Mthings(i,j)​∑j=1HF​×WF​​d(i,j)⋅Mthings(i,j)​​\",\"於是乎最後的整體 Loss function 也就形成。\",\"L=LS​+LT​+λFD​LFD​\",\"λFD​ 是一個 hyperparameter。\"]},\"64\":{\"h\":\"Learning Rate Warmup for UDA\",\"t\":[\"過去訓練 CNN 或是 Transformer 都會習慣使用 linear learning rate warmup，這裡也加進來，他們透過實驗發現這很不錯。\",\"ηt​=ηbase​⋅t/twarm​\"]},\"65\":{\"h\":\"Results\"},\"66\":{\"h\":\"實驗設定\",\"t\":[\"在 Dataset 的使用上如同過去我們看過的 DACS 與 ProDA，都是採用 UDA 當中常見的 datasets：Cityscapes、GTA5、SYNTHIA。\",\"實作上採用了常見的 mmsegmentation framework，Network 的架構如同前面所述，encoder 採用 MiT-B5 encoder，decoder 的部分作者另外的調整時選用的 dilation rate 分別是 1, 6, 12, 18。Encoder 已經 pretrain 在 ImageNet-1K 上。\",\"至於詳細的 hyperparameter 設定請詳閱 paper。\"]},\"67\":{\"h\":\"Summary\",\"t\":[\"這裡先簡單總結一下。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"首先看到上面的表格，可以觀察到\",\"加上 Warmup 之後 performance 提升了約 6.4 mIoU (row 1 & 2)\",\"加上 RCS 之後 performance 提升了約 5.8 mIoU (row 2 & 4)\",\"加上 FD 之後 performance 提升了約 3.5 mIoU (row 2 & 6)\",\"加上 Warmup、RCS、FD 之後 performance 提升了約 14.4 mIoU (row 1 & 7)\",\"再多一點調整後可以再提升約 0.8 mIoU\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"作者也給出每個 classes 在加上不同的調整後得出的結果，可以看到所有 class 經過 DAFormer 都可以有獲得提升，甚至那些 rare classes 也變得能夠預測了。\"]},\"68\":{\"h\":\"Learning Rate Warmup\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"經過實驗後發現無論是採用 DeepLabV2 或是 SegFormer，如果搭配 Learning Rate Warmup 都對於 performance 有所提升。\"]},\"69\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"上圖展現出 Rider 和 Bicycle 這兩個 Class 預測的結果，在沒有使用 RCS 的情況下(藍色線)，IoU 的變化很大程度跟 Random Seed 的選用有關，這一點尤其在 Bicycle 最明顯。並且也可以觀察到那些比較早開始有所提升的 Random Seed 最後得到的 IoU 也會最大。\",\"因此作者認為這是跟圖片被 sample 到的時間有所相關，進而提出 RCS 去提升 Rare Class 被 Sample 的機率(橘色線)，可以發現搭配了 RCS 後，IoU 的變化就比較不與 Random Seed 的選擇相關，並且普遍最後的 IoU 都會高過於原本的狀況。\"]},\"70\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"在上圖的橘色線是原本的模型隨著訓練後對於不同 class 預測的 IoU 變化。可以觀察到 Train 這個類別居然會隨著訓練時間預測結果越糟糕，而最一開始的結果其實是還不錯的。\",\"作者認為這是因為 MiT-B5 太強，導致 overfit source domain，進而產生這樣的結果。\",\"透過加上 FD 之後，可以看到在綠色線的部分，成功避免了預測結果變差的狀況。\",\"此外，作者也注意到 Cityscapes 的圖片由於是透過車子上裝設攝影鏡頭去蒐集的，所以圖片底下的部分實際上並不是跟街景相關，而是自駕車車體。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"此外，在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示。\",\"Image from ResearchGate\",\"因此，作者進一步去忽略畫面上方 15 pixels 以及畫面下方 120 pixels 的 pseudo label。另外也考慮到 Transformer 的表達能力可以更強，進一步提高 α 到 0.999，最終得到更好的結果，如上面 summary 所示。\"]},\"71\":{\"h\":\"DAFormer Decoder\",\"t\":[\"Image from ResearchGate\",\"作者進一步去比較自己改良的 decoder 跟其他架構相比，發現到 DAFormer 搭配 Depthwise Separable Convolution 確實能夠得到好的結果。儘管 UperNet 在 Oracle 上可以得到較好的結果，但是在 UDA 上 DAFormer 仍然有更好的 performance。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"最終也可以看到，與過去的 SOTA 相較之下，DAFormer 成功在幾乎所有的 class 上 outperform 其他 SOTA，並且最終的 mIoU 與過去的 SOTA 都有相當大的改進。\"]},\"72\":{\"h\":\"Contribution\",\"t\":[\"研究不同的 backbone 架構對於 UDA performance 的影響\",\"成功將 Transformer 的成功帶進 UDA 領域 \",\"提出了三個方法避免 overfitting 的問題\",\"只需要一張 RTX 2080 Ti GPU 訓練 16 個小時，與過去的資源消耗相較減輕甚多\"]},\"73\":{\"h\":\"值得一看的文章們\",\"t\":[\"[論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\"[CVPR22] DAFormer: Improving Network Architectures for Domain-Adaptive Semantic Segmentation\",\"DAFormer Github\",\"DAFormer(CVPR2022)阅读笔记\",\"DAFormer Extension Paper\"]},\"74\":{\"c\":[\"Note\"]},\"75\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"CVPR\"]},\"76\":{\"h\":\"Playing Atari with Deep Reinforcement Learning\"},\"77\":{\"h\":\"Basic Information\",\"t\":[\"2013 NeurIPS\",\"Volodymyr Mnih, Koray Kavukcuoglu David Silver et al.\",\"這個論文提出的做法稱為 DQN(Deep Q-Networks)\"]},\"78\":{\"h\":\"問題描述\",\"t\":[\"過去在 RL 領域當中把一些 high-dimensional 的感官資料（如：視覺影像、語音資料等）作為 agent 的輸入去學習一直是一個很大的挑戰。然而我們也看到近幾年 Deep Learning 已經能夠在這種資料上去擷取特徵，進而去完成許多複雜的任務。\",\"所以「能不能把 Deep Learning 的成功也放進 RL 當中呢？」這樣的想法自然而然就出現了。\",\"不過從 Deep Learning 的角度來看 RL 的話，會有幾個明顯的問題。\",\"RL 的訓練資料（如：Reward）需要透過與環境互動取得，但數值範圍往往很 sparse，而且也往往會經過一段時間的延遲才取得 與 Deep Learning 相較之下，DL 的資料通常都會先 Label 好，可以直接把資料之間的關聯建構起來。\",\"RL 的訓練資料具有高度相關性 在 DL 當中我們會預設資料之間是沒有什麼相依性的，但在 RL 當中同一個 episode 的 state、action、reward 之間都會具有相當高的相關性。\",\"RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化 DL 往往假設資料的分布會維持住。\",\"這一篇論文成功將 CNN 應用在 RL 上，也避免了上述提及的幾個問題。\"]},\"79\":{\"h\":\"Related Works\",\"t\":[\"Q-Networks\",\"TD-gammon\",\"收斂性相關研究 \",\"Residual algorithms: Reinforcement learning with function approximation.\",\"Q-learning\",\"Neural fitted Q-learning (NFQ)\"]},\"80\":{\"h\":\"Q-Networks\",\"t\":[\"在 RL 當中我們會透過 MDP 去 model 整個問題，而 RL 的目標就是要讓整體的 reward 總和最大化。\",\"定義 optimal action value function Q∗ 如下\",\"Q∗(s,a)=πmax​E[Rt​∣st​=s,at​=a,π]\",\"也就是在 state s 採取 action a 並 follow policy π 得到的最大 return。其中 Return 的定義如下，這裡考慮有 discount 的版本。\",\"Rt​=t′=t∑T​γt′−trt′​\",\"γ 為 discount factor\",\"rt​ 表示在時間 t 取得的 reward\",\"既然 RL 的目的是要讓整體的 return 最大化，也就是要找到 Q∗ 了。Q-Network 就是用 Neural Network 來近似 Q∗，也就是要讓底下的 Loss 最小化。\",\"Li​(θi​)yi​​=Es,a∼ρ(⋅)​[(yi​−Q(s,a;θi​))2]=Es′∼ε​[r+γa′max​Q(s′,a′;θi−1​)]​\",\"需要特別注意到對於 θi​ 來說，他要去近似的是 θi−1​ 的模型得出來的結果，也就是說，Q-Network 透過固定訓練的目標(Target Network)，解決了前面提及的第三個問題「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"81\":{\"h\":\"TD-gammon\",\"t\":[\"一個 model-free RL 算法，透過一個 multi-layer perceptron 和一層 hidden layer 去預測 value function，成功在雙陸棋上面 outperform 人類。\",\"不過這裡的成功只停止在雙陸棋上，並無法繼續擴充到其他的領域。\"]},\"82\":{\"h\":\"收斂性相關研究\",\"t\":[\"過去的研究當中發現到如果是 model-free 搭配 non-linear function approximators 或是 off-policy learning 的話會導致 Q-network 發散，無法收斂。\",\"後續的研究中則發現到 Q-network 無法收斂的問題可以透過 gradient TD 舒緩，並且證明了底下兩個狀況是可以確保收斂。\",\"固定 policy，使用非線性的 approximator\",\"使用線性的 approximator 去學 control\",\"然而這些研究都並未能夠給出用非線性去學 control 的方法。\"]},\"83\":{\"h\":\"NFQ\",\"t\":[\"跟這一篇 paper 最相近的一個研究，他們會先透過 Computer Vision 的模型萃取出圖片的特徵，然後再把這些特徵丟去給 RL 訓練。\",\"不過 DQN 與 NFQ 不同的地方在於 DQN 是 end-to-end，也就是說可以直接從 visual input 去訓練，而 NFQ 不是。\"]},\"84\":{\"h\":\"Methodology\",\"t\":[\"在 TD-gammon 當中我們看到了使用 Neural Network 去學習 value function 有還不錯的成效，DQN 稍微修改了這個做法，將 Q-Network 和 Experience Replay 結合起來。\",\"Experience Replay 會將 agent 跟環境的互動過程當中的 experience 記錄在 replay memory D 當中。當要去更新模型的時候，我們是從 replay memory 當中取得隨機幾筆去更新。\",\"Info\",\"在時間 t 的 experience 包含了 state, action, reward, next state\",\"et​=(st​,at​,rt​,st+1​)\",\"因此 experience 就定義成\",\"D=e1​,e2​,…,eN​\",\"在經過 experience replay 之後，agent 會透過 ϵ-greedy 去選擇 action。\",\"實作上 experience 只會儲存最後 N 筆，並且 history 當中的 frames 只會取出最後 4 個，拿出來做一些 preprocess ϕ(s) 之後作為實際上儲存進 experience 的 state。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"透過 Q-Network 中的 Target Network 以及 Experience Replay，DQN 順利避免了最初提及的兩個問題「RL 的訓練資料具有高度相關性」以及「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"85\":{\"h\":\"Results\"},\"86\":{\"h\":\"實驗設定\",\"t\":[\"實驗做在 7 個 Atari games，Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest 以及 Space Invaders。\",\"每個遊戲的 reward 一開始都不太相同，實驗上調整了 reward 的大小，使得所有 positive reward 固定為 1；negative reward 為 −1，其餘則為 0 表示不影響。\",\"並且會使用 frame skipping 的技巧，讓 agent 只會每經過 k 個 frames 才會去擷取畫面，並且做出相對應的 action。至於那些被忽略的 frames，就持續上一個做出的 action。除了 Space Invaders 因為遊戲當中的雷射會跑很快，所以設定 k=3，其他遊戲則都是 k=4。\"]},\"87\":{\"h\":\"評估方式\",\"t\":[\"在 Deep Learning 當中如果要評估一個 Network 的好壞，可以單純透過觀察模型在 validation set 上的 performance 即可，但是在 RL 當中並沒有 validation set，因此評估一個 agent 的好壞就相對困難。\",\"過去會透過多次遊戲中 agent 獲得的 reward 平均去評估，也就是說理想上每經過一輪更新，模型能夠得到的 reward 應該要慢慢變大。不過作者發現在他們的模型得出來的結果往往會是很不穩定的。作者推測是因為權重即便只有小的變化也會對 policy distribution 有大的影響，導致接下來會經過的 state 就很不相同。\",\"因此作者改成 Q 的平均去評估，也確實發現會平滑許多。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\"]},\"88\":{\"h\":\"比較基準\",\"t\":[\"最後將 DQN 跟幾個 Baseline 去比較\",\"Sarsa \",\"On-control policy\",\"Linear approximator\",\"人工提取 features\",\"Contingency \",\"跟 Sarsa 類似，但有包含了部分的學習過程\",\"HNeat Best \",\"訓練的過程包含了一點專家系統的概念\",\"事先標記好 object 的位置以及類型\",\"HNeat Pixel \",\"訓練的過程包含了一點專家系統的概念\",\"事先處理好了 8 color channel representation\",\"Human\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"最後得出來的結果，DQN 在幾乎所有的遊戲當中都 outperform 所有的算法，證明了 DQN 的成功。\"]},\"89\":{\"h\":\"Contribution\",\"t\":[\"成功結合 Deep Learning 以及 Reinforcement Learning\",\"直接從 raw RGB 當作輸入，不需要事先經過其他的分解\",\"透過 Experience Replay 以及 Target Network 解決過去 Deep Learning 結合 RL 時訓練不佳的問題\"]},\"90\":{\"c\":[\"Note\"]},\"91\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"NeurIPS\"]},\"92\":{\"h\":\"Noisy Networks for Exploration\"},\"93\":{\"h\":\"Basic Information\",\"t\":[\"2018 ICLR\",\"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind\"]},\"94\":{\"h\":\"問題描述\",\"t\":[\"在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"例如在 Alpha Go 當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",\"Info\",\"論文中有提及在 Matthieu Geist, Olivier Pietquin (2014) 有提及一個使用 Neural Network 的方法，不過並沒有保證收斂，因此仍然沒有解決問題。\",\"因此這一篇論文提出一個方法試圖去消除 exploration 效率與品質的問題。\"]},\"95\":{\"h\":\"Related Works\",\"t\":[\"ϵ-greedy、Entropy Regularization\",\"Parameter Space Noise for Exploration\",\"用來加上 Noisy-Net 的各種 RL 架構 \",\"DQN\",\"Double-DQN\",\"Dueling DQN\",\"A3C\"]},\"96\":{\"h\":\"Parameter Space Noise for Exploration\",\"t\":[\"2017 年由 OpenAI 發表在 ICLR 的 paper。其方法與這一篇可說是大同小異。\",\"在過往的研究可以發現到說往往我們在設計讓 agent 有更多的 exploration 的時候都是透過增加 noise 來達成。\",\"Info\",\"舉例而言，ϵ-greedy 就是在 action space 上增加了 noise，讓選擇更多樣，以達成 exploration。\",\"而在 A3C 當中加上 Entropy Regularization，是在 Loss 上鼓勵 policy 的亂度越高越好，達到鼓勵 exploration 的效果。\",\"Image from OpenAI - Better exploration with parameter noise\",\"核心的概念很簡單，過去增加探索的方法大多都是在 action space 上增加 noise，而這裡則選擇在 parameter space 上增加 noise，並且達到了很棒的效果。\",\"Tips\",\"ϵ-greedy 就像是獵人裡面的凱特，行動之前需要先看運氣抽接下來使用的武器，即便自己知道當下用哪一個 action 比較好，卻會受到 ϵ 的限制。\",\"而在 parameter space 加上 noise 就像是可以換個角度去想其他人會怎麼做，試著用那一個人的做法走過一次，得到不同的經驗。\",\"相較之下，action space 加 noise 就比較像是在亂試，反之在 parameter space 上加 noise 就比較有系統性一些。\",\"具體來說就是他們試圖在 parameter 上加上 Gaussian Noise。\",\"θ~=θ+N(0,σ2I)\",\"Action Space Noise\",\"Parameter Space Noise\",\"Videos from OpenAI - Better exploration with parameter noise\",\"Warning\",\"底下的內容只是單純的 Review\"]},\"97\":{\"h\":\"DQN\",\"t\":[\"透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"於是他們定義了底下的 Loss function 去試圖得到 Q∗。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"D 是上一個 replay buffer 的 transition distribution \",\"state s\",\"action a\",\"reward r=R(s,a)\",\"probability y∼P(⋅∣s,a)\",\"θ− 是被固定的參數\",\"Tips\",\"DQN 帶來了幾個好處\",\"Experience Replay 透過儲存 Experience，更新參數是從 replay buffer 中隨意挑一筆，降低了資料之間的相關性，讓 Neural Network 訓練避免偏差。\",\"Target Network 避免了訓練目標經常地變動造成訓練效果差\",\"使用 Neural Network 替代 action value function 避免了 Q-Learning 的 table 維度過大訓練困難的問題\"]},\"98\":{\"h\":\"Double-DQN\",\"t\":[\"在 DQN 當中我們需要同時訓練兩個 model，也就是 θ 與 θ−。然而 DQN 的設定上對於目標被發現存在高估的問題，因此 Double-DQN 提出了解決這個高估問題的方法。\",\"原始 DQN 目標\",\"Double-DQN 目標\",\"r+γmaxb∈A​Q(y,b;θ−)\",\"r+γQ(y,maxb∈A​Q(y,b;θ);θ−)\",\"高估的狀況如底下的綠線。紫線是目標函數，橘線是綠線與紫線的誤差，不難發現到確實都存在高估的狀況。\",\"然而使用了 Double-DQN 之後的誤差(藍線)就小到幾乎不存在了。\",\"Image from Hado van Hasselt, Arthur Guez, David Silver (2015)\",\"Tips\",\"Double-DQN 帶來的幾個好處\",\"讓 DQN 高估的問題消失，有更好的效果\"]},\"99\":{\"h\":\"Dueling DQN\",\"t\":[\"Dueling DQN 的概念仍然是透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"與 DQN 不同的地方在於他並不是直接去學習 Q∗，而是另外定義了一個 Advantage functionA。\",\"A(s,a)=Q(s,a)−V(s)\",\"V(s) 就像是 baseline，表示著在當前這個 state s 底下你預期可以拿到多好的 return，所以 A(s,a) 意義上就是在看每個 action 有多好多壞。\",\"透過 V 和 A 的總和就能夠得到 Q。上圖就是在最後分開成兩個輸出結果 V 和 A，最後合併成 Q。\",\"Q(s,a;θ)=V(s;θV​)+A(s,a;θA​)\",\"Info\",\"在實務上為了避免像是 V(s) 都是 0，實際上跟 DQN 沒有差異的問題，因此細節上是還會對 A(s,a) 加上總和為 0 的限制。\",\"Q(s,a,θ,α,β)=V(a,θ,α)+​A(s,a,θ,β)−∣A∣1​a′∈∣A∣∑​A(s,a′,θ,β)​\",\"α,β 只是調整 V 和 A 兩部分影響程度的參數。\",\"把 Dueling DQN 搭配 Double DQN 之後可以得到底下的 Loss。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"Tips\",\"Dueling DQN 帶來了幾個好處\",\"使用 Advantage function 增加模型的更新與 exploration。\"]},\"100\":{\"h\":\"A3C\",\"t\":[\"在 DQN 當中使用了 Experience Reply 去避免訓練資料上的強關聯性，然而存在幾個缺點\",\"需要額外的 memory 去儲存 replay buffer\",\"需要 off-policy alogorithm，對於 online RL 來說可能導致收斂不穩定以及緩慢等問題\",\"A3C 的概念就如同火影忍者的影分身之術，讓每個分身在各自的環境當中訓練，訓練成效也就翻倍。\",\"Image from Arthur Juliani@Medium\",\"A3C 是使用 advantage actor-critic 的方式，會直接去學 policy 以及 value function。因此在參數上也就包含了兩項 θπ​ 以及 θv​ 分別表示 policy 以及 value function 的參數。考慮在時間 t 往後看 k 步的更新，參照 A3C 的論文，兩個參數的 Loss 計算分別如下。\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"A：Advantage function 也就是 A3C 當中的 R−V\",\"H：Entropy function 根據 A3C 的論文，加上這一項能夠促使模型更好 exploration\",\"β：調整 A 和 H 的影響程度\",\"Qi​：在時間 i 時於 state st+i​ 執行 policy π 的 return\",\"Note\",\"紅色的部分也就是前面提及的 entropy regularization\",\"最後整體的 Loss 也就如下\",\"L(θ)=Lπ(θπ​)+λLV(θv​)\",\"λ 用來調整兩個 Loss 的影響力\",\"Info\",\"Note：原始 A3C 論文中更新一步，並且沒有使用 entropy 的算式\",\"θ′θv′​​:dθ←dθ+∇θ′​logπ(ai​∣si​;θ′)(R−V(si​;θv′​)):dθv​←dθv​+∂(R−V(si​;θv′​))2/∂θv′​​\",\"Tips\",\"A3C 帶來了幾個好處\",\"降低訓練資料之間的關聯性 畢竟每個 agent 訓練的環境都不同，得到的資料也就不同\",\"能夠使用 on-policy 或是 off-policy，增加通用性\",\"可以平行化加速訓練\",\"穩定地訓練\"]},\"101\":{\"h\":\"Methodology\"},\"102\":{\"h\":\"基本想法\",\"t\":[\"Noisy-Net 的想法跟 Parameter Space Noise for Exploration 的想法基本上是相同方向，都是要對 parameter space 去加上 noise。\",\"作法上，對於每個可訓練的參數拆解成 ζ=(μ,σ)，然後再透過 zero-mean 的 ϵ 增加 noise。也就是說對於一個參數 θ 我們會寫成：\",\"θ=defμ+σ⊙ϵ\",\"所以對於一個 Linear Layer 來說\",\"y=wx+b⇒y=(μw+σw⊙ϵw)x+(μb+σb⊙ϵb)\",\"就只是這樣而已，不要想太多！\",\"Tips\",\"刻意挑 zero-mean 的 noise 是為了採用底下的特性方便後續 Loss 的計算。\",\"Lˉ(ζ)=E[L(θ)]\",\"因此\",\"∇Lˉ(ζ)=∇E[L(θ)]=E[∇μ,Σ​L(μ+Σ⊙ϵ)]\",\"Σ 包含了所有 σ\",\"加上了 Monte-Carlo approximation 之後，可以用單一的 sample ξ 去近似\",\"∇Lˉ(ζ)≈∇μ,Σ​L(μ+Σ⊙ξ)\",\"跟 OpenAI 提出的方法略為不同的地方在於他並不是直接對 network 的參數加上 Gaussian Noise，而是給了參數 ϵw,ϵb 去決定要加怎樣的 noise。\",\"在每一個 episode 開始之前先把參數加上 noise，接下來這一整個 episode 就都是用這個 network 去訓練，意即在過程中不會對 noise 做調整。\"]},\"103\":{\"h\":\"減少產 random number 時間\",\"t\":[\"這樣的做法下每一個 episode 都需要 random noise 在 weight 和 bias 上。假如 w∈Rq×p,b∈Rq，那麼 ϵw∈Rq×p,ϵb∈Rq，也就意味著需要 random 出 pq+q 個數值。\",\"上面基本的做法作者稱他為 Independent Gaussian noise，而接下來作者給出一個 Factorised Gaussian noise 的做法。\",\"基本上就是將 random number 拆分\",\"ϵi,jw​ϵjb​​=f(ϵi​)f(ϵj​)=f(ϵj​)​\",\"其中 f(x)=sgn(x)∣x∣​,ϵi​∈Rq,ϵj​∈Rp。\",\"如此一來，只需要產出 p+q 個 random number 也可以達到類似的效果。\"]},\"104\":{\"h\":\"DQN & Dueling DQN\",\"t\":[\"由於 DQN 和 Dueling DQN 是在 single-thread 上訓練，因此上述的 Random Overhead 會比較大，在這裡採用 Factorised Gaussian noise。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 ϵ-greedy 了。\",\"原本的 DQN 對 Loss 的定義如下。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γb∈Amax​Q(y,b,ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\",\"最外層的期望值是對 ϵ 和 ϵ′\",\"同樣地也可以對 Dueling DQN 做修改。原本的定義為\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γQ(y,argb∈Amax​Q(y,b,ϵ′′;ζ),ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\"]},\"105\":{\"h\":\"Distributed A3C\",\"t\":[\"由於 A3C 是在 multi-thread 上訓練，因此不太需要考慮上述的 Random Overhead，在這裡採用 Independent Gaussian noise 即可。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 Entropy function 了。\",\"原本的 A3C\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"NoisyNet-A3C\",\"∇ζπ​​Lπ(ζπ​)LV(ζv​)​=−E[Eπ[i=0∑k​∇ζπ​​log(π(at+i​∣st+i​;ζπ​,ϵ))A(st+i​,at+i​;ζv​,ϵ)]]=E[i=0∑k​Eπ[(Qi​−V(st+i​;ζv​,ϵ))∣st+i​]]2​\",\"Noise initialize details\",\"Independent Gaussian noise\",\"μi,j​∼U[−p3​​,+p3​​]\",\"p 是 input 的數量\",\"σi,j​=0.017\",\"Factorised Gaussian noise\",\"μi,j​∼U[−p1​​,+p1​​]\",\"p 是 input 的數量\",\"σi,j​=p​0.5​\"]},\"106\":{\"h\":\"Results\"},\"107\":{\"h\":\"Experiments\",\"t\":[\"實驗是做在 57 Atari games 上。每 1M 個 frames 評估一次，episode 每 108K frames 會 truncate 一次。將沒有做任何修正的 DQN、Dueling DQN、A3C 作為 Baseline。\",\"首先把 Baseline 以及加上 NoisyNet 的模型都跟 Human 比較，底下是用來評估優劣的評分方式。\",\"100×ScoreHuman​−ScoreRandom​Scoreagent​−ScoreRandom​​\",\"Note\",\"最後得出的結果為 0：跟 Random 一樣糟 最後得出的結果為 100：跟 Human 一樣好\",\"可以從分數上明顯看出來加上了 NoisyNet 後對於 Mean 以及 Median 都有正面的影響。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"接下來評估加上 NoisyNet 帶來的影響力，評分方式會也跟 Baseline 比較。\",\"100×max(ScoreHuman​,ScoreBaseline​)−ScoreRandom​Scoreagent​−ScoreBaseline​​\",\"可以看到在大多數的遊戲加上了 NoisyNet 之後的結果都有些進步。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"不過進步主要在 DQN 以及 Dueling 上較為顯著。A3C 的部分在退步也是有幾項退步蠻多，也並不是每次加上 NoisyNet 都會帶來 improvement。\",\"從訓練中的曲線也可以明顯看到 NoisyNet 可以帶來很不錯的 improvement。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"108\":{\"h\":\"Analysis\",\"t\":[\"為了進一步去釐清這樣的做法為什麼是可行、合理的，作者進一步去研究。\",\"回顧一下我們加上 Noise 的方法，是把一個可訓練參數拆成 ζ=(μ,σ)，再額外多一個 Noise ϵ。\",\"θ=defμ+σ⊙ϵ\",\"理想上，我們最後的 Loss 應該要能夠好好收斂，也就是說最後的 solution 應該要是 deterministic。那麼這裡加上的 ϵ 就應該隨著訓練慢慢被忽視，作用只在於訓練的前中期提供 exploration。因此，我們也就會期待 σ 這個參數會漸漸趨近於 0 了！\",\"定義底下的平均\",\"Σˉ=Nweights​1​i∑​∣σiw​∣\",\"作者發現在每一個遊戲當中最後一個 Layer 的 Σˉ 都是會逐漸趨近於 0 的，然而若觀察倒數第二個 Layer 卻並不一定了，有些甚至是遞增的。也就是說，其實 NoisyNet 並不會都得出 deterministic solution。\",\"此外，透過 Σˉ 的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同，也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"109\":{\"h\":\"Contribution\",\"t\":[\"提供一個簡單又有效的 Exploration 方式\",\"能夠在 on-policy 以及 off-policy 上適用\",\"能夠輕易地套用在所有的 RL 算法當中\"]},\"110\":{\"h\":\"值得一看的文章們\",\"t\":[\"强化学习中on-policy 与off-policy有什么区别？\",\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\",\"Asynchronous Methods for Deep Reinforcement Learning\",\"Deep Exploration via Randomized Value Functions\",\"Kalman Temporal Differences\",\"VIME: Variational Information Maximizing Exploration\",\"Parameter Space Noise for Exploration\",\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"Better exploration with parameter noise\",\"强化学习中的探索与利用（count-based)\"]},\"111\":{\"c\":[\"Note\"]},\"112\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"]},\"113\":{\"h\":\"Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\"},\"114\":{\"h\":\"Basic Information\",\"t\":[\"Pan Zhang1, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen @ University of Science and Technology of China, Microsoft Research Asia\",\"2021 CVPR\"]},\"115\":{\"h\":\"問題描述\",\"t\":[\"如同前面看過的 DACS，這一篇 paper 也是想要解決 semantic segmentation 當中 UDA(Unsupervised Domain Adaption) 的問題。\",\"近年來流行 self-training 方法，透過 Pseudo Labelling 的方式來處理。也就是說會在訓練的過程當中透過當下的預測給這些 training data 一個假的 label，然後再拿去訓練。雖然這種做法開始能夠讓 source domain 適應 target domain 了，但比起 supervised learning 與 semi-supervised learning 的 performance 還是相差許多。\",\"作者認為目前的做法存在兩個問題\",\"只選擇信度高於某個閥值的預測作為 pseudo label，但結果不一定正確，會使模型被誤導 例如下圖，圈起來的 - 就被錯誤分類。\",\"Image modified from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"由於 Domain Gap 很大，Network 傾向在 target domain 上產生比較雜亂的特徵 就像是看到有殼的動物就當成是昆蟲，把四隻腳站立的動物都當成草食類動物一樣。認知跟實際充滿巨大的 Gap 導致對結果的特徵很雜亂。\",\"針對上述兩個問題，作者分別使用 Online pseudo labels denoising 以及 learning a compact target structure 來解決。\"]},\"116\":{\"h\":\"Related Works\",\"t\":[\"UDA\",\"Unsupervised representation learning\",\"Learning from noisy labels\",\"Self-training\"]},\"117\":{\"h\":\"Methodology\"},\"118\":{\"h\":\"Preliminary\",\"t\":[\"這裡先定義一下接下來會用到的基本 Notation。\",\"ns​,nt​ 分別表示 source 和 target dataset 的大小\",\"Xs​={xs​}j=1ns​​,Xt​={xt​}j=1nt​​ 分別表示 source 和 target dataset\",\"Ys​={ys​}j=1ns​​,Yt​={yt​}j=1nt​​ 分別表示 source 和 target dataset 對應的 segmentation labels\",\"y^​t​ 表示 pseudo label\",\"Ys​ 和 Yt​ 都有 K 個共通的 classes\"]},\"119\":{\"h\":\"Target\",\"t\":[\"semantic segmentation 當中的 UDA 問題，目標在於給定 Xs​,Xt​,Ys​，不知道 Yt​ 的前提下，去預測 target dataset 的 semantic segmentation。\",\"其中一種方法是採用 Pseudo Labels，透過如 Cross Entropy Loss 來調整模型的機率分布。\",\"lcet​=−i=1∑H×W​k=1∑K​y^​t(i,k)​log(pt(i,k)​)\",\"其中 pt(i,k)​ 是一個 softmax probability 表示 pixel xt(i)​ 是 class k 的機率。 至於 y^​t(i,k)​ 則會直接表示屬於哪一個 class，也就是 hard labels。也額外定義 ξ 來轉換 soft 與 hard labels。\",\"y^​t(i,k)​={1,0,​if k=argmaxk′​pt(i,k′)​otherwise​,yt​^​=ξ(pt​)\",\"而一個 network h 也可以拆成 feature extractor f 以及 classifier g 兩個部分，用 h=f∘g 來表示。\"]},\"120\":{\"h\":\"Prototypical pseudo label denoising\",\"t\":[\"作者認為每經過一個 training stage 才去更新 pseudo label 會太慢，在一個 training stage 當中 network 可能已經 overfit 在那些充滿噪點的 labels，被錯誤的資訊誤導了。\",\"很直覺地，會想要讓 network 的參數更新、pseudo label 的更新 兩個可以同時處理。\",\"然而，若直接同時更新的話，network 會很容易忽略了細部的特徵，進而傾向 overfit 在 source domain，只在 source domain 獲得高的分數。\",\"因此作者提出的方法是將 soft pseudo labels 固定住，對於每個 class k 選擇一個 prototype η(k) 以及一個對應的 weight wt(i,k)​。訓練過程中根據與 prototype 之間的距離去調整 weight，進而影響預測的 pseudo label。\",\"y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​)\",\"wt(i,k)​ 就是上述的 weight\",\"pt,0(i,k)​ 與過去的 soft pseudo label pt,(i,k)​ 稍有不同，整個訓練過程中都會固定住\",\":::success 跟 Clustering 頗類似，每個 cluster 的中心點就如同這裡的 prototype，距離 cluster A 中心點越近，模型就越相信他是屬於 cluster A。\",\"我們會隨著訓練過程慢慢調整 prototype，讓他越來越貼合真實的狀況。\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"當然，這裡的距離是投射到高維空間之後 feature 之間的距離！ :::\"]},\"121\":{\"h\":\"權重計算\",\"t\":[\"權重的計算方式如下\",\"wt(i,k)​=∑k′​exp(−∥f~​(xt​)(i)−η(k′)∥/τ)exp(−∥f~​(xt​)(i)−η(k)∥/τ)​\",\"f(xt​)(i) 表示第 i 筆 target data 的 feature\",\"f~​(xt​)(i) 是 momentum encoder，可以看成是更新較慢的 encoder\",\"η(k) 表示 class k 的特徵中心點\",\"τ 表示 softmax temperature，這裡設為 1\",\"權重的計算方式本質上就是 softmax function。計算的是 feature 跟每個 class 的中心點 η(k) 距離遠近。\",\"當距離很大時，產出的權重就會很小\",\"當距離很小時，產出的權重就會很大\"]},\"122\":{\"h\":\"prototype 計算\",\"t\":[\"而 prototype 的計算方式如下\",\"η(k)=∑xt​∈Xt​​∑i​1(y^​t(i,k)​==1)∑xt​∈Xt​​∑i​f(xt​)(i)∗1(y^​t(i,k)​==1)​\",\"Prototype 的計算本質上就是找中心點。把所有對應到 class k 的 feature 加總後平均。\",\"然而這種做法每次要更新 prototype 就需要看過整個 target dataset 的所有 features，計算上負擔過大。因此作者改用一個 mini-batch 當中中心點的 moving average 來估計 (Exponential Moving Average, EMA)。\",\"η(k)←λη(k)+(1−λ)η′(k)\",\"η′(k) 表示當前 mini-batch 當中 class k 的 feature 平均\",\"λ=0.9999\"]},\"123\":{\"h\":\"Loss 計算\",\"t\":[\"至此我們有了新的方法取得 pseudo labels (也就是 y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​))，最後就剩下更新的 Loss 如何計算。\",\"與傳統的 Cross Entropy(CE) 不同，這裡作者採用 Symmetric Cross Entropy(SCE) 試圖增加對 lebel noise 的容忍程度。\",\"lscet​=αlce​(pt​,y^​t​)+βlce​(y^​t​,pt​)\",\"α=0.1\",\"β=1\",\":::success 改成透過 prototype 去調整 pseudo label 能夠帶來許多的好處\",\"對 outlier 比較不敏感\",\"每個 class 都是平等的，較不會因為 class 的不平衡導致預測錯誤 \",\"在 semantic segmentation 中這一點尤其重要，因為 class 的分布往往分散\",\"實際上對於 hard class 的預測有改善\",\"對於一開始預測錯誤的 pseudo labels 能夠漸漸改正 :::\"]},\"124\":{\"h\":\"Structure learning by enforcing consistency\",\"t\":[\"理想上，只要我們的 feature extractor 能好好表示出 feature，即便在 target domain 上也能好好地區分不同的 class，那麼 pseudo label 也就可以更好地減輕 noise 的影響。\",\"然而因為 Domain Adaption 尤其 UDA 對於 target domain 的認識嚴重缺乏，encode 出來的 features 往往會很分散。\",\"作者透過對所擁有的 target domain 知識增強來改善，並且分成了弱增強 T(xt​) 以及 強增強 T′(xt​)。實際上，弱增強只是給原圖，強增強只是加上 data augumentation。\",\"這裡使用的 Data Augumentation 包含了旋轉、明暗調整、彩度調整等\",\"我們的目標是要讓 T(xt​) 與 T′(xt​) 對應的 feature 可以比較接近。作者分別去計算兩者的 weight zT​ 與 zT′​ (稱為 Soft prototypical assignment)，試圖讓他們產出的分布要越接近越好。因此透過 KL divergence 去計算 loss lklt​。\",\"lklt​=KL(zT​∥zT′​)\",\"其中\",\"zT(i,k)​=∑k′​exp(−∥f~​(T(xt​))(i)−η(k′)∥/τ)exp(−∥f~​(T(xt​))(i)−η(k)∥/τ)​,zT′(i,k)​=∑k′​exp(−∥f(T(xt​))(i)−η(k′)∥/τ)exp(−∥f(T(xt​))(i)−η(k)∥/τ)​\",\"兩者差異只在於使用的 encoder 分別是 f 和 f~​。因為 zT​ 是由弱增強得到，受到的干擾較少，因此適合用來教 encoder 經過強增強的 prototype assignment 應該與經過弱增強的相同。\",\"ProDA\",\":::success 如此一來，就能夠迫使模型對於這些略有不同的 feature 具有相同的 pseudo label，使得 target domain features 更加密集。 :::\",\"最後，為了避免所謂的 degeneration issue，也就是有些 cluster 是空的狀態，作者進一步設計一個 loss lregt​ 鼓勵類別盡量地平均。\",\"lregt​=−i=1∑H×W​j=1∑K​logpt(i,k)​\",\"將上述的種種 loss 結合，合併成底下的 loss ltotal​。\",\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",\"γ1​=10\",\"γ2​=0.1\"]},\"125\":{\"h\":\"Distillation to self-supervised model\",\"t\":[\"最後，作者進一步加上了 Knowledge Distillation，使最終的結果進一步提昇。\",\"透過前面的步驟得到的模型稱為 h，會做為 KD 當中的 Teacher Model。\",\"要訓練的 Student Model 稱為 h†。會採用 SimCLRv2 的 pretrain weights 開始訓練。\",\"為了避免 student model 忘記 source domain 的知識，所以也會把 source domain 的資料拿進來使用。整體的 Loss lKD​ 計算方式如下。\",\"lKD​=lces​(ps​,py​)+lcet​(pt†​,ξ(pt​))+βKL(pt​∥pt†​)\"]},\"126\":{\"h\":\"整體流程\",\"t\":[\"整體流程被分成三個階段。\",\"第一階段包含了 Prototypical Pseudo Label Denoising 以及 Target Structure Learning。 目的是要先訓練出一個 Teacher Model。意即讓 ltotoal​ 收斂。\",\"ProDA_Losses\",\"第二與第三階段都是 Knowledge Distillation。 目的是要訓練出一個 Student Model。意即讓 lKD​ 收斂。\",\"ProDA_Losses2\",\"Caution\",\"根據 Appendix 附上的 algorithm，實際上他所謂的 避免模型忘記 source domain 只不過是把 source data 扔進 \\\"teacher model\\\"，取得 lces​(ps​,py​)，然後拿去 tune \\\"student model\\\"。\",\"但是從頭到尾都沒丟給 student model，為甚麼可以直接拿去 tune，並且預期能夠讓 student model \\\"學會 source domain\\\" 的資料?\"]},\"127\":{\"h\":\"Results\"},\"128\":{\"h\":\"實驗設定\",\"t\":[\"Segmentation 模型採用 DeepLabv2 搭配 ResNet-101 Backbone。\",\"訓練前首先透過 AdaptSegNet 搭配對抗式學習對 segmentation 模型 warmup。\",\"Knowledge Distillation 的部分採用了 pretrained SimCLRv2 搭配 ResNet-101 backbone。\",\"Dataset 的部分一如既往採用 GTA5、SYNTHIA、Cityscapes 這三個 datasets。之前在 DACS 也有介紹過同樣的 datasets。\",\"接下來直接看 ProDA 在兩個 benchmarks 上面的表現。\"]},\"129\":{\"h\":\"GTA5 → \\\\rightarrow → Cityscapes\",\"t\":[\"在 GTA5 → Cityscapes 的部分明顯可以看到最後的 mIOU 比起過去的 SOTA models 好許多。在絕大多數的類別當中也是比起過去的做法還要強。\",\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升。因為我們現在對每一個 classes 都是平等對待所導致。\",\"image\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"上半部分是用 domain alignment 的解決方案，下面則是 self-training。\"]},\"130\":{\"h\":\"SYNTHIA → \\\\rightarrow → Cityscapes\",\"t\":[\"在 SYNTHIA → Cityscapes 的部分如下，同樣可以看到 mIoU 比起過去的 SOTA 有不少的提升。\",\"image\"]},\"131\":{\"h\":\"Contribution\",\"t\":[\"提出一個可以即時修正 psuedo label 的方法(prototypes)\",\"展示知識蒸餾(Knowledge Distillation)在 UDA 上同樣可以獲得更多改善\",\"提出一個新的 UDA for semantic segmentation 的 SOTA model ProDA\"]},\"132\":{\"h\":\"值得一看的文章們\",\"t\":[\"［論文筆記］Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\",\"UDA 論文 HackMD 筆記\",\"剖析深度學習 (2)：你知道Cross Entropy和KL Divergence代表什麼意義嗎？談機器學習裡的資訊理論\",\"【读】领域自适应语义分割 - ProDA\",\"論文筆記 — Momentum Contrast for Unsupervised Visual Representation Learning(MOCO)\",\"Momentum Contrast for Unsupervised Visual Representation Learning\",\"Symmetric Cross Entropy\"]},\"133\":{\"c\":[\"Note\"]},\"134\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"CVPR\"]},\"135\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"领域自适应语义分割\",{\"1\":{\"132\":1}}],[\"读\",{\"1\":{\"132\":1}}],[\"談機器學習裡的資訊理論\",{\"1\":{\"132\":1}}],[\"剖析深度學習\",{\"1\":{\"132\":1}}],[\"展示知識蒸餾\",{\"1\":{\"131\":1}}],[\"比起過去的\",{\"1\":{\"129\":1,\"130\":1}}],[\"比較不敏感\",{\"1\":{\"123\":1}}],[\"比較\",{\"1\":{\"107\":2}}],[\"比較好\",{\"1\":{\"96\":1}}],[\"比較基準\",{\"0\":{\"88\":1}}],[\"比較差的結果\",{\"1\":{\"21\":1}}],[\"比較傾向去試試看那些不熟的\",{\"1\":{\"6\":1}}],[\"比較大的時候\",{\"1\":{\"6\":1}}],[\"→\",{\"0\":{\"129\":2,\"130\":2},\"1\":{\"129\":1,\"130\":1}}],[\"學會\",{\"1\":{\"126\":1}}],[\"學習\",{\"1\":{\"19\":1}}],[\"扔進\",{\"1\":{\"126\":1}}],[\"附上的\",{\"1\":{\"126\":1}}],[\"第二與第三階段都是\",{\"1\":{\"126\":1}}],[\"第一階段包含了\",{\"1\":{\"126\":1}}],[\"第一個能夠在所有\",{\"1\":{\"27\":1}}],[\"收斂\",{\"1\":{\"126\":2}}],[\"收斂性相關研究\",{\"0\":{\"82\":1},\"1\":{\"79\":1}}],[\"整體流程被分成三個階段\",{\"1\":{\"126\":1}}],[\"整體流程\",{\"0\":{\"126\":1}}],[\"整體的\",{\"1\":{\"125\":1}}],[\"整個訓練過程中都會固定住\",{\"1\":{\"120\":1}}],[\"整個問題\",{\"1\":{\"80\":1}}],[\"要訓練的\",{\"1\":{\"125\":1}}],[\"要傾向\",{\"1\":{\"14\":2}}],[\"鼓勵類別盡量地平均\",{\"1\":{\"124\":1}}],[\"鼓勵去探索那些在\",{\"1\":{\"6\":1}}],[\"受到的干擾較少\",{\"1\":{\"124\":1}}],[\"稱為\",{\"1\":{\"124\":1,\"125\":1}}],[\"彩度調整等\",{\"1\":{\"124\":1}}],[\"明暗調整\",{\"1\":{\"124\":1}}],[\"弱增強只是給原圖\",{\"1\":{\"124\":1}}],[\"強增強只是加上\",{\"1\":{\"124\":1}}],[\"強增強\",{\"1\":{\"124\":1}}],[\"知識增強來改善\",{\"1\":{\"124\":1}}],[\"較不會因為\",{\"1\":{\"123\":1}}],[\"較不具有彈性\",{\"1\":{\"22\":1}}],[\"試圖讓他們產出的分布要越接近越好\",{\"1\":{\"124\":1}}],[\"試圖增加對\",{\"1\":{\"123\":1}}],[\"試著用那一個人的做法走過一次\",{\"1\":{\"96\":1}}],[\"←λη\",{\"1\":{\"122\":1}}],[\"∗1\",{\"1\":{\"122\":1}}],[\"∑xt​∈xt​​∑i​f\",{\"1\":{\"122\":1}}],[\"∑k−1​rk​\",{\"1\":{\"15\":1}}],[\"∑k−1​1\",{\"1\":{\"15\":1}}],[\"產出的權重就會很大\",{\"1\":{\"121\":1}}],[\"產出的權重就會很小\",{\"1\":{\"121\":1}}],[\"產生一個新的\",{\"1\":{\"38\":1}}],[\"產生出選擇\",{\"1\":{\"13\":1}}],[\"∥\",{\"1\":{\"121\":2,\"124\":4}}],[\"∥2​\",{\"1\":{\"63\":1}}],[\"權重的計算方式本質上就是\",{\"1\":{\"121\":1}}],[\"權重的計算方式如下\",{\"1\":{\"121\":1}}],[\"權重計算\",{\"0\":{\"121\":1}}],[\"距離遠近\",{\"1\":{\"121\":1}}],[\"距離\",{\"1\":{\"120\":1}}],[\"頗類似\",{\"1\":{\"120\":1}}],[\"頗偏\",{\"1\":{\"49\":1}}],[\"稍有不同\",{\"1\":{\"120\":1}}],[\"稍微修改了這個做法\",{\"1\":{\"84\":1}}],[\"稍微翻了一下\",{\"1\":{\"3\":1}}],[\"η\",{\"1\":{\"120\":1,\"121\":2,\"122\":4}}],[\"ηt​=ηbase​⋅t\",{\"1\":{\"64\":1}}],[\"獲得高的分數\",{\"1\":{\"120\":1}}],[\"獲得的\",{\"1\":{\"87\":1}}],[\"若直接同時更新的話\",{\"1\":{\"120\":1}}],[\"若已經又經過\",{\"1\":{\"19\":1}}],[\"才去更新\",{\"1\":{\"120\":1}}],[\"才會去擷取畫面\",{\"1\":{\"86\":1}}],[\"則會直接表示屬於哪一個\",{\"1\":{\"119\":1}}],[\"則是現實世界當中的影像\",{\"1\":{\"44\":1}}],[\"認知跟實際充滿巨大的\",{\"1\":{\"115\":1}}],[\"很直覺地\",{\"1\":{\"120\":1}}],[\"很大\",{\"1\":{\"115\":1}}],[\"很多時候我們並不會直接去蒐集真實的資料\",{\"1\":{\"33\":1}}],[\"圈起來的\",{\"1\":{\"115\":1}}],[\"適應\",{\"1\":{\"115\":1}}],[\"適應不同的環境\",{\"1\":{\"24\":1}}],[\"强化学习中的探索与利用\",{\"1\":{\"110\":1}}],[\"强化学习中on\",{\"1\":{\"110\":1}}],[\"与off\",{\"1\":{\"110\":1}}],[\"方式\",{\"1\":{\"109\":1}}],[\"方法\",{\"1\":{\"38\":1,\"61\":1,\"115\":1}}],[\"方法也是屬於\",{\"1\":{\"38\":1}}],[\"理想上\",{\"1\":{\"108\":1,\"124\":1}}],[\"理應因此得到較好的結果\",{\"1\":{\"63\":1}}],[\"回顧一下我們加上\",{\"1\":{\"108\":1}}],[\"合併成底下的\",{\"1\":{\"124\":1}}],[\"合理的\",{\"1\":{\"108\":1}}],[\"合成出\",{\"1\":{\"38\":2}}],[\"評分方式會也跟\",{\"1\":{\"107\":1}}],[\"評估一次\",{\"1\":{\"107\":1}}],[\"評估方式\",{\"0\":{\"87\":1}}],[\"現在替換成\",{\"1\":{\"104\":2}}],[\"現在\",{\"1\":{\"104\":1,\"105\":1}}],[\"拆分\",{\"1\":{\"103\":1}}],[\"拆開來解決\",{\"1\":{\"22\":1}}],[\"拆開來\",{\"1\":{\"12\":1}}],[\"假如\",{\"1\":{\"103\":1}}],[\"減少產\",{\"0\":{\"103\":1}}],[\"意即讓\",{\"1\":{\"126\":2}}],[\"意即在過程中不會對\",{\"1\":{\"102\":1}}],[\"意義上就是在看每個\",{\"1\":{\"99\":1}}],[\"≈∇μ\",{\"1\":{\"102\":1}}],[\"ξ\",{\"1\":{\"102\":1,\"119\":1,\"125\":1}}],[\"∇ζπ​​lπ\",{\"1\":{\"105\":1}}],[\"∇μ\",{\"1\":{\"102\":1}}],[\"∇lˉ\",{\"1\":{\"102\":2}}],[\"∇θπ​​lπ\",{\"1\":{\"100\":1,\"105\":1}}],[\"ζv​\",{\"1\":{\"105\":3}}],[\"ζπ​\",{\"1\":{\"105\":2}}],[\"ζ−\",{\"1\":{\"104\":2}}],[\"ζ\",{\"1\":{\"102\":3,\"104\":5}}],[\"ζ=\",{\"1\":{\"102\":1,\"108\":1}}],[\"刻意挑\",{\"1\":{\"102\":1}}],[\"σˉ\",{\"1\":{\"108\":2}}],[\"σˉ=nweights​1​i∑​∣σiw​∣\",{\"1\":{\"108\":1}}],[\"σi\",{\"1\":{\"105\":2}}],[\"σ​l\",{\"1\":{\"102\":2}}],[\"σ\",{\"1\":{\"102\":3,\"108\":2}}],[\"σ2i\",{\"1\":{\"96\":1}}],[\"穩定地訓練\",{\"1\":{\"100\":1}}],[\"∂θv\",{\"1\":{\"100\":1}}],[\"紅色的部分也就是前面提及的\",{\"1\":{\"100\":1}}],[\"執行\",{\"1\":{\"100\":1}}],[\"參照\",{\"1\":{\"100\":1}}],[\"步的更新\",{\"1\":{\"100\":1}}],[\"往後看\",{\"1\":{\"100\":1}}],[\"往往會很分散\",{\"1\":{\"124\":1}}],[\"往往會有許多我們沒有的\",{\"1\":{\"33\":1}}],[\"往往假設資料的分布會維持住\",{\"1\":{\"78\":1}}],[\"往往在邊界上會有許多誤判的\",{\"1\":{\"40\":1}}],[\"往往先從貼標籤開始\",{\"1\":{\"37\":1}}],[\"往往就會\",{\"1\":{\"34\":1}}],[\"∈∣a∣∑​a\",{\"1\":{\"99\":1}}],[\"∈cthings​\",{\"1\":{\"63\":1}}],[\"沒有差異的問題\",{\"1\":{\"99\":1}}],[\"底下是用來評估優劣的評分方式\",{\"1\":{\"107\":1}}],[\"底下你預期可以拿到多好的\",{\"1\":{\"99\":1}}],[\"底下的內容只是單純的\",{\"1\":{\"96\":1}}],[\"藍線\",{\"1\":{\"98\":1}}],[\"藍色線\",{\"1\":{\"69\":1}}],[\"橘線是綠線與紫線的誤差\",{\"1\":{\"98\":1}}],[\"橘色線\",{\"1\":{\"69\":1}}],[\"紫線是目標函數\",{\"1\":{\"98\":1}}],[\"紫色圓點表示\",{\"1\":{\"22\":1}}],[\"原本的定義為\",{\"1\":{\"104\":1}}],[\"原本的\",{\"1\":{\"104\":1,\"105\":1}}],[\"原始\",{\"1\":{\"98\":1,\"100\":1}}],[\"原先\",{\"1\":{\"17\":1}}],[\"維度過大訓練困難的問題\",{\"1\":{\"97\":1}}],[\"替代\",{\"1\":{\"97\":1}}],[\"帶來的影響力\",{\"1\":{\"107\":1}}],[\"帶來的幾個好處\",{\"1\":{\"98\":1}}],[\"帶來的效益\",{\"1\":{\"25\":1}}],[\"帶來了幾個好處\",{\"1\":{\"97\":1,\"99\":1,\"100\":1}}],[\"∼d​\",{\"1\":{\"97\":1,\"99\":1,\"104\":4}}],[\"∼ε​\",{\"1\":{\"80\":1}}],[\"加總後平均\",{\"1\":{\"122\":1}}],[\"加\",{\"1\":{\"96\":1}}],[\"加上這一項能夠促使模型更好\",{\"1\":{\"100\":1}}],[\"加上總和為\",{\"1\":{\"99\":1}}],[\"加上了\",{\"1\":{\"22\":1,\"102\":1}}],[\"加上了一個\",{\"1\":{\"15\":1}}],[\"加上\",{\"1\":{\"22\":1,\"61\":1,\"67\":4,\"96\":1}}],[\"加上不同的偏好\",{\"1\":{\"13\":1}}],[\"核心的概念很簡單\",{\"1\":{\"96\":1}}],[\"達到鼓勵\",{\"1\":{\"96\":1}}],[\"年由\",{\"1\":{\"96\":1}}],[\"架構\",{\"1\":{\"95\":1}}],[\"架構對於\",{\"1\":{\"72\":1}}],[\"效率與品質的問題\",{\"1\":{\"94\":1}}],[\"論文\",{\"1\":{\"132\":1}}],[\"論文中更新一步\",{\"1\":{\"100\":1}}],[\"論文中有提及在\",{\"1\":{\"94\":1}}],[\"論文筆記\",{\"1\":{\"61\":1,\"73\":1,\"132\":2}}],[\"證明了\",{\"1\":{\"88\":1}}],[\"事先處理好了\",{\"1\":{\"88\":1}}],[\"事先標記好\",{\"1\":{\"88\":1}}],[\"類似\",{\"1\":{\"88\":1}}],[\"人工提取\",{\"1\":{\"88\":1}}],[\"人類\",{\"1\":{\"81\":1}}],[\"人類的\",{\"1\":{\"3\":2}}],[\"即可\",{\"1\":{\"87\":1,\"105\":1}}],[\"即便自己知道當下用哪一個\",{\"1\":{\"96\":1}}],[\"即便在\",{\"1\":{\"61\":1,\"124\":1}}],[\"即便在參數比較異常的狀況下仍然能有很不錯的學習成果\",{\"1\":{\"24\":1}}],[\"即便這些\",{\"1\":{\"61\":1}}],[\"即便這個環境設定是相當簡單的\",{\"1\":{\"22\":1}}],[\"即便\",{\"1\":{\"24\":1,\"36\":1}}],[\"順利避免了最初提及的兩個問題\",{\"1\":{\"84\":1}}],[\"ϕ\",{\"1\":{\"84\":1}}],[\"ϕt+1​←αϕt​+\",{\"1\":{\"60\":1}}],[\"筆記\",{\"1\":{\"132\":1}}],[\"筆\",{\"1\":{\"84\":1,\"121\":1}}],[\"記錄在\",{\"1\":{\"84\":1}}],[\"固定住\",{\"1\":{\"120\":1}}],[\"固定為\",{\"1\":{\"86\":1}}],[\"固定\",{\"1\":{\"82\":1}}],[\"舒緩\",{\"1\":{\"82\":1}}],[\"發表在\",{\"1\":{\"96\":1}}],[\"發散\",{\"1\":{\"82\":1}}],[\"發現到\",{\"1\":{\"71\":1}}],[\"算法當中\",{\"1\":{\"109\":1}}],[\"算法\",{\"1\":{\"81\":1}}],[\"既然\",{\"1\":{\"80\":1}}],[\"既然有\",{\"1\":{\"8\":1}}],[\"總和最大化\",{\"1\":{\"80\":1}}],[\"總之\",{\"1\":{\"19\":1}}],[\"應該與經過弱增強的相同\",{\"1\":{\"124\":1}}],[\"應該要是\",{\"1\":{\"108\":1}}],[\"應該要能夠好好收斂\",{\"1\":{\"108\":1}}],[\"應該要慢慢變大\",{\"1\":{\"87\":1}}],[\"應該要給\",{\"1\":{\"3\":1}}],[\"應用在\",{\"1\":{\"78\":1}}],[\"好許多\",{\"1\":{\"129\":1}}],[\"好\",{\"1\":{\"78\":1}}],[\"能好好表示出\",{\"1\":{\"124\":1}}],[\"能不能把\",{\"1\":{\"78\":1}}],[\"能夠漸漸改正\",{\"1\":{\"123\":1}}],[\"能夠帶來許多的好處\",{\"1\":{\"123\":1}}],[\"能夠輕易地套用在所有的\",{\"1\":{\"109\":1}}],[\"能夠使用\",{\"1\":{\"100\":1}}],[\"能夠使模型更好去學習各自的\",{\"1\":{\"12\":1}}],[\"能夠在\",{\"1\":{\"109\":1}}],[\"能夠在所有的\",{\"1\":{\"25\":1}}],[\"能夠在上述\",{\"1\":{\"5\":1}}],[\"能夠上\",{\"1\":{\"22\":1}}],[\"能夠調整自己的\",{\"1\":{\"16\":1}}],[\"能夠盡可能去正確探索環境\",{\"1\":{\"3\":1}}],[\"語音資料等\",{\"1\":{\"78\":1}}],[\"視覺影像\",{\"1\":{\"78\":1}}],[\"阅读笔记\",{\"1\":{\"73\":1}}],[\"研究不同的\",{\"1\":{\"72\":1}}],[\"仍然有更好的\",{\"1\":{\"71\":1}}],[\"儘管\",{\"1\":{\"71\":1}}],[\"α=0\",{\"1\":{\"123\":1}}],[\"α\",{\"1\":{\"70\":1,\"99\":3}}],[\"αt​\",{\"1\":{\"6\":1}}],[\"另外也考慮到\",{\"1\":{\"70\":1}}],[\"另一方面\",{\"1\":{\"13\":1,\"22\":1,\"24\":1}}],[\"成功結合\",{\"1\":{\"89\":1}}],[\"成功在雙陸棋上面\",{\"1\":{\"81\":1}}],[\"成功在幾乎所有的\",{\"1\":{\"71\":1}}],[\"成功將\",{\"1\":{\"72\":1}}],[\"成功避免了預測結果變差的狀況\",{\"1\":{\"70\":1}}],[\"成新的\",{\"1\":{\"40\":1}}],[\"太強\",{\"1\":{\"70\":1}}],[\"變化\",{\"1\":{\"70\":1}}],[\"後對於\",{\"1\":{\"107\":1}}],[\"後續的研究中則發現到\",{\"1\":{\"82\":1}}],[\"後\",{\"1\":{\"69\":1}}],[\"後可以得到更好的成效\",{\"1\":{\"24\":1}}],[\"預測的\",{\"1\":{\"70\":1}}],[\"預測的結果\",{\"1\":{\"69\":1}}],[\"預設是不會知道的\",{\"1\":{\"60\":1}}],[\"甚至那些\",{\"1\":{\"67\":1}}],[\"甚至對\",{\"1\":{\"49\":1}}],[\"設定請詳閱\",{\"1\":{\"66\":1}}],[\"設定如下\",{\"1\":{\"6\":1}}],[\"已經能夠在這種資料上去擷取特徵\",{\"1\":{\"78\":1}}],[\"已經\",{\"1\":{\"66\":1}}],[\"已知\",{\"1\":{\"14\":1}}],[\"幾乎都是訓練在\",{\"1\":{\"63\":1}}],[\"忘記\",{\"1\":{\"63\":1,\"125\":1}}],[\"希望他們在\",{\"1\":{\"63\":1}}],[\"希望改善這兩個對\",{\"1\":{\"3\":1}}],[\"搞壞所導致\",{\"1\":{\"63\":1}}],[\"被錯誤的資訊誤導了\",{\"1\":{\"120\":1}}],[\"被\",{\"1\":{\"62\":1,\"69\":1}}],[\"被預測成\",{\"1\":{\"40\":2}}],[\"出\",{\"1\":{\"103\":1}}],[\"出現的頻率\",{\"1\":{\"62\":1}}],[\"出來的\",{\"1\":{\"12\":1,\"124\":1}}],[\"干擾形成\",{\"1\":{\"62\":1}}],[\"數量以及大小\",{\"1\":{\"61\":1}}],[\"數量相同\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"給出不同\",{\"1\":{\"61\":1}}],[\"給你學習\",{\"1\":{\"13\":1}}],[\"通常在\",{\"1\":{\"63\":1}}],[\"通常會有個通病是在\",{\"1\":{\"61\":1}}],[\"通常並不會差太多\",{\"1\":{\"36\":1}}],[\"根據\",{\"1\":{\"100\":1,\"126\":1}}],[\"根據實驗的結果我們選擇使用\",{\"1\":{\"61\":1}}],[\"根據過去的研究\",{\"1\":{\"60\":1}}],[\"再額外多一個\",{\"1\":{\"108\":1}}],[\"再多一點調整後可以再提升約\",{\"1\":{\"67\":1}}],[\"再使用不同的\",{\"1\":{\"61\":1}}],[\"再依據得到的affinity\",{\"1\":{\"61\":1}}],[\"再次說明了\",{\"1\":{\"25\":1}}],[\"階段能夠動態地依據當下的輸入資料的相似性來產生對應的affinity\",{\"1\":{\"61\":1}}],[\"圖中圈起來的是各種交通工具\",{\"1\":{\"61\":1}}],[\"圖片中以\",{\"1\":{\"24\":1}}],[\"他要去近似的是\",{\"1\":{\"80\":1}}],[\"他可以很好地把不同的\",{\"1\":{\"61\":1}}],[\"他們會先透過\",{\"1\":{\"83\":1}}],[\"他們透過實驗發現這很不錯\",{\"1\":{\"64\":1}}],[\"他們後來發現\",{\"1\":{\"61\":1}}],[\"他們也試著用相同的手段訓練模型\",{\"1\":{\"50\":1}}],[\"他們選擇用\",{\"1\":{\"50\":1}}],[\"他們選擇其中\",{\"1\":{\"48\":1}}],[\"他們認為在其他的\",{\"1\":{\"50\":1}}],[\"他們並不是採用\",{\"1\":{\"21\":1}}],[\"他們將\",{\"1\":{\"6\":1}}],[\"進一步提高\",{\"1\":{\"70\":1}}],[\"進一步去研究不同大小的\",{\"1\":{\"61\":1}}],[\"進而影響預測的\",{\"1\":{\"120\":1}}],[\"進而影響到結果\",{\"1\":{\"22\":1}}],[\"進而傾向\",{\"1\":{\"120\":1}}],[\"進而去完成許多複雜的任務\",{\"1\":{\"78\":1}}],[\"進而產生這樣的結果\",{\"1\":{\"70\":1}}],[\"進而提出\",{\"1\":{\"69\":1}}],[\"進而解決這個問題\",{\"1\":{\"38\":1}}],[\"進而使得整體訓練採用的\",{\"1\":{\"13\":1}}],[\"訓練前首先透過\",{\"1\":{\"128\":1}}],[\"訓練過程中根據與\",{\"1\":{\"120\":1}}],[\"訓練的環境都不同\",{\"1\":{\"100\":1}}],[\"訓練的過程包含了一點專家系統的概念\",{\"1\":{\"88\":2}}],[\"訓練成效也就翻倍\",{\"1\":{\"100\":1}}],[\"訓練避免偏差\",{\"1\":{\"97\":1}}],[\"訓練\",{\"1\":{\"61\":1,\"72\":1,\"83\":1}}],[\"包含了旋轉\",{\"1\":{\"124\":1}}],[\"包含了所有\",{\"1\":{\"102\":1}}],[\"包含了\",{\"1\":{\"60\":1,\"84\":1}}],[\"包含了幾個重要的部分\",{\"1\":{\"5\":1}}],[\"否則給\",{\"1\":{\"60\":1}}],[\"至此我們有了新的方法取得\",{\"1\":{\"123\":1}}],[\"至於\",{\"1\":{\"119\":1}}],[\"至於那些被忽略的\",{\"1\":{\"86\":1}}],[\"至於詳細的\",{\"1\":{\"66\":1}}],[\"至少信心度要超過\",{\"1\":{\"60\":1}}],[\"至今平均的\",{\"1\":{\"14\":1}}],[\"至今被嘗試的次數\",{\"1\":{\"14\":1}}],[\"信心度的標準\",{\"1\":{\"60\":1}}],[\"期待預測的\",{\"1\":{\"60\":1}}],[\"避免了\",{\"1\":{\"97\":1}}],[\"避免了訓練目標經常地變動造成訓練效果差\",{\"1\":{\"97\":1}}],[\"避免模型忘記\",{\"1\":{\"126\":1}}],[\"避免模型\",{\"1\":{\"63\":1}}],[\"避免\",{\"1\":{\"57\":1}}],[\"避免太大或是太小\",{\"1\":{\"6\":1}}],[\"領域當中把一些\",{\"1\":{\"78\":1}}],[\"領域當中做出了劃時代的貢獻\",{\"1\":{\"56\":1}}],[\"領域\",{\"1\":{\"72\":1}}],[\"領域都已經是過時的產物\",{\"1\":{\"57\":1}}],[\"等\",{\"1\":{\"94\":1}}],[\"等架構\",{\"1\":{\"57\":1}}],[\"等不同的做法\",{\"1\":{\"36\":1}}],[\"物件偵測的領域自適應\",{\"1\":{\"52\":1}}],[\"挑選\",{\"1\":{\"50\":1}}],[\"針對上述兩個問題\",{\"1\":{\"115\":1}}],[\"針對\",{\"1\":{\"50\":1,\"61\":1}}],[\"針對不同的\",{\"1\":{\"7\":1}}],[\"判斷要不要\",{\"1\":{\"50\":1}}],[\"考慮在時間\",{\"1\":{\"100\":1}}],[\"考慮到\",{\"1\":{\"49\":1}}],[\"考慮有限的\",{\"1\":{\"8\":1}}],[\"真的很糟\",{\"1\":{\"48\":1}}],[\"除了越準確越好\",{\"1\":{\"60\":1}}],[\"除了\",{\"1\":{\"48\":1,\"49\":1,\"86\":1}}],[\"除了展現驚人的成果以外\",{\"1\":{\"25\":1}}],[\"單純用\",{\"1\":{\"48\":1,\"49\":1}}],[\"單純的\",{\"1\":{\"15\":1,\"40\":1}}],[\"建構的\",{\"1\":{\"47\":1}}],[\"照片是在\",{\"1\":{\"46\":1,\"47\":1}}],[\"照片是在城市當中開車拍下的各種照片\",{\"1\":{\"45\":1}}],[\"照著這樣的想法\",{\"1\":{\"13\":1}}],[\">r\",{\"1\":{\"63\":1}}],[\">τ\",{\"1\":{\"60\":1}}],[\">\",{\"0\":{\"48\":1,\"49\":1},\"1\":{\"44\":2}}],[\"作用只在於訓練的前中期提供\",{\"1\":{\"108\":1}}],[\"作法上\",{\"1\":{\"102\":1}}],[\"作為\",{\"1\":{\"43\":1,\"61\":1,\"78\":1,\"107\":1}}],[\"作者進一步加上了\",{\"1\":{\"125\":1}}],[\"作者進一步設計一個\",{\"1\":{\"124\":1}}],[\"作者進一步去研究\",{\"1\":{\"108\":1}}],[\"作者進一步去比較自己改良的\",{\"1\":{\"71\":1}}],[\"作者進一步去忽略畫面上方\",{\"1\":{\"70\":1}}],[\"作者進一步去分析究竟是哪一個部分使最後得到好的結果\",{\"1\":{\"61\":1}}],[\"作者分別去計算兩者的\",{\"1\":{\"124\":1}}],[\"作者分別使用\",{\"1\":{\"115\":1}}],[\"作者透過對所擁有的\",{\"1\":{\"124\":1}}],[\"作者發現在每一個遊戲當中最後一個\",{\"1\":{\"108\":1}}],[\"作者推測是因為權重即便只有小的變化也會對\",{\"1\":{\"87\":1}}],[\"作者把這些\",{\"1\":{\"63\":1}}],[\"作者認為每經過一個\",{\"1\":{\"120\":1}}],[\"作者認為目前的做法存在兩個問題\",{\"1\":{\"115\":1}}],[\"作者認為這是因為\",{\"1\":{\"70\":1}}],[\"作者認為這是因為若這些\",{\"1\":{\"62\":1}}],[\"作者認為這是好的\",{\"1\":{\"63\":1}}],[\"作者認為在這兩者都有一個共通點\",{\"1\":{\"23\":1}}],[\"作者在實驗的過程當中發現到\",{\"1\":{\"62\":1}}],[\"作者在實驗當中發現如果採用\",{\"1\":{\"17\":1}}],[\"作者懷疑會不會其實我們應該要試著採用更好的\",{\"1\":{\"57\":1}}],[\"作者將\",{\"1\":{\"23\":1,\"24\":1}}],[\"作者也注意到\",{\"1\":{\"70\":1}}],[\"作者也給出每個\",{\"1\":{\"67\":1}}],[\"作者也觀察了在幾款遊戲訓練過程中當中\",{\"1\":{\"24\":1}}],[\"作者也發現到\",{\"1\":{\"22\":1}}],[\"作者也發現如果把\",{\"1\":{\"22\":1}}],[\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子\",{\"1\":{\"12\":1}}],[\"作者比較兩個模型在不同\",{\"1\":{\"22\":1}}],[\"作者建構一個簡單的\",{\"1\":{\"22\":1}}],[\"部分的影響程度\",{\"1\":{\"41\":1}}],[\"調整\",{\"1\":{\"41\":1,\"100\":1}}],[\"λ=0\",{\"1\":{\"122\":1}}],[\"λfd​\",{\"1\":{\"63\":1}}],[\"λ\",{\"1\":{\"41\":1,\"100\":1}}],[\"得出的\",{\"1\":{\"61\":1}}],[\"得到的資料也就不同\",{\"1\":{\"100\":1}}],[\"得到的最大\",{\"1\":{\"80\":1}}],[\"得到不同的經驗\",{\"1\":{\"96\":1}}],[\"得到\",{\"1\":{\"41\":1}}],[\"得以有不同程度的影響\",{\"1\":{\"27\":1}}],[\"經過強增強的\",{\"1\":{\"124\":1}}],[\"經過實驗後發現無論是採用\",{\"1\":{\"68\":1}}],[\"經過\",{\"1\":{\"41\":1,\"67\":1}}],[\"經過相同的\",{\"1\":{\"36\":1}}],[\"取出圖片\",{\"1\":{\"41\":1}}],[\"取出圖片與\",{\"1\":{\"41\":1}}],[\"取得的\",{\"1\":{\"80\":1}}],[\"取得\",{\"1\":{\"19\":1,\"41\":1,\"126\":1}}],[\"取得不同的\",{\"1\":{\"9\":1}}],[\"詳細的步驟具體來說\",{\"1\":{\"41\":1}}],[\"覆蓋\",{\"1\":{\"40\":1}}],[\"許多的\",{\"1\":{\"40\":1}}],[\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清\",{\"1\":{\"38\":1}}],[\"先轉成\",{\"1\":{\"38\":1}}],[\"先在\",{\"1\":{\"37\":1}}],[\"像是\",{\"1\":{\"38\":1,\"40\":1,\"48\":1}}],[\"像是直接有一台車會去蒐集真實街景資料\",{\"1\":{\"33\":1}}],[\"混合成新的圖片\",{\"1\":{\"38\":1}}],[\"拿出來做一些\",{\"1\":{\"84\":1}}],[\"拿出來\",{\"1\":{\"63\":1}}],[\"拿出兩張\",{\"1\":{\"38\":1}}],[\"拿去訓練\",{\"1\":{\"40\":1,\"41\":1}}],[\"拿來比較加上\",{\"1\":{\"24\":1}}],[\"看\",{\"1\":{\"37\":1}}],[\"缺乏\",{\"1\":{\"37\":1}}],[\"換句話說\",{\"1\":{\"37\":1}}],[\"畢竟每個\",{\"1\":{\"100\":1}}],[\"畢竟在\",{\"1\":{\"50\":1}}],[\"畢竟\",{\"1\":{\"37\":1}}],[\"了\",{\"1\":{\"37\":1,\"80\":1,\"104\":1,\"105\":1,\"108\":1,\"115\":1}}],[\"舉例而言\",{\"1\":{\"96\":1}}],[\"舉例來說\",{\"1\":{\"37\":1}}],[\"舉一個在\",{\"1\":{\"33\":1}}],[\"舉一個例子來說\",{\"1\":{\"13\":1}}],[\"半監督式學習\",{\"1\":{\"37\":1,\"52\":2}}],[\"天空之類的就通常會像是在半空中\",{\"1\":{\"36\":1}}],[\"天生就有一些自己的偏好\",{\"1\":{\"13\":1}}],[\"號誌\",{\"1\":{\"36\":1}}],[\"行動之前需要先看運氣抽接下來使用的武器\",{\"1\":{\"96\":1}}],[\"行人都還是會跟地板黏在一起\",{\"1\":{\"36\":1}}],[\"行為模式改變\",{\"1\":{\"15\":1}}],[\"汽車\",{\"1\":{\"36\":1}}],[\"同樣可以看到\",{\"1\":{\"130\":1}}],[\"同樣地也可以對\",{\"1\":{\"104\":1}}],[\"同樣以自駕車的例子來說\",{\"1\":{\"36\":1}}],[\"同時避免\",{\"1\":{\"61\":1}}],[\"同時也能在\",{\"1\":{\"25\":1}}],[\"同時也可以看到\",{\"1\":{\"25\":1}}],[\"依照\",{\"1\":{\"36\":1}}],[\"依照過去\",{\"1\":{\"6\":1}}],[\"做修改\",{\"1\":{\"104\":1}}],[\"做調整\",{\"1\":{\"102\":1}}],[\"做為新的\",{\"1\":{\"61\":1}}],[\"做得很棒不能直接表達在整體會表達很棒\",{\"1\":{\"50\":1}}],[\"做出一個\",{\"1\":{\"38\":1}}],[\"做\",{\"1\":{\"37\":1}}],[\"做對抗式學習\",{\"1\":{\"36\":1}}],[\"做了兩個小修正\",{\"1\":{\"16\":1}}],[\"還是相差許多\",{\"1\":{\"115\":1}}],[\"還是存在差異的\",{\"1\":{\"37\":1}}],[\"還是\",{\"1\":{\"36\":1}}],[\"還要強\",{\"1\":{\"24\":1}}],[\"結合\",{\"1\":{\"89\":1,\"124\":1}}],[\"結合起來\",{\"1\":{\"84\":1}}],[\"結果相當糟糕\",{\"1\":{\"34\":1}}],[\"結束\",{\"1\":{\"19\":1}}],[\"直接從\",{\"1\":{\"89\":1}}],[\"直接使用\",{\"1\":{\"61\":1}}],[\"直接把訓練在虛擬環境的模型應用在真實環境\",{\"1\":{\"34\":1}}],[\"直到\",{\"1\":{\"19\":1}}],[\"擅自用自己的思維解讀\",{\"1\":{\"34\":1}}],[\"轉變到\",{\"1\":{\"34\":1}}],[\"影像分割\",{\"1\":{\"34\":1}}],[\"近年來流行\",{\"1\":{\"115\":1}}],[\"近年來透過\",{\"1\":{\"34\":1}}],[\"近似\",{\"1\":{\"12\":2}}],[\"讓他越來越貼合真實的狀況\",{\"1\":{\"120\":1}}],[\"讓選擇更多樣\",{\"1\":{\"96\":1}}],[\"讓\",{\"1\":{\"86\":1,\"97\":1,\"98\":1}}],[\"讓我們得以用較低的成本在虛擬環境中訓練模型\",{\"1\":{\"33\":1}}],[\"讓每個分身在各自的環境當中訓練\",{\"1\":{\"100\":1}}],[\"讓每個\",{\"1\":{\"13\":1}}],[\"降低訓練資料之間的關聯性\",{\"1\":{\"100\":1}}],[\"降低了資料之間的相關性\",{\"1\":{\"97\":1}}],[\"降低\",{\"1\":{\"33\":1,\"41\":1}}],[\"想解決的就是盡可能地將\",{\"1\":{\"33\":1}}],[\"到的時間有所相關\",{\"1\":{\"69\":1}}],[\"到的機率\",{\"1\":{\"62\":1}}],[\"到\",{\"1\":{\"33\":1,\"62\":1,\"70\":1}}],[\"環境與虛擬世界有差距\",{\"1\":{\"33\":1}}],[\"環境中取得\",{\"1\":{\"19\":1}}],[\"虛擬世界\",{\"1\":{\"33\":1}}],[\"投射到同一個平面上\",{\"1\":{\"33\":1}}],[\"所有的算法\",{\"1\":{\"88\":1}}],[\"所示\",{\"1\":{\"70\":1}}],[\"所使用的\",{\"1\":{\"63\":1}}],[\"所以也會把\",{\"1\":{\"125\":1}}],[\"所以對於一個\",{\"1\":{\"102\":1}}],[\"所以設定\",{\"1\":{\"86\":1}}],[\"所以\",{\"1\":{\"78\":1,\"99\":1}}],[\"所以圖片底下的部分實際上並不是跟街景相關\",{\"1\":{\"70\":1}}],[\"所以採用這個方法\",{\"1\":{\"60\":1}}],[\"所以他們認為這樣不太公平\",{\"1\":{\"50\":1}}],[\"所以在數據上\",{\"1\":{\"49\":1}}],[\"所謂的半監督式學習也就是說\",{\"1\":{\"37\":1}}],[\"所謂的\",{\"1\":{\"33\":1,\"37\":1}}],[\"所在的位置\",{\"1\":{\"22\":1}}],[\"共同發表\",{\"1\":{\"32\":1}}],[\"查爾摩斯理工大學\",{\"1\":{\"32\":1}}],[\"<>\",{\"1\":{\"28\":1}}],[\"值得一看的文章們\",{\"0\":{\"28\":1,\"52\":1,\"73\":1,\"110\":1,\"132\":1}}],[\"值得一提的是\",{\"1\":{\"22\":1}}],[\"使最終的結果進一步提昇\",{\"1\":{\"125\":1}}],[\"使得\",{\"1\":{\"124\":1}}],[\"使得所有\",{\"1\":{\"86\":1}}],[\"使得同類型的資料會相近\",{\"1\":{\"33\":1}}],[\"使模型具有更好的普遍性\",{\"1\":{\"27\":1}}],[\"使用線性的\",{\"1\":{\"82\":1}}],[\"使用非線性的\",{\"1\":{\"82\":1}}],[\"使用\",{\"1\":{\"9\":1,\"49\":1,\"61\":2,\"97\":1,\"99\":1}}],[\"使用不同的\",{\"1\":{\"9\":1}}],[\"自適應不同環境\",{\"1\":{\"27\":1}}],[\"自己調整\",{\"1\":{\"24\":1}}],[\"解決過去\",{\"1\":{\"89\":1}}],[\"解決\",{\"1\":{\"61\":1}}],[\"解決訓練不穩定\",{\"1\":{\"27\":1}}],[\"解決了前面提及的第三個問題\",{\"1\":{\"80\":1}}],[\"解決了\",{\"1\":{\"12\":1,\"16\":1,\"17\":1,\"25\":1}}],[\"解決了一些\",{\"1\":{\"10\":1}}],[\"顯示了\",{\"1\":{\"25\":1}}],[\"卻並不一定了\",{\"1\":{\"108\":1}}],[\"卻會受到\",{\"1\":{\"96\":1}}],[\"卻反而往往得到很糟糕的結果\",{\"1\":{\"63\":1}}],[\"卻是最差的\",{\"1\":{\"25\":1}}],[\"卻在一些簡單的問題做得很差\",{\"1\":{\"10\":1}}],[\"雖然這種做法開始能夠讓\",{\"1\":{\"115\":1}}],[\"雖然已經有\",{\"1\":{\"37\":1}}],[\"雖然在\",{\"1\":{\"25\":1}}],[\"雖然兩個模型都會把\",{\"1\":{\"12\":1}}],[\"提供一個簡單又有效的\",{\"1\":{\"109\":1}}],[\"提供了更大的普遍性\",{\"1\":{\"24\":1}}],[\"提出一個新的\",{\"1\":{\"131\":1}}],[\"提出一個可以即時修正\",{\"1\":{\"131\":1}}],[\"提出的方法略為不同的地方在於他並不是直接對\",{\"1\":{\"102\":1}}],[\"提出了解決這個高估問題的方法\",{\"1\":{\"98\":1}}],[\"提出了三個方法避免\",{\"1\":{\"72\":1}}],[\"提出如\",{\"1\":{\"37\":1}}],[\"提出\",{\"1\":{\"27\":1}}],[\"提出透過\",{\"1\":{\"27\":1}}],[\"提升了約\",{\"1\":{\"67\":4}}],[\"提升\",{\"1\":{\"22\":1}}],[\"7\",{\"1\":{\"50\":2,\"67\":1,\"86\":1}}],[\"79\",{\"1\":{\"24\":1}}],[\"76\",{\"1\":{\"24\":1,\"50\":1}}],[\"70\",{\"1\":{\"24\":2}}],[\"68\",{\"1\":{\"50\":1}}],[\"6\",{\"1\":{\"24\":1,\"66\":1,\"67\":2}}],[\"69571\",{\"1\":{\"24\":1}}],[\"6463\",{\"1\":{\"24\":1}}],[\"601\",{\"1\":{\"24\":1}}],[\"60\",{\"1\":{\"24\":2}}],[\"49\",{\"1\":{\"50\":1}}],[\"4\",{\"1\":{\"37\":1,\"67\":3,\"84\":1}}],[\"4336\",{\"1\":{\"24\":1}}],[\"4596\",{\"1\":{\"24\":1}}],[\"4753\",{\"1\":{\"24\":1}}],[\"400\",{\"1\":{\"19\":1}}],[\"40\",{\"1\":{\"17\":1}}],[\"8\",{\"1\":{\"67\":2,\"88\":1,\"110\":1}}],[\"83\",{\"1\":{\"24\":1,\"50\":1}}],[\"89\",{\"1\":{\"24\":2}}],[\"84\",{\"1\":{\"24\":1,\"50\":1}}],[\"80\",{\"1\":{\"17\":2}}],[\"3\",{\"1\":{\"67\":1}}],[\"35\",{\"1\":{\"50\":1}}],[\"31\",{\"1\":{\"24\":1}}],[\"30\",{\"1\":{\"24\":2}}],[\"32\",{\"1\":{\"24\":1}}],[\"38\",{\"1\":{\"24\":1}}],[\"349971\",{\"1\":{\"24\":1}}],[\"±\",{\"1\":{\"24\":10}}],[\"98\",{\"1\":{\"50\":1}}],[\"9400\",{\"1\":{\"47\":1}}],[\"90\",{\"1\":{\"24\":1}}],[\"96\",{\"1\":{\"24\":1}}],[\"999\",{\"1\":{\"70\":1}}],[\"9999\",{\"1\":{\"21\":1,\"24\":1,\"122\":1}}],[\"99\",{\"1\":{\"21\":1,\"24\":1}}],[\"款比較困難的遊戲當中有些甚至是能夠比\",{\"1\":{\"24\":1}}],[\"異常地大\",{\"1\":{\"24\":1}}],[\"尤其\",{\"1\":{\"124\":1}}],[\"尤其是從\",{\"1\":{\"34\":1}}],[\"尤其在\",{\"1\":{\"23\":1,\"37\":1}}],[\"尤其當\",{\"1\":{\"10\":1}}],[\"反之在\",{\"1\":{\"96\":1}}],[\"反之會走出最短路\",{\"1\":{\"22\":1}}],[\"反之則相遠\",{\"1\":{\"33\":1}}],[\"反之則是\",{\"1\":{\"22\":1}}],[\"反之則會去走那些比較熟悉的\",{\"1\":{\"6\":1}}],[\"可能已經\",{\"1\":{\"120\":1}}],[\"可對應到\",{\"1\":{\"46\":1}}],[\"可見\",{\"1\":{\"22\":1}}],[\"可以比較接近\",{\"1\":{\"124\":1}}],[\"可以看成是更新較慢的\",{\"1\":{\"121\":1}}],[\"可以看到在大多數的遊戲加上了\",{\"1\":{\"107\":1}}],[\"可以看到在綠色線的部分\",{\"1\":{\"70\":1}}],[\"可以看到所有\",{\"1\":{\"67\":1}}],[\"可以看到比較明顯的結果\",{\"1\":{\"23\":1}}],[\"可以帶來很不錯的\",{\"1\":{\"107\":1}}],[\"可以從分數上明顯看出來加上了\",{\"1\":{\"107\":1}}],[\"可以用單一的\",{\"1\":{\"102\":1}}],[\"可以平行化加速訓練\",{\"1\":{\"100\":1}}],[\"可以單純透過觀察模型在\",{\"1\":{\"87\":1}}],[\"可以直接把資料之間的關聯建構起來\",{\"1\":{\"78\":1}}],[\"可以觀察到\",{\"1\":{\"67\":1,\"70\":1}}],[\"可以有更好的\",{\"1\":{\"61\":1}}],[\"可以得到更好的結果\",{\"1\":{\"57\":1}}],[\"可以分成\",{\"1\":{\"36\":1}}],[\"可以發現搭配了\",{\"1\":{\"69\":1}}],[\"可以發現\",{\"1\":{\"48\":1,\"49\":1}}],[\"可以發現單純用\",{\"1\":{\"34\":1}}],[\"可以發現到通常越大的模型能夠提供更好的效益\",{\"1\":{\"61\":1}}],[\"可以發現到當\",{\"1\":{\"61\":1}}],[\"可以發現到\",{\"1\":{\"25\":1,\"61\":2}}],[\"可以發現到不同的遊戲會有不同的偏好\",{\"1\":{\"24\":1}}],[\"可以發現到加上\",{\"1\":{\"24\":1}}],[\"可以認為\",{\"1\":{\"24\":1}}],[\"可以移除\",{\"1\":{\"16\":1}}],[\"可以選擇\",{\"1\":{\"14\":1}}],[\"可以拿到多少\",{\"1\":{\"8\":1}}],[\"可以幫助我們去評估如果我們\",{\"1\":{\"8\":1}}],[\"移除\",{\"1\":{\"22\":1}}],[\"確實能夠得到好的結果\",{\"1\":{\"71\":1}}],[\"確實能夠帶來相當好的效益\",{\"1\":{\"22\":1}}],[\"確實它會傾向讓\",{\"1\":{\"14\":1}}],[\"增加通用性\",{\"1\":{\"100\":1}}],[\"增加模型的更新與\",{\"1\":{\"99\":1}}],[\"增加\",{\"1\":{\"22\":1,\"94\":1,\"102\":1}}],[\"導致對結果的特徵很雜亂\",{\"1\":{\"115\":1}}],[\"導致接下來會經過的\",{\"1\":{\"87\":1}}],[\"導致卡車時常被預測成汽車\",{\"1\":{\"37\":1}}],[\"導致預測失準\",{\"1\":{\"34\":1}}],[\"導致\",{\"1\":{\"22\":1,\"70\":1}}],[\"與過去的資源消耗相較減輕甚多\",{\"1\":{\"72\":1}}],[\"與過去的\",{\"1\":{\"71\":2,\"120\":1}}],[\"與過去的方法相比\",{\"1\":{\"56\":1}}],[\"與傳統的\",{\"1\":{\"61\":1,\"123\":1}}],[\"與\",{\"1\":{\"22\":1,\"25\":1,\"32\":1,\"60\":1,\"66\":1,\"78\":1,\"83\":1,\"98\":1,\"99\":1,\"115\":1,\"119\":1,\"124\":2}}],[\"由此可見\",{\"1\":{\"22\":1}}],[\"由於標記\",{\"1\":{\"57\":1}}],[\"由於\",{\"1\":{\"22\":2,\"61\":1,\"104\":1,\"105\":1,\"115\":1}}],[\"越大變得越小\",{\"1\":{\"22\":1}}],[\"綠色圓點表示\",{\"1\":{\"22\":1}}],[\"注意並不是\",{\"1\":{\"22\":1}}],[\"軸表示\",{\"1\":{\"22\":2}}],[\"各自的\",{\"1\":{\"34\":1}}],[\"各自最傾向\",{\"1\":{\"22\":1}}],[\"各有一個\",{\"1\":{\"13\":1}}],[\"接著作者比較\",{\"1\":{\"22\":1}}],[\"接下來直接看\",{\"1\":{\"128\":1}}],[\"接下來評估加上\",{\"1\":{\"107\":1}}],[\"接下來這一整個\",{\"1\":{\"102\":1}}],[\"接下來依照你的需求不同\",{\"1\":{\"13\":1}}],[\"接下來用\",{\"1\":{\"7\":1}}],[\"走到\",{\"1\":{\"22\":1}}],[\"最外層的期望值是對\",{\"1\":{\"104\":1}}],[\"最相近的一個研究\",{\"1\":{\"83\":1}}],[\"最小化\",{\"1\":{\"80\":1}}],[\"最大化\",{\"1\":{\"80\":1}}],[\"最明顯\",{\"1\":{\"69\":1}}],[\"最主要的貢獻\",{\"1\":{\"61\":1}}],[\"最好的\",{\"1\":{\"48\":1}}],[\"最\",{\"1\":{\"40\":1,\"60\":1}}],[\"最初被用於把\",{\"1\":{\"38\":1}}],[\"最初是為了解決\",{\"1\":{\"37\":1}}],[\"最終也可以看到\",{\"1\":{\"71\":1}}],[\"最終得到更好的結果\",{\"1\":{\"70\":1}}],[\"最終則是\",{\"1\":{\"57\":1}}],[\"最終在所有的\",{\"1\":{\"25\":1}}],[\"最終\",{\"1\":{\"22\":1}}],[\"最傾向\",{\"1\":{\"22\":2}}],[\"最多\",{\"1\":{\"22\":1}}],[\"最後就剩下更新的\",{\"1\":{\"123\":1}}],[\"最後整體的\",{\"1\":{\"100\":1}}],[\"最後合併成\",{\"1\":{\"99\":1}}],[\"最後得出的結果為\",{\"1\":{\"107\":2}}],[\"最後得出來的結果\",{\"1\":{\"88\":1}}],[\"最後得到的\",{\"1\":{\"69\":1}}],[\"最後將\",{\"1\":{\"88\":1}}],[\"最後比較\",{\"1\":{\"25\":1}}],[\"最後是針對\",{\"1\":{\"24\":1}}],[\"最後\",{\"1\":{\"16\":1,\"24\":1,\"25\":1,\"124\":1,\"125\":1}}],[\"最後你一樣可以透過這些\",{\"1\":{\"13\":1}}],[\"最後只需要設定\",{\"1\":{\"9\":1}}],[\"最後分數越多越好\",{\"1\":{\"3\":1}}],[\"右移動\",{\"1\":{\"22\":1}}],[\"右邊是\",{\"1\":{\"7\":1}}],[\"左\",{\"1\":{\"22\":1}}],[\"左邊是\",{\"1\":{\"7\":1}}],[\"下面則是\",{\"1\":{\"129\":1}}],[\"下有多強\",{\"1\":{\"61\":1}}],[\"下拍攝\",{\"1\":{\"47\":1}}],[\"下拍攝的\",{\"1\":{\"46\":1}}],[\"下\",{\"1\":{\"22\":1,\"52\":1}}],[\"隨機地放在地圖上的任意格子\",{\"1\":{\"22\":1}}],[\"我們的目標是要讓\",{\"1\":{\"124\":1}}],[\"我們也就會期待\",{\"1\":{\"108\":1}}],[\"我們也可以去定義當前\",{\"1\":{\"60\":1}}],[\"我們最後的\",{\"1\":{\"108\":1}}],[\"我們是從\",{\"1\":{\"84\":1}}],[\"我們理想上會預期那些\",{\"1\":{\"63\":1}}],[\"我們會隨著訓練過程慢慢調整\",{\"1\":{\"120\":1}}],[\"我們會寫成\",{\"1\":{\"102\":1}}],[\"我們會盡可能讓出現頻率越低的\",{\"1\":{\"62\":1}}],[\"我們會選其中最大的當成是最後的答案\",{\"1\":{\"60\":1}}],[\"我們就希望讓他在訓練過程當中出現的頻率可以更高\",{\"1\":{\"62\":1}}],[\"我們對於\",{\"1\":{\"37\":1}}],[\"我們可以想成現在\",{\"1\":{\"36\":1}}],[\"我們透過\",{\"1\":{\"22\":1}}],[\"我還沒有理解這一段做了什麼\",{\"1\":{\"8\":1}}],[\"範圍變成\",{\"1\":{\"21\":1}}],[\"其實\",{\"1\":{\"108\":1}}],[\"其實在訓練初期其實是能夠辨別這些\",{\"1\":{\"63\":1}}],[\"其方法與這一篇可說是大同小異\",{\"1\":{\"96\":1}}],[\"其餘則為\",{\"1\":{\"86\":1}}],[\"其他遊戲則都是\",{\"1\":{\"86\":1}}],[\"其他的\",{\"1\":{\"48\":1}}],[\"其他絕大多都是\",{\"1\":{\"37\":1}}],[\"其他像是路燈\",{\"1\":{\"36\":1}}],[\"其他\",{\"1\":{\"21\":1,\"71\":1}}],[\"其他部分基本上都跟\",{\"1\":{\"19\":1}}],[\"其中一種方法是採用\",{\"1\":{\"119\":1}}],[\"其中一半的\",{\"1\":{\"38\":1}}],[\"其中的\",{\"1\":{\"13\":1}}],[\"其中\",{\"1\":{\"8\":2,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"80\":1,\"103\":1,\"119\":1,\"124\":1}}],[\"交給\",{\"1\":{\"19\":1}}],[\"將上述的種種\",{\"1\":{\"124\":1}}],[\"將沒有做任何修正的\",{\"1\":{\"107\":1}}],[\"將產出的\",{\"1\":{\"36\":1}}],[\"將\",{\"1\":{\"19\":1,\"40\":1,\"41\":1,\"84\":1}}],[\"重複\",{\"1\":{\"19\":1}}],[\"⋅∣st+i​\",{\"1\":{\"100\":1,\"105\":1}}],[\"⋅∣s\",{\"1\":{\"97\":1}}],[\"⋅mthings\",{\"1\":{\"63\":1}}],[\"⋅\",{\"1\":{\"19\":1,\"60\":1,\"80\":1}}],[\"估計當前\",{\"1\":{\"19\":1}}],[\"開始訓練\",{\"1\":{\"125\":1}}],[\"開始前\",{\"1\":{\"19\":1}}],[\"開始之前先把參數加上\",{\"1\":{\"102\":1}}],[\"開始之前他們把一個\",{\"1\":{\"22\":1}}],[\"開始之前\",{\"1\":{\"13\":1}}],[\"搭配對抗式學習對\",{\"1\":{\"128\":1}}],[\"搭配\",{\"1\":{\"17\":2,\"43\":1,\"57\":1,\"71\":1,\"82\":1,\"99\":1,\"128\":2}}],[\"y^​t\",{\"1\":{\"119\":2,\"120\":1,\"122\":2,\"123\":1}}],[\"y^​t​\",{\"1\":{\"118\":1,\"123\":2}}],[\"yong\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"you\",{\"1\":{\"52\":1}}],[\"y=wx+b⇒y=\",{\"1\":{\"102\":1}}],[\"y∼p\",{\"1\":{\"97\":1}}],[\"yt\",{\"1\":{\"60\":1}}],[\"yt​\",{\"1\":{\"60\":1,\"118\":2,\"119\":1}}],[\"yt​=\",{\"1\":{\"60\":1,\"118\":1}}],[\"yt​^​=ξ\",{\"1\":{\"119\":1}}],[\"yt​^​=t^q\",{\"1\":{\"8\":1}}],[\"yt​^​\",{\"1\":{\"8\":1,\"41\":2}}],[\"ysc​\",{\"1\":{\"63\":1}}],[\"ys\",{\"1\":{\"60\":1,\"62\":1,\"63\":2}}],[\"ys​=\",{\"1\":{\"60\":1,\"118\":1}}],[\"ys​\",{\"1\":{\"41\":5,\"60\":1,\"118\":2,\"119\":1}}],[\"ym​\",{\"1\":{\"41\":4}}],[\"ya​\",{\"1\":{\"38\":1}}],[\"yang\",{\"1\":{\"37\":1}}],[\"yi​−q\",{\"1\":{\"80\":1}}],[\"yi​​=es\",{\"1\":{\"80\":1}}],[\"yi\",{\"1\":{\"36\":2,\"43\":1}}],[\"yiheng\",{\"1\":{\"34\":1}}],[\"y\",{\"1\":{\"22\":1,\"97\":2,\"98\":3,\"99\":3,\"104\":10}}],[\"yk​\",{\"1\":{\"16\":1}}],[\"之前在\",{\"1\":{\"128\":1}}],[\"之類的\",{\"1\":{\"40\":1}}],[\"之間的距離\",{\"1\":{\"120\":1}}],[\"之間的距離去調整\",{\"1\":{\"120\":1}}],[\"之間的距離可以拉近\",{\"1\":{\"63\":1}}],[\"之間都會具有相當高的相關性\",{\"1\":{\"78\":1}}],[\"之間有一些重疊的\",{\"1\":{\"24\":1}}],[\"之間均勻分布的隨機\",{\"1\":{\"16\":1}}],[\"之間均勻分布的隨機值\",{\"1\":{\"16\":1}}],[\"之後的結果都有些進步\",{\"1\":{\"107\":1}}],[\"之後的誤差\",{\"1\":{\"98\":1}}],[\"之後可以得到底下的\",{\"1\":{\"99\":1}}],[\"之後作為實際上儲存進\",{\"1\":{\"84\":1}}],[\"之後得到的成效在\",{\"1\":{\"24\":1}}],[\"之後\",{\"1\":{\"24\":1,\"67\":4,\"70\":1,\"84\":1,\"102\":1}}],[\"之後仍然不會停止的話就不會出現\",{\"1\":{\"22\":1}}],[\"之後交給\",{\"1\":{\"19\":1}}],[\"之所以說\",{\"1\":{\"3\":1}}],[\"多加上\",{\"1\":{\"16\":1}}],[\"多多探索\",{\"1\":{\"6\":1}}],[\"對應的\",{\"1\":{\"38\":2,\"60\":2,\"118\":1,\"124\":1}}],[\"對陌生人的認識\",{\"1\":{\"37\":1}}],[\"對抗式學習\",{\"1\":{\"36\":1}}],[\"對於一開始預測錯誤的\",{\"1\":{\"123\":1}}],[\"對於每個\",{\"1\":{\"120\":1}}],[\"對於每個可訓練的參數拆解成\",{\"1\":{\"102\":1}}],[\"對於每個實驗的\",{\"1\":{\"21\":1}}],[\"對於這些\",{\"1\":{\"62\":1}}],[\"對於\",{\"1\":{\"17\":1,\"37\":1,\"100\":1,\"124\":1}}],[\"對於結果並不會有影響\",{\"1\":{\"16\":1}}],[\"對於學習是並沒有幫助的\",{\"1\":{\"10\":1}}],[\"對\",{\"1\":{\"16\":1,\"104\":1,\"123\":1}}],[\"剩餘的都是相同的\",{\"1\":{\"15\":1}}],[\"剩下的四款遊戲則是因為環境太大\",{\"1\":{\"3\":1}}],[\"剩下這些遊戲有怎樣的共通點呢\",{\"1\":{\"3\":1}}],[\"僅僅是加上\",{\"1\":{\"15\":1}}],[\"來估計\",{\"1\":{\"122\":1}}],[\"來表示\",{\"1\":{\"119\":1}}],[\"來轉換\",{\"1\":{\"119\":1}}],[\"來調整模型的機率分布\",{\"1\":{\"119\":1}}],[\"來調整整體\",{\"1\":{\"22\":1}}],[\"來達成\",{\"1\":{\"96\":1}}],[\"來近似\",{\"1\":{\"80\":1}}],[\"來說可能導致收斂不穩定以及緩慢等問題\",{\"1\":{\"100\":1}}],[\"來說\",{\"1\":{\"80\":1,\"102\":1}}],[\"來說由於缺乏對於\",{\"1\":{\"37\":1}}],[\"來說是相當大的問題\",{\"1\":{\"37\":1}}],[\"來讓每個\",{\"1\":{\"27\":1}}],[\"來比較\",{\"1\":{\"23\":1}}],[\"來限制要考慮多久之前的經驗\",{\"1\":{\"15\":1}}],[\"來解決它\",{\"1\":{\"14\":1}}],[\"來解決\",{\"1\":{\"13\":1,\"115\":1}}],[\"採取\",{\"1\":{\"80\":1}}],[\"採取的次數以及得到的\",{\"1\":{\"15\":1}}],[\"採用\",{\"1\":{\"66\":1}}],[\"採用了\",{\"1\":{\"63\":1}}],[\"採用不同的\",{\"1\":{\"13\":1}}],[\"採用分散式學習\",{\"1\":{\"9\":1}}],[\"而接下來作者給出一個\",{\"1\":{\"103\":1}}],[\"而這裡則選擇在\",{\"1\":{\"96\":1}}],[\"而這種探索的困難度甚至是指數性地成長\",{\"1\":{\"94\":1}}],[\"而在\",{\"1\":{\"96\":2}}],[\"而且也往往會經過一段時間的延遲才取得\",{\"1\":{\"78\":1}}],[\"而最一開始的結果其實是還不錯的\",{\"1\":{\"70\":1}}],[\"而一個\",{\"1\":{\"62\":1,\"119\":1}}],[\"而\",{\"1\":{\"37\":1,\"43\":1,\"44\":1,\"80\":1,\"83\":1,\"122\":1}}],[\"而半監督式學習困難的點在於雖然對於\",{\"1\":{\"37\":1}}],[\"而被提出的\",{\"1\":{\"37\":1}}],[\"而是給了參數\",{\"1\":{\"102\":1}}],[\"而是另外定義了一個\",{\"1\":{\"99\":1}}],[\"而是自駕車車體\",{\"1\":{\"70\":1}}],[\"而是包含了機率的概念\",{\"1\":{\"60\":1}}],[\"而是將\",{\"1\":{\"41\":1}}],[\"而是\",{\"1\":{\"21\":1}}],[\"而已\",{\"1\":{\"15\":1}}],[\"而隨著\",{\"1\":{\"15\":1}}],[\"而整體的\",{\"1\":{\"6\":1}}],[\"而整體\",{\"1\":{\"6\":1}}],[\"並無法繼續擴充到其他的領域\",{\"1\":{\"81\":1}}],[\"並無法好好只透過一個\",{\"1\":{\"22\":1}}],[\"並\",{\"1\":{\"80\":1}}],[\"並沒有得到比\",{\"1\":{\"61\":1}}],[\"並沒有\",{\"1\":{\"50\":1}}],[\"並不會都得出\",{\"1\":{\"108\":1}}],[\"並不會是一個好的選項\",{\"1\":{\"15\":1}}],[\"並不一定要是\",{\"1\":{\"48\":1}}],[\"並不是所有的\",{\"1\":{\"37\":1}}],[\"並且預期能夠讓\",{\"1\":{\"126\":1}}],[\"並且分成了弱增強\",{\"1\":{\"124\":1}}],[\"並且沒有使用\",{\"1\":{\"100\":1}}],[\"並且達到了很棒的效果\",{\"1\":{\"96\":1}}],[\"並且做出相對應的\",{\"1\":{\"86\":1}}],[\"並且會使用\",{\"1\":{\"86\":1}}],[\"並且\",{\"1\":{\"84\":1}}],[\"並且證明了底下兩個狀況是可以確保收斂\",{\"1\":{\"82\":1}}],[\"並且最終的\",{\"1\":{\"71\":1}}],[\"並且普遍最後的\",{\"1\":{\"69\":1}}],[\"並且也可以觀察到那些比較早開始有所提升的\",{\"1\":{\"69\":1}}],[\"並且也是\",{\"1\":{\"12\":1}}],[\"並且透過觀察訓練過程作者發現到\",{\"1\":{\"63\":1}}],[\"並且有趣的是\",{\"1\":{\"61\":1}}],[\"並且不同\",{\"1\":{\"27\":1}}],[\"並且每個\",{\"1\":{\"22\":1}}],[\"然後拿去\",{\"1\":{\"126\":1}}],[\"然後拿到了\",{\"1\":{\"50\":1}}],[\"然後會有一個\",{\"1\":{\"36\":1}}],[\"然後應用在真實的環境當中\",{\"1\":{\"33\":1}}],[\"然後再拿去訓練\",{\"1\":{\"115\":1}}],[\"然後再透過\",{\"1\":{\"102\":1}}],[\"然後再把這些特徵丟去給\",{\"1\":{\"83\":1}}],[\"然後再應用在真實的世界當中\",{\"1\":{\"33\":1}}],[\"然後再讓\",{\"1\":{\"9\":1}}],[\"然後結束這個\",{\"1\":{\"22\":1}}],[\"然後繼續跟環境互動\",{\"1\":{\"19\":1}}],[\"然而因為\",{\"1\":{\"124\":1}}],[\"然而若觀察倒數第二個\",{\"1\":{\"108\":1}}],[\"然而存在幾個缺點\",{\"1\":{\"100\":1}}],[\"然而使用了\",{\"1\":{\"98\":1}}],[\"然而在現實狀況下往往並不會如此簡單\",{\"1\":{\"94\":1}}],[\"然而這種做法每次要更新\",{\"1\":{\"122\":1}}],[\"然而這種方法的\",{\"1\":{\"60\":1}}],[\"然而這些研究都並未能夠給出用非線性去學\",{\"1\":{\"82\":1}}],[\"然而我們也看到近幾年\",{\"1\":{\"78\":1}}],[\"然而\",{\"1\":{\"15\":1,\"63\":1,\"98\":1,\"120\":1}}],[\"從訓練中的曲線也可以明顯看到\",{\"1\":{\"107\":1}}],[\"從\",{\"1\":{\"41\":2}}],[\"從這裡也可以了解到實際上讓每個\",{\"1\":{\"24\":1}}],[\"從這些觀察當中可以得到兩個待改善的地方\",{\"1\":{\"3\":1}}],[\"從上面的圖片中也可以觀察到這樣的\",{\"1\":{\"24\":1}}],[\"從結果可以發現到\",{\"1\":{\"22\":1}}],[\"從式子當中也可以觀察到\",{\"1\":{\"14\":1}}],[\"嘗試機率高\",{\"1\":{\"14\":1}}],[\"嘗試機率低\",{\"1\":{\"14\":1}}],[\"嘗試次數少\",{\"1\":{\"14\":1}}],[\"嘗試次數少的選擇\",{\"1\":{\"14\":1}}],[\"嘗試次數多的選擇\",{\"1\":{\"14\":1}}],[\"探索機率更高\",{\"1\":{\"14\":1}}],[\"探索機率高\",{\"1\":{\"14\":1}}],[\"更加密集\",{\"1\":{\"124\":1}}],[\"更早看到它\",{\"1\":{\"62\":1}}],[\"更多關於模型選擇的實驗\",{\"1\":{\"61\":1}}],[\"更好的表現\",{\"1\":{\"61\":1}}],[\"更好的成效\",{\"1\":{\"27\":1}}],[\"更好學習\",{\"1\":{\"8\":1}}],[\"更新參數是從\",{\"1\":{\"97\":1}}],[\"更新模型參數\",{\"1\":{\"19\":1}}],[\"更新\",{\"1\":{\"15\":1}}],[\"更高\",{\"1\":{\"14\":1}}],[\"高估的問題消失\",{\"1\":{\"98\":1}}],[\"高估的狀況如底下的綠線\",{\"1\":{\"98\":1}}],[\"高\",{\"1\":{\"14\":5}}],[\"➡️\",{\"1\":{\"14\":8}}],[\"低\",{\"1\":{\"14\":3}}],[\"平均去評估\",{\"1\":{\"87\":1}}],[\"平均\",{\"1\":{\"14\":5,\"15\":1,\"122\":1}}],[\"未知\",{\"1\":{\"14\":1}}],[\"基本想法\",{\"0\":{\"102\":1}}],[\"基本概念\",{\"1\":{\"28\":1}}],[\"基本上就是將\",{\"1\":{\"103\":1}}],[\"基本上就是從\",{\"1\":{\"38\":1}}],[\"基本上都相當接近\",{\"1\":{\"22\":1}}],[\"基本上使用了\",{\"1\":{\"9\":1}}],[\"基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界\",{\"1\":{\"14\":1}}],[\"目的是要訓練出一個\",{\"1\":{\"126\":1}}],[\"目的是要先訓練出一個\",{\"1\":{\"126\":1}}],[\"目的也是希望能夠讓\",{\"1\":{\"5\":1}}],[\"目標在於給定\",{\"1\":{\"119\":1}}],[\"目標\",{\"1\":{\"98\":2}}],[\"目標是在整個\",{\"1\":{\"14\":1}}],[\"你知道cross\",{\"1\":{\"132\":1}}],[\"你選擇\",{\"1\":{\"14\":1}}],[\"你分別把這幾個\",{\"1\":{\"13\":1}}],[\"kd\",{\"1\":{\"125\":1}}],[\"kl\",{\"1\":{\"124\":1}}],[\"k=argmaxk\",{\"1\":{\"119\":1}}],[\"k=4\",{\"1\":{\"86\":1}}],[\"k=3\",{\"1\":{\"86\":1}}],[\"k=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"koray\",{\"1\":{\"77\":1,\"84\":1,\"87\":1,\"88\":1}}],[\"knowledge\",{\"1\":{\"60\":1,\"125\":1,\"126\":1,\"128\":1,\"131\":1}}],[\"know\",{\"1\":{\"52\":1}}],[\"k−τ\",{\"1\":{\"15\":2}}],[\"k−1\",{\"1\":{\"14\":1,\"15\":1}}],[\"kalman\",{\"1\":{\"110\":1}}],[\"kavukcuoglu\",{\"1\":{\"77\":1,\"84\":1,\"87\":1,\"88\":1}}],[\"kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"kapturowski\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"k\",{\"1\":{\"14\":2,\"15\":1,\"86\":1,\"100\":1,\"118\":1,\"119\":7,\"120\":9,\"121\":6,\"122\":9,\"123\":3,\"124\":7}}],[\"017\",{\"1\":{\"105\":1}}],[\"02\",{\"1\":{\"50\":1}}],[\"04\",{\"1\":{\"24\":1}}],[\"00\",{\"1\":{\"24\":6}}],[\"06\",{\"1\":{\"24\":1}}],[\"0\",{\"1\":{\"14\":1,\"15\":2,\"16\":2,\"21\":3,\"22\":3,\"24\":5,\"50\":1,\"60\":1,\"67\":1,\"70\":1,\"86\":1,\"96\":1,\"99\":2,\"107\":1,\"108\":2,\"119\":1,\"120\":2,\"123\":1}}],[\"簡單設計成一個\",{\"1\":{\"14\":1}}],[\"把所有對應到\",{\"1\":{\"122\":1}}],[\"把四隻腳站立的動物都當成草食類動物一樣\",{\"1\":{\"115\":1}}],[\"把兩個圖片\",{\"1\":{\"38\":1}}],[\"把這個上界當成是它預期的\",{\"1\":{\"14\":1}}],[\"把\",{\"1\":{\"14\":1,\"38\":3,\"40\":1,\"41\":1,\"61\":1,\"99\":1}}],[\"ϵj​∈rp\",{\"1\":{\"103\":1}}],[\"ϵj​\",{\"1\":{\"103\":2}}],[\"ϵi​∈rq\",{\"1\":{\"103\":1}}],[\"ϵi​\",{\"1\":{\"103\":1}}],[\"ϵi\",{\"1\":{\"103\":1}}],[\"ϵb∈rq\",{\"1\":{\"103\":1}}],[\"ϵb\",{\"1\":{\"102\":1}}],[\"ϵw∈rq×p\",{\"1\":{\"103\":1}}],[\"ϵw\",{\"1\":{\"102\":1}}],[\"ϵucb​\",{\"1\":{\"16\":1}}],[\"ϵ\",{\"1\":{\"13\":1,\"16\":1,\"84\":1,\"94\":1,\"95\":1,\"96\":3,\"97\":1,\"99\":1,\"102\":1,\"104\":8,\"105\":3,\"108\":2}}],[\"ϵl​=ϵ1+αl−11​\",{\"1\":{\"19\":1}}],[\"ϵl​\",{\"1\":{\"13\":1,\"19\":2}}],[\"亦即\",{\"1\":{\"13\":1,\"25\":1}}],[\"wt\",{\"1\":{\"120\":3,\"121\":1,\"123\":1}}],[\"w∈rq×p\",{\"1\":{\"103\":1}}],[\"wen\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"weights\",{\"1\":{\"125\":1}}],[\"weight\",{\"1\":{\"103\":1,\"120\":3,\"124\":1}}],[\"weak\",{\"1\":{\"57\":1}}],[\"wf​\",{\"1\":{\"63\":1}}],[\"w\",{\"1\":{\"60\":1,\"63\":1}}],[\"with\",{\"0\":{\"76\":1},\"1\":{\"51\":1,\"62\":1,\"79\":1,\"96\":2,\"110\":2}}],[\"wilhelm\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"winter\",{\"1\":{\"32\":1}}],[\"window\",{\"0\":{\"15\":1,\"16\":1,\"17\":1,\"23\":1},\"1\":{\"15\":2,\"16\":1}}],[\"work\",{\"1\":{\"34\":1}}],[\"works\",{\"0\":{\"4\":1,\"35\":1,\"58\":1,\"79\":1,\"95\":1,\"116\":1}}],[\"what\",{\"0\":{\"33\":1}}],[\"wang\",{\"1\":{\"97\":1,\"99\":1,\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"warmup\",{\"0\":{\"64\":1,\"68\":1},\"1\":{\"61\":1,\"64\":1,\"67\":2,\"68\":1,\"128\":1}}],[\"warning\",{\"1\":{\"13\":1,\"96\":1}}],[\"wall\",{\"1\":{\"47\":1}}],[\"wacv\",{\"1\":{\"32\":1},\"2\":{\"54\":1}}],[\"中這一點尤其重要\",{\"1\":{\"123\":1}}],[\"中心點越近\",{\"1\":{\"120\":1}}],[\"中隨意挑一筆\",{\"1\":{\"97\":1}}],[\"中的\",{\"1\":{\"84\":1}}],[\"中文敘述參考\",{\"1\":{\"61\":1}}],[\"中各取圖片\",{\"1\":{\"36\":1}}],[\"中\",{\"1\":{\"13\":1}}],[\"此外\",{\"1\":{\"13\":1,\"21\":1,\"22\":1,\"24\":1,\"60\":2,\"61\":1,\"70\":2,\"108\":1}}],[\"前面都加上一組\",{\"1\":{\"13\":1}}],[\"具有相同的\",{\"1\":{\"124\":1}}],[\"具有更高的靈活性\",{\"1\":{\"13\":1}}],[\"具體而言有三個部分\",{\"1\":{\"61\":1}}],[\"具體來說就是他們試圖在\",{\"1\":{\"96\":1}}],[\"具體來說如下圖\",{\"1\":{\"21\":1}}],[\"具體來說\",{\"1\":{\"5\":1,\"38\":1}}],[\"蒐集的\",{\"1\":{\"13\":1}}],[\"蒐集一些\",{\"1\":{\"13\":1}}],[\"什麼時候該\",{\"1\":{\"13\":1}}],[\"透過前面的步驟得到的模型稱為\",{\"1\":{\"125\":1}}],[\"透過如\",{\"1\":{\"119\":1}}],[\"透過儲存\",{\"1\":{\"97\":1}}],[\"透過一個\",{\"1\":{\"81\":1}}],[\"透過一些方式混在一起\",{\"1\":{\"38\":1}}],[\"透過固定訓練的目標\",{\"1\":{\"80\":1}}],[\"透過產生假想的\",{\"1\":{\"60\":1}}],[\"透過這個模型我們就有辦法給\",{\"1\":{\"37\":1}}],[\"透過上一個\",{\"1\":{\"19\":1}}],[\"透過各自的\",{\"1\":{\"19\":1}}],[\"透過\",{\"1\":{\"16\":1,\"17\":1,\"19\":1,\"25\":1,\"36\":1,\"41\":1,\"84\":1,\"89\":1,\"97\":1,\"99\":1,\"108\":1,\"115\":1}}],[\"透過它決定接下來要使用的\",{\"1\":{\"13\":1}}],[\"透過加上\",{\"1\":{\"13\":1,\"70\":1}}],[\"透過拆開訓練\",{\"1\":{\"12\":1}}],[\"無法收斂的問題可以透過\",{\"1\":{\"82\":1}}],[\"無法收斂\",{\"1\":{\"82\":1}}],[\"無法好好處理\",{\"1\":{\"10\":1,\"17\":1}}],[\"無論是否有使用\",{\"1\":{\"12\":1}}],[\"輸入進去\",{\"1\":{\"12\":1}}],[\"於是他們定義了底下的\",{\"1\":{\"97\":1}}],[\"於是乎最後的整體\",{\"1\":{\"63\":1}}],[\"於是作者嘗試修改\",{\"1\":{\"61\":1}}],[\"於是\",{\"1\":{\"12\":1,\"60\":1,\"61\":1,\"62\":1}}],[\"為甚麼可以直接拿去\",{\"1\":{\"126\":1}}],[\"為了避免\",{\"1\":{\"125\":1}}],[\"為了避免所謂的\",{\"1\":{\"124\":1}}],[\"為了進一步去釐清這樣的做法為什麼是可行\",{\"1\":{\"108\":1}}],[\"為當前\",{\"1\":{\"12\":1}}],[\"為目標\",{\"1\":{\"12\":1}}],[\"為\",{\"1\":{\"12\":3,\"80\":1,\"86\":1}}],[\"表示當前\",{\"1\":{\"122\":1}}],[\"表示第\",{\"1\":{\"121\":1}}],[\"表示著在當前這個\",{\"1\":{\"99\":1}}],[\"表示不影響\",{\"1\":{\"86\":1}}],[\"表示不同的\",{\"1\":{\"13\":1}}],[\"表示在時間\",{\"1\":{\"80\":1}}],[\"表示\",{\"1\":{\"12\":1,\"24\":1,\"60\":8,\"118\":1,\"119\":1,\"121\":2}}],[\"表示從\",{\"1\":{\"12\":1}}],[\"表示使用的是哪一個\",{\"1\":{\"12\":1}}],[\"因為我們現在對每一個\",{\"1\":{\"129\":1}}],[\"因為\",{\"1\":{\"123\":1,\"124\":1}}],[\"因為遊戲當中的雷射會跑很快\",{\"1\":{\"86\":1}}],[\"因為缺乏對他人的理解\",{\"1\":{\"34\":1}}],[\"因為過去的經驗即便在現實狀況改變仍然有大影響力\",{\"1\":{\"15\":1}}],[\"因為是一次更新\",{\"1\":{\"12\":1}}],[\"因此適合用來教\",{\"1\":{\"124\":1}}],[\"因此透過\",{\"1\":{\"124\":1}}],[\"因此不太需要考慮上述的\",{\"1\":{\"105\":1}}],[\"因此上述的\",{\"1\":{\"104\":1}}],[\"因此在參數上也就包含了兩項\",{\"1\":{\"100\":1}}],[\"因此細節上是還會對\",{\"1\":{\"99\":1}}],[\"因此仍然沒有解決問題\",{\"1\":{\"94\":1}}],[\"因此作者改用一個\",{\"1\":{\"122\":1}}],[\"因此作者改成\",{\"1\":{\"87\":1}}],[\"因此作者提出的方法是將\",{\"1\":{\"120\":1}}],[\"因此作者認為這是跟圖片被\",{\"1\":{\"69\":1}}],[\"因此作者認為\",{\"1\":{\"24\":1}}],[\"因此評估一個\",{\"1\":{\"87\":1}}],[\"因此這裡的拉近只會針對\",{\"1\":{\"63\":1}}],[\"因此這一篇論文提出一個方法試圖去消除\",{\"1\":{\"94\":1}}],[\"因此這一篇\",{\"1\":{\"57\":1}}],[\"因此這時候\",{\"1\":{\"38\":1}}],[\"因此會比較能夠好好評估\",{\"1\":{\"21\":1}}],[\"因此\",{\"1\":{\"12\":1,\"15\":1,\"33\":1,\"63\":1,\"70\":1,\"84\":1,\"98\":1,\"102\":1,\"108\":1}}],[\"個數值\",{\"1\":{\"103\":1}}],[\"個小時\",{\"1\":{\"72\":1}}],[\"個共通的\",{\"1\":{\"60\":1,\"118\":1}}],[\"個的平均\",{\"1\":{\"49\":1}}],[\"個平均跟\",{\"1\":{\"49\":1}}],[\"個比較困難的遊戲當中\",{\"1\":{\"24\":1}}],[\"個比較難的遊戲當中測試的結果\",{\"1\":{\"23\":1}}],[\"個\",{\"1\":{\"12\":1,\"14\":1,\"19\":1,\"22\":1,\"47\":1,\"49\":2,\"63\":1,\"84\":1,\"86\":2,\"103\":1,\"107\":1}}],[\"個遊戲場景\",{\"1\":{\"3\":1}}],[\"個遊戲是\",{\"1\":{\"3\":1}}],[\"個遊戲當中有\",{\"1\":{\"3\":1}}],[\"細節上\",{\"1\":{\"12\":1,\"13\":1,\"22\":1}}],[\"時於\",{\"1\":{\"100\":1}}],[\"時訓練不佳的問題\",{\"1\":{\"89\":1}}],[\"時常我們會訓練在合成資料上\",{\"1\":{\"33\":1}}],[\"時間\",{\"0\":{\"103\":1},\"1\":{\"12\":2}}],[\"時\",{\"1\":{\"12\":1}}],[\"跟每個\",{\"1\":{\"121\":1}}],[\"跟幾個\",{\"1\":{\"88\":1}}],[\"跟環境的互動過程當中的\",{\"1\":{\"84\":1}}],[\"跟這一篇\",{\"1\":{\"83\":1}}],[\"跟其他架構相比\",{\"1\":{\"71\":1}}],[\"跟目標相同\",{\"1\":{\"60\":1}}],[\"跟\",{\"1\":{\"12\":1,\"24\":1,\"38\":2,\"41\":1,\"43\":1,\"88\":1,\"102\":1,\"107\":2,\"120\":1}}],[\"相較之下\",{\"1\":{\"71\":1,\"78\":1,\"96\":1}}],[\"相鄰而導致的誤判被稱為\",{\"1\":{\"40\":1}}],[\"相差過大\",{\"1\":{\"33\":1}}],[\"相對的\",{\"1\":{\"22\":1}}],[\"相同\",{\"1\":{\"19\":1,\"60\":1}}],[\"相同的\",{\"1\":{\"12\":1}}],[\"相當重要的問題\",{\"1\":{\"3\":1}}],[\"兩者差異只在於使用的\",{\"1\":{\"124\":1}}],[\"兩部分影響程度的參數\",{\"1\":{\"99\":1}}],[\"兩個可以同時處理\",{\"1\":{\"120\":1}}],[\"兩個參數的\",{\"1\":{\"100\":1}}],[\"兩個部分\",{\"1\":{\"61\":1,\"119\":1}}],[\"兩個模型都是使用\",{\"1\":{\"12\":1}}],[\"兩個\",{\"1\":{\"12\":1,\"36\":1}}],[\"兩篇\",{\"1\":{\"3\":1}}],[\"j=1nt​​\",{\"1\":{\"118\":2}}],[\"j=1ns​​\",{\"1\":{\"118\":2}}],[\"j​=p​0\",{\"1\":{\"105\":1}}],[\"j​=0\",{\"1\":{\"105\":1}}],[\"j​∼u\",{\"1\":{\"105\":2}}],[\"jw​ϵjb​​=f\",{\"1\":{\"103\":1}}],[\"juliani\",{\"1\":{\"100\":1}}],[\"jitter\",{\"1\":{\"60\":1}}],[\"john\",{\"1\":{\"19\":1}}],[\"j\",{\"1\":{\"12\":5,\"13\":1,\"19\":1,\"60\":8,\"62\":1,\"63\":13}}],[\"處理成相同\",{\"1\":{\"61\":1}}],[\"處理\",{\"1\":{\"12\":1,\"34\":1,\"63\":1}}],[\"用\",{\"1\":{\"119\":1}}],[\"用來調整兩個\",{\"1\":{\"100\":1}}],[\"用來調整兩種\",{\"1\":{\"6\":1}}],[\"用來加上\",{\"1\":{\"95\":1}}],[\"用來比較\",{\"1\":{\"61\":1}}],[\"用來表示一個\",{\"1\":{\"14\":2}}],[\"用兩個\",{\"1\":{\"12\":1}}],[\"首先把\",{\"1\":{\"107\":1}}],[\"首先看到上面的表格\",{\"1\":{\"67\":1}}],[\"首先\",{\"1\":{\"61\":1}}],[\"首先針對\",{\"1\":{\"12\":1}}],[\"首先定義從\",{\"1\":{\"8\":1}}],[\"問題在於不同的\",{\"1\":{\"34\":1}}],[\"問題\",{\"1\":{\"10\":1,\"14\":1,\"119\":1}}],[\"問題描述\",{\"0\":{\"3\":1,\"34\":1,\"57\":1,\"78\":1,\"94\":1,\"115\":1},\"1\":{\"57\":1}}],[\"但結果不一定正確\",{\"1\":{\"115\":1}}],[\"但比起\",{\"1\":{\"115\":1}}],[\"但有包含了部分的學習過程\",{\"1\":{\"88\":1}}],[\"但在\",{\"1\":{\"78\":1}}],[\"但在邊界上往往還是難以有好的結果\",{\"1\":{\"37\":1}}],[\"但數值範圍往往很\",{\"1\":{\"78\":1}}],[\"但數學有點太難\",{\"1\":{\"8\":1}}],[\"但卻隨著訓練過程慢慢地變糟\",{\"1\":{\"63\":1}}],[\"但透過剪貼則可以造成不同環境的突兀感\",{\"1\":{\"38\":1}}],[\"但主要的問題來自於\",{\"1\":{\"37\":1}}],[\"但並不\",{\"1\":{\"25\":1}}],[\"但最後能取得更好的\",{\"1\":{\"23\":1}}],[\"但整體來說兩者都能在最後趨近於\",{\"1\":{\"22\":1}}],[\"但是從頭到尾都沒丟給\",{\"1\":{\"126\":1}}],[\"但是這些架構在\",{\"1\":{\"57\":1}}],[\"但是這種做法實際上效果很糟糕\",{\"1\":{\"40\":1}}],[\"但是並不全面\",{\"1\":{\"37\":1}}],[\"但是對於真實世界\",{\"1\":{\"33\":1}}],[\"但是在\",{\"1\":{\"25\":1,\"71\":1,\"87\":1}}],[\"但是\",{\"1\":{\"13\":1}}],[\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的\",{\"1\":{\"13\":1}}],[\"但是卻跟其他人有同樣的影響力\",{\"1\":{\"10\":1}}],[\"但\",{\"1\":{\"12\":1,\"48\":1}}],[\"每\",{\"1\":{\"107\":2}}],[\"每一個\",{\"1\":{\"13\":1}}],[\"每個遊戲的\",{\"1\":{\"86\":1}}],[\"每個\",{\"1\":{\"12\":1,\"13\":2,\"16\":1,\"19\":1,\"120\":1,\"123\":1}}],[\"每種\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"每忽略一個\",{\"1\":{\"3\":1}}],[\"難以收斂的問題\",{\"1\":{\"12\":1,\"27\":1}}],[\"難以收斂\",{\"1\":{\"10\":1}}],[\"實驗是做在\",{\"1\":{\"107\":1}}],[\"實驗上調整了\",{\"1\":{\"86\":1}}],[\"實驗做在\",{\"1\":{\"86\":1}}],[\"實驗設定\",{\"0\":{\"43\":1,\"66\":1,\"86\":1,\"128\":1}}],[\"實作上採用了常見的\",{\"1\":{\"66\":1}}],[\"實作上\",{\"1\":{\"10\":1,\"84\":1}}],[\"實際上他所謂的\",{\"1\":{\"126\":1}}],[\"實際上\",{\"1\":{\"124\":1}}],[\"實際上對於\",{\"1\":{\"123\":1}}],[\"實際上跟\",{\"1\":{\"99\":1}}],[\"實際上包含了\",{\"1\":{\"61\":1}}],[\"實際上是有幫助的\",{\"1\":{\"24\":1}}],[\"實際上還會為了讓\",{\"1\":{\"8\":1}}],[\"實際上訓練的\",{\"1\":{\"8\":1}}],[\"丟在\",{\"1\":{\"9\":1}}],[\"有不少的提升\",{\"1\":{\"130\":1}}],[\"有不少人最後給的結果之所以那麼好看是因為\",{\"1\":{\"50\":1}}],[\"有多好多壞\",{\"1\":{\"99\":1}}],[\"有多少影響呢\",{\"1\":{\"23\":1}}],[\"有更好的效果\",{\"1\":{\"98\":1}}],[\"有更多的\",{\"1\":{\"96\":1}}],[\"有提及一個使用\",{\"1\":{\"94\":1}}],[\"有大的影響\",{\"1\":{\"87\":1}}],[\"有所提升\",{\"1\":{\"68\":1}}],[\"有較高的機會被\",{\"1\":{\"62\":1}}],[\"有較大的影響\",{\"1\":{\"22\":1}}],[\"有另一個有趣的好處是\",{\"1\":{\"61\":1}}],[\"有許多相像的地方\",{\"1\":{\"61\":1}}],[\"有許多新的架構可以得到更高的\",{\"1\":{\"57\":1}}],[\"有許多的\",{\"1\":{\"9\":1}}],[\"有兩列分別表示\",{\"1\":{\"49\":1}}],[\"有點偏以及\",{\"1\":{\"48\":1}}],[\"有還不錯的成效\",{\"1\":{\"84\":1}}],[\"有還不錯的\",{\"1\":{\"48\":1}}],[\"有一些常見的\",{\"1\":{\"44\":1}}],[\"有可能就被誤判成人行道\",{\"1\":{\"37\":1}}],[\"有部分的認知\",{\"1\":{\"37\":1}}],[\"有相當大的差異\",{\"1\":{\"36\":1}}],[\"有最大的\",{\"1\":{\"25\":1}}],[\"有最好的結果\",{\"1\":{\"25\":1}}],[\"有了評斷信心水平的標準\",{\"1\":{\"60\":1}}],[\"有了\",{\"1\":{\"24\":1}}],[\"有了目標\",{\"1\":{\"8\":1}}],[\"有人天生愛保險\",{\"1\":{\"13\":1}}],[\"有人天生愛探險\",{\"1\":{\"13\":1}}],[\"有些甚至是遞增的\",{\"1\":{\"108\":1}}],[\"有些是\",{\"1\":{\"49\":1}}],[\"有些傾向\",{\"1\":{\"13\":2}}],[\"有些則不需要\",{\"1\":{\"10\":1}}],[\"有些環境需要更多的\",{\"1\":{\"10\":1}}],[\"有些\",{\"1\":{\"10\":1,\"49\":1}}],[\"有時會很不穩定\",{\"1\":{\"10\":1}}],[\"只不過是把\",{\"1\":{\"126\":1}}],[\"只不過輸入上會丟\",{\"1\":{\"9\":1}}],[\"只要我們的\",{\"1\":{\"124\":1}}],[\"只在\",{\"1\":{\"120\":1}}],[\"只選擇信度高於某個閥值的預測作為\",{\"1\":{\"115\":1}}],[\"只需要產出\",{\"1\":{\"103\":1}}],[\"只需要一張\",{\"1\":{\"72\":1}}],[\"只會每經過\",{\"1\":{\"86\":1}}],[\"只會取出最後\",{\"1\":{\"84\":1}}],[\"只會儲存最後\",{\"1\":{\"84\":1}}],[\"只是調整\",{\"1\":{\"99\":1}}],[\"只是為了\",{\"1\":{\"63\":1}}],[\"只是單純符合條件給\",{\"1\":{\"60\":1}}],[\"只是用來限制\",{\"1\":{\"6\":1}}],[\"只對簡單的\",{\"1\":{\"48\":1}}],[\"只有\",{\"1\":{\"37\":1}}],[\"只訓練在\",{\"1\":{\"34\":1,\"61\":1}}],[\"只用了一個\",{\"1\":{\"10\":1}}],[\"∣st+i​\",{\"1\":{\"105\":1}}],[\"∣st​\",{\"1\":{\"7\":1}}],[\"∣x∣​\",{\"1\":{\"103\":1}}],[\"∣z∣+1+ϵ\",{\"1\":{\"8\":1}}],[\"∣z∣+1​−1\",{\"1\":{\"8\":1}}],[\"zt\",{\"1\":{\"124\":3}}],[\"zt​∥zt\",{\"1\":{\"124\":1}}],[\"zt​\",{\"1\":{\"124\":2}}],[\"zero\",{\"1\":{\"102\":2}}],[\"ziyu\",{\"1\":{\"97\":1,\"99\":1}}],[\"zurich\",{\"1\":{\"56\":1}}],[\"zou\",{\"1\":{\"37\":1}}],[\"zhang1\",{\"1\":{\"114\":1}}],[\"zhang\",{\"1\":{\"34\":1,\"114\":2,\"115\":3,\"120\":3,\"129\":3}}],[\"z\",{\"1\":{\"8\":4}}],[\"∀z∈r\",{\"1\":{\"8\":2}}],[\"+lcet​\",{\"1\":{\"125\":1}}],[\"+\",{\"1\":{\"122\":1}}],[\"+p1​​\",{\"1\":{\"105\":1}}],[\"+p3​​\",{\"1\":{\"105\":1}}],[\"+λlv\",{\"1\":{\"100\":1}}],[\"+λh\",{\"1\":{\"41\":1}}],[\"+​a\",{\"1\":{\"99\":1}}],[\"+a\",{\"1\":{\"99\":1}}],[\"+0\",{\"1\":{\"50\":1}}],[\"+1\",{\"1\":{\"50\":2}}],[\"+2\",{\"1\":{\"50\":1}}],[\"+βkl\",{\"1\":{\"125\":1}}],[\"+βlce​\",{\"1\":{\"123\":1}}],[\"+βi=0∑k​∇θπ​​h\",{\"1\":{\"100\":1,\"105\":1}}],[\"+βnk−1​\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"+βj​q\",{\"1\":{\"12\":1}}],[\"+ϵz=sgn\",{\"1\":{\"8\":1}}],[\"+t≥0∑​γt\",{\"1\":{\"8\":1}}],[\"+s=t∑t+k−1​γs−t\",{\"1\":{\"8\":1}}],[\"x+\",{\"1\":{\"102\":1}}],[\"xm​\",{\"1\":{\"41\":4}}],[\"xs\",{\"1\":{\"60\":2,\"63\":3}}],[\"xs​=\",{\"1\":{\"60\":1,\"118\":1}}],[\"xs​\",{\"1\":{\"41\":5,\"118\":1,\"119\":1}}],[\"xsb​\",{\"1\":{\"12\":3}}],[\"xa​\",{\"1\":{\"38\":2}}],[\"x\",{\"1\":{\"8\":2,\"12\":6,\"22\":1,\"103\":2}}],[\"xt\",{\"1\":{\"60\":4,\"119\":1}}],[\"xt+1​\",{\"1\":{\"8\":3,\"19\":1}}],[\"xt​=\",{\"1\":{\"60\":1,\"118\":1}}],[\"xt​\",{\"1\":{\"8\":8,\"19\":2,\"41\":3,\"118\":1,\"119\":1,\"121\":4,\"122\":1,\"124\":8}}],[\"版本\",{\"1\":{\"8\":1}}],[\"改成透過\",{\"1\":{\"123\":1}}],[\"改成\",{\"1\":{\"8\":1}}],[\"上同樣可以獲得更多改善\",{\"1\":{\"131\":1}}],[\"上半部分是用\",{\"1\":{\"129\":1}}],[\"上也能好好地區分不同的\",{\"1\":{\"124\":1}}],[\"上也許我們能夠對各種物件去做標記\",{\"1\":{\"33\":1}}],[\"上產生比較雜亂的特徵\",{\"1\":{\"115\":1}}],[\"上適用\",{\"1\":{\"109\":1}}],[\"上較為顯著\",{\"1\":{\"107\":1}}],[\"上圖就是在最後分開成兩個輸出結果\",{\"1\":{\"99\":1}}],[\"上圖展現出\",{\"1\":{\"69\":1}}],[\"上加上\",{\"1\":{\"96\":1}}],[\"上加\",{\"1\":{\"96\":1}}],[\"上增加\",{\"1\":{\"96\":2}}],[\"上增加了\",{\"1\":{\"96\":1}}],[\"上鼓勵\",{\"1\":{\"96\":1}}],[\"上可以得到較好的結果\",{\"1\":{\"71\":1}}],[\"上多加上一項去\",{\"1\":{\"63\":1}}],[\"上述的三者分數都是以\",{\"1\":{\"61\":1}}],[\"上會發生\",{\"1\":{\"40\":1}}],[\"上訓練\",{\"1\":{\"104\":1,\"105\":1}}],[\"上訓練一個模型\",{\"1\":{\"37\":1}}],[\"上訓練的模型難以直接\",{\"1\":{\"33\":1}}],[\"上\",{\"1\":{\"33\":1,\"38\":2,\"52\":1,\"57\":1,\"63\":1,\"66\":1,\"71\":2,\"78\":1,\"103\":1,\"107\":1}}],[\"上面的表現\",{\"1\":{\"128\":1}}],[\"上面的差異就是這裡傳入的分別是\",{\"1\":{\"12\":1}}],[\"上面基本的做法作者稱他為\",{\"1\":{\"103\":1}}],[\"上面提及的是單純的\",{\"1\":{\"8\":1}}],[\"上的重要性\",{\"1\":{\"57\":1}}],[\"上的認知\",{\"1\":{\"37\":1}}],[\"上的時候\",{\"1\":{\"34\":1}}],[\"上的例子\",{\"1\":{\"33\":1}}],[\"上的\",{\"1\":{\"8\":1,\"36\":1,\"87\":1}}],[\"−∥f\",{\"1\":{\"124\":2}}],[\"−∥f~​\",{\"1\":{\"121\":2,\"124\":2}}],[\"−η\",{\"1\":{\"121\":2,\"124\":4}}],[\"−scorerandom​scoreagent​−scorebaseline​​\",{\"1\":{\"107\":1}}],[\"−p1​​\",{\"1\":{\"105\":1}}],[\"−p3​​\",{\"1\":{\"105\":1}}],[\"−∣a∣1​a\",{\"1\":{\"99\":1}}],[\"−v\",{\"1\":{\"99\":1}}],[\"−trt\",{\"1\":{\"80\":1}}],[\"−t^r\",{\"1\":{\"12\":1}}],[\"−fθ​\",{\"1\":{\"63\":1}}],[\"−1\",{\"1\":{\"8\":1,\"86\":1}}],[\"−h−1q\",{\"1\":{\"8\":1}}],[\"−yt​^​\",{\"1\":{\"8\":1}}],[\"−q\",{\"1\":{\"8\":1,\"97\":1,\"99\":1,\"104\":4}}],[\"θ=defμ+σ⊙ϵ\",{\"1\":{\"102\":1,\"108\":1}}],[\"θv\",{\"1\":{\"100\":3}}],[\"θv​\",{\"1\":{\"99\":1,\"100\":5,\"105\":3}}],[\"θπ​\",{\"1\":{\"100\":5,\"105\":3}}],[\"θa​\",{\"1\":{\"99\":1}}],[\"θ~=θ+n\",{\"1\":{\"96\":1}}],[\"θt​\",{\"1\":{\"60\":1}}],[\"θl​\",{\"1\":{\"19\":1}}],[\"θi−1​\",{\"1\":{\"80\":2}}],[\"θi​\",{\"1\":{\"80\":3}}],[\"θi\",{\"1\":{\"12\":2}}],[\"θe∪θi\",{\"1\":{\"12\":1}}],[\"θe\",{\"1\":{\"12\":2}}],[\"θ\",{\"1\":{\"8\":2,\"12\":6,\"41\":1,\"97\":2,\"98\":2,\"99\":8,\"100\":3,\"102\":3,\"104\":5}}],[\"θ−\",{\"1\":{\"8\":2,\"12\":3,\"97\":2,\"98\":3,\"99\":1,\"104\":2}}],[\"一次\",{\"1\":{\"107\":1}}],[\"一開始都不太相同\",{\"1\":{\"86\":1}}],[\"一開始都是一樣的\",{\"1\":{\"13\":1}}],[\"一開始我們一樣先看一下這一篇論文當中會用到的\",{\"1\":{\"60\":1}}],[\"一起\",{\"1\":{\"41\":1}}],[\"一個假的\",{\"1\":{\"115\":1}}],[\"一個\",{\"1\":{\"81\":1}}],[\"一個常見的問題是產出的結果通常會傾向去預測結果為常見的\",{\"1\":{\"37\":1}}],[\"一個簡單的方法是想辦法給這些\",{\"1\":{\"37\":1}}],[\"一個是\",{\"1\":{\"8\":2}}],[\"一些\",{\"1\":{\"19\":1,\"37\":1}}],[\"一樣好\",{\"1\":{\"107\":1}}],[\"一樣糟\",{\"1\":{\"107\":1}}],[\"一樣\",{\"1\":{\"8\":1,\"43\":1}}],[\"​exp\",{\"1\":{\"121\":1,\"124\":2}}],[\"​otherwise​\",{\"1\":{\"119\":1}}],[\"​if\",{\"1\":{\"119\":1}}],[\"​log\",{\"1\":{\"119\":1}}],[\"​logπ\",{\"1\":{\"100\":1}}],[\"​loggθ​\",{\"1\":{\"60\":2}}],[\"​∑j=1hf​×wf​​d\",{\"1\":{\"63\":1}}],[\"​⋅\",{\"1\":{\"63\":1}}],[\"​pt\",{\"1\":{\"60\":1,\"119\":1,\"120\":1,\"123\":1}}],[\"​hϕ​\",{\"1\":{\"60\":2}}],[\"​​\",{\"1\":{\"14\":1,\"15\":1,\"63\":1,\"100\":2}}],[\"​​​∀0≤k≤n−1∀n≤k≤k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​==1\",{\"1\":{\"122\":2}}],[\"​=∑k\",{\"1\":{\"121\":1,\"124\":2}}],[\"​=∑j​mthings\",{\"1\":{\"63\":1}}],[\"​=ξ\",{\"1\":{\"120\":1,\"123\":1}}],[\"​=−e\",{\"1\":{\"105\":1}}],[\"​=−eπ\",{\"1\":{\"100\":1,\"105\":1}}],[\"​=−j=1∑h×w​c=1∑c​qt\",{\"1\":{\"60\":1}}],[\"​=−j=1∑h×w​c=1∑c​ys\",{\"1\":{\"60\":1}}],[\"​=c\",{\"1\":{\"63\":1}}],[\"​=h⋅w∑j=1h×w​\",{\"1\":{\"60\":1}}],[\"​=\",{\"1\":{\"60\":1,\"63\":1,\"119\":1}}],[\"​=m=max\",{\"1\":{\"15\":1}}],[\"​=m=0∑k−1​1\",{\"1\":{\"14\":1}}],[\"​=nk​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​=sgn\",{\"1\":{\"8\":1}}],[\"​−1​\",{\"1\":{\"8\":1}}],[\"​\",{\"1\":{\"8\":3,\"60\":11,\"62\":3,\"63\":5,\"80\":3,\"99\":1,\"100\":3,\"103\":1,\"105\":1,\"119\":4,\"120\":5,\"121\":1,\"122\":1,\"123\":1,\"124\":5,\"125\":2}}],[\"δth​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δth​\",{\"1\":{\"8\":1}}],[\"δt​cs​​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δs​\",{\"1\":{\"8\":1}}],[\"=∑xt​∈xt​​∑i​1\",{\"1\":{\"122\":1}}],[\"=∑c\",{\"1\":{\"62\":1}}],[\"=sgn\",{\"1\":{\"103\":1}}],[\"=f\",{\"1\":{\"103\":1}}],[\"=∇e\",{\"1\":{\"102\":1}}],[\"=lπ\",{\"1\":{\"100\":1}}],[\"=i=0∑k​eπ\",{\"1\":{\"100\":1,\"105\":1}}],[\"=v\",{\"1\":{\"99\":2}}],[\"=t∑t​γt\",{\"1\":{\"80\":1}}],[\"=πmax​e\",{\"1\":{\"80\":1}}],[\"=1∑c​ys\",{\"1\":{\"63\":1}}],[\"=1c​e1−fc\",{\"1\":{\"62\":1}}],[\"=∥fimagenet​\",{\"1\":{\"63\":1}}],[\"=b=0∑b−1​s=t∑t+h−1​\",{\"1\":{\"12\":1}}],[\"=arga∈amax​q\",{\"1\":{\"12\":1}}],[\"=\",{\"1\":{\"8\":1,\"60\":1}}],[\"=λmin\",{\"1\":{\"8\":1}}],[\"=q\",{\"1\":{\"8\":1,\"12\":1,\"99\":1}}],[\"=es\",{\"1\":{\"80\":1}}],[\"=eμ​\",{\"1\":{\"8\":1}}],[\"=e\",{\"1\":{\"7\":1,\"41\":1,\"97\":1,\"99\":1,\"102\":2,\"104\":4,\"105\":1}}],[\"定義底下的平均\",{\"1\":{\"108\":1}}],[\"定義一個\",{\"1\":{\"62\":1}}],[\"定義\",{\"1\":{\"8\":1,\"63\":2,\"80\":1}}],[\"τ∈n∗\",{\"1\":{\"15\":1}}],[\"τ=\",{\"1\":{\"8\":1}}],[\"τ\",{\"1\":{\"8\":1,\"15\":8,\"16\":2,\"60\":1,\"121\":3,\"124\":4}}],[\"π​q\",{\"1\":{\"12\":1}}],[\"π\",{\"1\":{\"8\":2,\"12\":4,\"80\":2,\"100\":3,\"105\":3}}],[\"μi\",{\"1\":{\"105\":2}}],[\"μ+σ⊙ξ\",{\"1\":{\"102\":1}}],[\"μ+σ⊙ϵ\",{\"1\":{\"102\":1}}],[\"μb+σb⊙ϵb\",{\"1\":{\"102\":1}}],[\"μw+σw⊙ϵw\",{\"1\":{\"102\":1}}],[\"μ^​k​\",{\"1\":{\"14\":2,\"15\":2}}],[\"μ\",{\"1\":{\"8\":3,\"12\":3,\"102\":1,\"108\":1}}],[\"演算法\",{\"1\":{\"8\":1}}],[\"計算方式如下\",{\"1\":{\"125\":1}}],[\"計算上負擔過大\",{\"1\":{\"122\":1}}],[\"計算的是\",{\"1\":{\"121\":1}}],[\"計算分別如下\",{\"1\":{\"100\":1}}],[\"計算\",{\"0\":{\"122\":1,\"123\":1},\"1\":{\"8\":1,\"19\":1}}],[\"那我們就可以用\",{\"1\":{\"37\":1}}],[\"那也就會有\",{\"1\":{\"8\":1}}],[\"那麼這裡加上的\",{\"1\":{\"108\":1}}],[\"那麼就會有更高的機會在\",{\"1\":{\"57\":1}}],[\"那麼\",{\"1\":{\"3\":1,\"103\":1,\"124\":1}}],[\"選擇一個\",{\"1\":{\"120\":1}}],[\"選擇採用\",{\"1\":{\"43\":1}}],[\"選擇中最大的\",{\"1\":{\"24\":1}}],[\"選擇出一組\",{\"1\":{\"19\":1}}],[\"選擇出現傾向\",{\"1\":{\"13\":1}}],[\"選擇其中最大的當成這次的選擇\",{\"1\":{\"14\":1}}],[\"選擇\",{\"1\":{\"13\":1,\"19\":1}}],[\"選擇的分布\",{\"1\":{\"7\":1}}],[\"選大一些\",{\"1\":{\"7\":1}}],[\"選小一些\",{\"1\":{\"7\":1}}],[\"需要\",{\"1\":{\"100\":1}}],[\"需要額外的\",{\"1\":{\"100\":1}}],[\"需要特別注意到對於\",{\"1\":{\"80\":1}}],[\"需要透過與環境互動取得\",{\"1\":{\"78\":1}}],[\"需要看遠一些\",{\"1\":{\"7\":1}}],[\"需要相當大量的探索之後才能得到\",{\"1\":{\"3\":1}}],[\"小\",{\"1\":{\"7\":1,\"15\":1}}],[\"傾向在\",{\"1\":{\"115\":1}}],[\"傾向\",{\"1\":{\"7\":2}}],[\"大小\",{\"1\":{\"13\":1}}],[\"大小為\",{\"1\":{\"12\":1}}],[\"大\",{\"1\":{\"7\":1}}],[\"γ2​=0\",{\"1\":{\"124\":1}}],[\"γ1​=10\",{\"1\":{\"124\":1}}],[\"γ=0\",{\"1\":{\"24\":1}}],[\"γj​\",{\"1\":{\"13\":1,\"19\":1}}],[\"γ\",{\"1\":{\"7\":4,\"16\":1,\"21\":1,\"24\":1,\"80\":1}}],[\"qi​\",{\"1\":{\"100\":1}}],[\"qi​−v\",{\"1\":{\"100\":1,\"105\":2}}],[\"q∗\",{\"1\":{\"80\":4,\"97\":2,\"99\":2}}],[\"qt\",{\"1\":{\"60\":2}}],[\"quan\",{\"1\":{\"19\":1}}],[\"qπ\",{\"1\":{\"8\":1}}],[\"q\",{\"0\":{\"80\":1},\"1\":{\"7\":2,\"8\":6,\"12\":3,\"19\":1,\"77\":1,\"79\":3,\"80\":2,\"82\":2,\"84\":2,\"86\":1,\"87\":1,\"97\":1,\"99\":4}}],[\"去計算\",{\"1\":{\"124\":1}}],[\"去調整\",{\"1\":{\"123\":1}}],[\"去調整選擇不同\",{\"1\":{\"13\":1}}],[\"去決定要加怎樣的\",{\"1\":{\"102\":1}}],[\"去加上\",{\"1\":{\"102\":1}}],[\"去儲存\",{\"1\":{\"100\":1}}],[\"去避免訓練資料上的強關聯性\",{\"1\":{\"100\":1}}],[\"去試圖得到\",{\"1\":{\"97\":1}}],[\"去增加\",{\"1\":{\"94\":1}}],[\"去比較\",{\"1\":{\"88\":1}}],[\"去選擇\",{\"1\":{\"84\":1}}],[\"去學\",{\"1\":{\"82\":1}}],[\"去學習\",{\"1\":{\"13\":1,\"22\":1,\"60\":1,\"84\":1,\"97\":1,\"99\":1}}],[\"去學習導致\",{\"1\":{\"10\":1}}],[\"去預測\",{\"1\":{\"81\":1,\"119\":1}}],[\"去提升\",{\"1\":{\"69\":1}}],[\"去初始化權重\",{\"1\":{\"63\":1}}],[\"去定義如下\",{\"1\":{\"62\":1}}],[\"去處理\",{\"1\":{\"61\":1}}],[\"去評估取得\",{\"1\":{\"61\":1}}],[\"去更新\",{\"1\":{\"60\":1}}],[\"去更新參數學習\",{\"1\":{\"9\":1}}],[\"去訓練\",{\"1\":{\"57\":1,\"83\":1,\"102\":1}}],[\"去訓練顯然很糟糕\",{\"1\":{\"48\":1,\"49\":1}}],[\"去\",{\"1\":{\"41\":1,\"48\":1,\"80\":1}}],[\"去轉移出來\",{\"1\":{\"37\":1}}],[\"去判別現在給我的究竟是\",{\"1\":{\"36\":1}}],[\"去拉近\",{\"1\":{\"36\":1}}],[\"去紀錄訓練過程當中的\",{\"1\":{\"21\":1}}],[\"去跟環境互動\",{\"1\":{\"19\":1}}],[\"去得到目標\",{\"1\":{\"8\":1}}],[\"去逼近\",{\"1\":{\"8\":1}}],[\"去近似\",{\"1\":{\"7\":1,\"102\":1}}],[\"去找到藏在地圖當中的寶藏\",{\"1\":{\"3\":1}}],[\"f~​\",{\"1\":{\"121\":1,\"124\":1}}],[\"f\",{\"1\":{\"103\":2,\"119\":1,\"121\":1,\"124\":1}}],[\"fitted\",{\"1\":{\"79\":1}}],[\"fine\",{\"1\":{\"37\":1}}],[\"fc​=ns​⋅h⋅w∑i=1ns​​∑j=1h×w​\",{\"1\":{\"62\":1}}],[\"fc​\",{\"1\":{\"62\":1}}],[\"fd\",{\"0\":{\"63\":1,\"70\":1},\"1\":{\"61\":1,\"67\":2,\"70\":1}}],[\"fence\",{\"1\":{\"47\":1}}],[\"features\",{\"1\":{\"63\":3,\"88\":1,\"122\":1,\"124\":2}}],[\"feature\",{\"0\":{\"63\":1,\"70\":1},\"1\":{\"36\":1,\"61\":3,\"63\":3,\"119\":1,\"120\":1,\"121\":2,\"122\":2,\"124\":4}}],[\"fθ​\",{\"1\":{\"41\":3}}],[\"fortunato\",{\"1\":{\"93\":1,\"107\":3,\"108\":1}}],[\"for\",{\"0\":{\"55\":1,\"60\":1,\"64\":1,\"92\":1,\"96\":1,\"113\":1},\"1\":{\"28\":1,\"37\":1,\"52\":4,\"56\":1,\"61\":3,\"73\":2,\"95\":1,\"102\":1,\"110\":2,\"131\":1,\"132\":3}}],[\"follow\",{\"1\":{\"8\":1,\"12\":1,\"80\":1}}],[\"fang\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"factorised\",{\"1\":{\"103\":1,\"104\":1,\"105\":1}}],[\"factor\",{\"1\":{\"24\":1,\"80\":1}}],[\"family\",{\"0\":{\"13\":1}}],[\"free\",{\"1\":{\"81\":1,\"82\":1}}],[\"freeway\",{\"1\":{\"24\":1}}],[\"frame\",{\"1\":{\"86\":1}}],[\"framework\",{\"1\":{\"51\":1,\"66\":1}}],[\"frames\",{\"1\":{\"19\":1,\"84\":1,\"86\":2,\"107\":2}}],[\"from\",{\"1\":{\"3\":2,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"33\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"56\":1,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":3,\"71\":2,\"84\":1,\"87\":1,\"88\":1,\"96\":2,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"107\":3,\"108\":1,\"115\":1,\"116\":1,\"120\":1,\"129\":1}}],[\"functions\",{\"1\":{\"110\":1}}],[\"functiona\",{\"1\":{\"99\":1}}],[\"function\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"63\":2,\"79\":1,\"80\":1,\"81\":1,\"82\":1,\"84\":1,\"97\":3,\"99\":2,\"100\":4,\"105\":1,\"121\":1}}],[\"會採用\",{\"1\":{\"125\":1}}],[\"會做為\",{\"1\":{\"125\":1}}],[\"會很容易忽略了細部的特徵\",{\"1\":{\"120\":1}}],[\"會想要讓\",{\"1\":{\"120\":1}}],[\"會太慢\",{\"1\":{\"120\":1}}],[\"會使模型被誤導\",{\"1\":{\"115\":1}}],[\"會使得\",{\"1\":{\"6\":1}}],[\"會\",{\"1\":{\"107\":1}}],[\"會比較大\",{\"1\":{\"104\":1}}],[\"會直接去學\",{\"1\":{\"100\":1}}],[\"會透過\",{\"1\":{\"84\":1}}],[\"會將\",{\"1\":{\"84\":1}}],[\"會有幾個明顯的問題\",{\"1\":{\"78\":1}}],[\"會有怎樣的影響\",{\"1\":{\"61\":1}}],[\"會有大量的下降\",{\"1\":{\"61\":1}}],[\"會有更好的\",{\"1\":{\"61\":1}}],[\"會有兩組總和\",{\"1\":{\"12\":1}}],[\"會有兩個\",{\"1\":{\"8\":1}}],[\"會期待預測出來的\",{\"1\":{\"60\":1}}],[\"會造成的問題是吻合的\",{\"1\":{\"40\":1}}],[\"會不同\",{\"1\":{\"34\":1}}],[\"會導致訓練前期較為緩慢\",{\"1\":{\"23\":1}}],[\"會掉\",{\"1\":{\"22\":1}}],[\"會偏向\",{\"1\":{\"22\":1}}],[\"會得到\",{\"1\":{\"22\":1}}],[\"會從\",{\"1\":{\"19\":1}}],[\"會把每個\",{\"1\":{\"13\":1}}],[\"會選擇不同的\",{\"1\":{\"7\":1}}],[\"會一直到遊戲的最後依照最後通過的時間決定\",{\"1\":{\"3\":1}}],[\"β=1\",{\"1\":{\"123\":1}}],[\"β=0\",{\"1\":{\"9\":1}}],[\"βj​=0\",{\"1\":{\"22\":2}}],[\"βj​=maxj​βj​\",{\"1\":{\"22\":2}}],[\"βj​\",{\"1\":{\"13\":1,\"19\":1,\"22\":4}}],[\"β\",{\"1\":{\"6\":1,\"7\":1,\"16\":1,\"22\":6,\"99\":4,\"100\":1}}],[\"βi​\",{\"1\":{\"6\":1,\"7\":5,\"9\":2,\"10\":1}}],[\"當距離很小時\",{\"1\":{\"121\":1}}],[\"當距離很大時\",{\"1\":{\"121\":1}}],[\"當然\",{\"1\":{\"120\":1}}],[\"當作輸入\",{\"1\":{\"89\":1}}],[\"當要去更新模型的時候\",{\"1\":{\"84\":1}}],[\"當兩個\",{\"1\":{\"33\":1}}],[\"當\",{\"1\":{\"6\":1,\"22\":2}}],[\"當中中心點的\",{\"1\":{\"122\":1}}],[\"當中使用了\",{\"1\":{\"100\":1}}],[\"當中使用的是\",{\"1\":{\"38\":1}}],[\"當中加上\",{\"1\":{\"96\":1}}],[\"當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",{\"1\":{\"94\":1}}],[\"當中就是環境給予的\",{\"1\":{\"6\":1}}],[\"當中並沒有\",{\"1\":{\"87\":1}}],[\"當中如果要評估一個\",{\"1\":{\"87\":1}}],[\"當中的\",{\"1\":{\"84\":1,\"100\":1,\"119\":1,\"125\":1}}],[\"當中取得隨機幾筆去更新\",{\"1\":{\"84\":1}}],[\"當中取得的\",{\"1\":{\"8\":1}}],[\"當中我們需要同時訓練兩個\",{\"1\":{\"98\":1}}],[\"當中我們往往仰賴對\",{\"1\":{\"94\":1}}],[\"當中我們看到了使用\",{\"1\":{\"84\":1}}],[\"當中我們會透過\",{\"1\":{\"80\":1}}],[\"當中我們會預設資料之間是沒有什麼相依性的\",{\"1\":{\"78\":1}}],[\"當中同一個\",{\"1\":{\"78\":1}}],[\"當中呢\",{\"1\":{\"78\":1}}],[\"當中常見的\",{\"1\":{\"66\":1}}],[\"當中有包含的\",{\"1\":{\"63\":1}}],[\"當中包含的\",{\"1\":{\"61\":1}}],[\"當中通常\",{\"1\":{\"37\":1}}],[\"當中都獲得了超過人類的成效\",{\"1\":{\"25\":1}}],[\"當中獲得比人類平均還要好的成果\",{\"1\":{\"25\":1}}],[\"當中獲得相當不錯的\",{\"1\":{\"3\":1}}],[\"當中是小許多的\",{\"1\":{\"24\":1}}],[\"當中你可以得到最好的\",{\"1\":{\"14\":1}}],[\"當中\",{\"1\":{\"3\":1,\"6\":1,\"19\":1,\"40\":1,\"84\":1,\"115\":1,\"120\":1,\"122\":1}}],[\"dθv​←dθv​+∂\",{\"1\":{\"100\":1}}],[\"dθ←dθ+∇θ\",{\"1\":{\"100\":1}}],[\"dueling\",{\"0\":{\"99\":1,\"104\":1},\"1\":{\"95\":1,\"99\":3,\"104\":2,\"107\":2}}],[\"d=e1​\",{\"1\":{\"84\":1}}],[\"dl\",{\"1\":{\"78\":3}}],[\"degeneration\",{\"1\":{\"124\":1}}],[\"denoising\",{\"0\":{\"113\":1,\"120\":1},\"1\":{\"115\":1,\"126\":1,\"132\":1}}],[\"dengxin\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"deterministic\",{\"1\":{\"108\":2}}],[\"details\",{\"1\":{\"105\":1}}],[\"depthwise\",{\"1\":{\"71\":1}}],[\"decoder\",{\"0\":{\"71\":1},\"1\":{\"61\":3,\"66\":1,\"71\":1}}],[\"deep\",{\"0\":{\"76\":1},\"1\":{\"77\":1,\"78\":4,\"87\":1,\"89\":2,\"110\":2}}],[\"deeplabv2\",{\"1\":{\"61\":1,\"68\":1,\"128\":1}}],[\"deeplabv3+\",{\"1\":{\"61\":1}}],[\"deeplab\",{\"1\":{\"43\":1,\"48\":1,\"57\":1}}],[\"deepmind\",{\"1\":{\"2\":1,\"93\":1}}],[\"dt​\",{\"1\":{\"41\":1}}],[\"ds​\",{\"1\":{\"41\":1}}],[\"dong\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"downsample\",{\"1\":{\"63\":1}}],[\"domain\",{\"0\":{\"31\":2,\"33\":1,\"36\":1,\"41\":2,\"55\":1,\"113\":1},\"1\":{\"33\":14,\"34\":5,\"36\":7,\"37\":10,\"40\":2,\"41\":6,\"48\":2,\"49\":1,\"52\":5,\"57\":1,\"58\":1,\"60\":6,\"61\":3,\"62\":1,\"70\":1,\"73\":2,\"115\":5,\"120\":2,\"124\":5,\"125\":2,\"126\":2,\"129\":1,\"132\":1},\"2\":{\"54\":1,\"75\":1,\"134\":1}}],[\"double\",{\"0\":{\"98\":1},\"1\":{\"8\":2,\"95\":1,\"98\":4,\"99\":1}}],[\"divergence代表什麼意義嗎\",{\"1\":{\"132\":1}}],[\"divergence\",{\"1\":{\"124\":1}}],[\"differences\",{\"1\":{\"110\":1}}],[\"dimensional\",{\"1\":{\"78\":1}}],[\"dilation\",{\"1\":{\"61\":1,\"66\":1}}],[\"distance\",{\"0\":{\"63\":1,\"70\":1},\"1\":{\"61\":1,\"63\":1}}],[\"distillation\",{\"0\":{\"125\":1},\"1\":{\"60\":1,\"125\":1,\"126\":1,\"128\":1,\"131\":1}}],[\"distribution\",{\"1\":{\"34\":1,\"87\":1,\"97\":1}}],[\"distributed\",{\"0\":{\"105\":1},\"1\":{\"28\":2}}],[\"discriminator\",{\"1\":{\"36\":1}}],[\"discussion\",{\"0\":{\"26\":1}}],[\"discount\",{\"1\":{\"24\":1,\"80\":2}}],[\"directed\",{\"1\":{\"28\":1}}],[\"dai\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"daformer\",{\"0\":{\"55\":1,\"61\":1,\"71\":1},\"1\":{\"56\":1,\"60\":1,\"61\":1,\"62\":1,\"67\":1,\"71\":3,\"73\":5}}],[\"day\",{\"1\":{\"52\":2}}],[\"datasets\",{\"1\":{\"66\":1,\"128\":2}}],[\"dataset\",{\"0\":{\"44\":1},\"1\":{\"40\":6,\"61\":5,\"66\":1,\"118\":3,\"119\":1,\"122\":1,\"128\":1}}],[\"data\",{\"1\":{\"33\":1,\"34\":2,\"36\":2,\"37\":7,\"38\":2,\"52\":1,\"60\":1,\"115\":1,\"121\":1,\"124\":2,\"126\":1}}],[\"dacs\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"36\":1,\"38\":1,\"41\":1,\"48\":1,\"49\":1,\"50\":3,\"52\":1,\"66\":1,\"115\":1,\"128\":1}}],[\"david\",{\"1\":{\"19\":1,\"77\":1,\"84\":1,\"87\":1,\"88\":1,\"98\":1}}],[\"dan\",{\"1\":{\"19\":1}}],[\"d\",{\"1\":{\"12\":2,\"63\":2,\"84\":1,\"97\":1}}],[\"dqn\",{\"0\":{\"97\":1,\"98\":1,\"99\":1,\"104\":2},\"1\":{\"8\":1,\"77\":1,\"83\":2,\"84\":2,\"88\":3,\"95\":3,\"97\":1,\"98\":8,\"99\":6,\"100\":1,\"104\":4,\"107\":3}}],[\"driven\",{\"1\":{\"6\":1}}],[\"ce\",{\"1\":{\"123\":1}}],[\"cluster\",{\"1\":{\"120\":3,\"124\":1}}],[\"clustering\",{\"1\":{\"120\":1}}],[\"classifier\",{\"1\":{\"119\":1}}],[\"classes\",{\"1\":{\"38\":1,\"45\":1,\"46\":2,\"47\":3,\"49\":1,\"50\":2,\"60\":1,\"61\":2,\"62\":4,\"63\":4,\"67\":2,\"118\":1,\"129\":1}}],[\"classmix\",{\"1\":{\"38\":2,\"40\":1,\"41\":1,\"43\":1,\"51\":1,\"52\":1,\"60\":1}}],[\"class\",{\"0\":{\"62\":1,\"63\":1,\"69\":1,\"70\":1},\"1\":{\"37\":2,\"40\":5,\"48\":2,\"49\":1,\"52\":1,\"61\":2,\"62\":3,\"63\":2,\"67\":1,\"69\":2,\"70\":1,\"71\":1,\"119\":2,\"120\":1,\"121\":2,\"122\":2,\"123\":4,\"124\":1}}],[\"cthings​\",{\"1\":{\"63\":1}}],[\"c=argmaxc\",{\"1\":{\"60\":1}}],[\"cce\",{\"1\":{\"60\":1}}],[\"c\",{\"1\":{\"60\":9,\"62\":5,\"63\":4}}],[\"cityscape\",{\"1\":{\"51\":1,\"61\":3}}],[\"cityscapes\",{\"0\":{\"45\":1,\"48\":1,\"49\":1,\"129\":1,\"130\":1},\"1\":{\"44\":3,\"46\":1,\"47\":1,\"50\":1,\"66\":1,\"70\":1,\"128\":1,\"129\":1,\"130\":1}}],[\"city\",{\"1\":{\"47\":1}}],[\"cbst\",{\"1\":{\"37\":1}}],[\"cnn\",{\"1\":{\"34\":2,\"61\":1,\"64\":1,\"78\":1}}],[\"cvpr2022\",{\"1\":{\"73\":1}}],[\"cvpr22\",{\"1\":{\"73\":1}}],[\"cvpr\",{\"1\":{\"56\":1,\"114\":1},\"2\":{\"75\":1,\"134\":1}}],[\"cv\",{\"1\":{\"33\":1}}],[\"caution\",{\"1\":{\"126\":1}}],[\"categorical\",{\"1\":{\"60\":1}}],[\"carlo\",{\"1\":{\"102\":1}}],[\"car\",{\"1\":{\"48\":1}}],[\"cars\",{\"1\":{\"32\":1}}],[\"capped\",{\"1\":{\"21\":1,\"25\":2}}],[\"china\",{\"1\":{\"114\":1}}],[\"chiehchen\",{\"1\":{\"34\":1}}],[\"chen\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"channel\",{\"1\":{\"88\":1}}],[\"channels\",{\"1\":{\"61\":1}}],[\"chalmers\",{\"1\":{\"32\":1}}],[\"chns=max\",{\"1\":{\"21\":1}}],[\"chns\",{\"1\":{\"21\":1}}],[\"critic\",{\"1\":{\"100\":1,\"110\":1}}],[\"cross\",{\"0\":{\"31\":1},\"1\":{\"41\":1,\"52\":1,\"60\":1,\"119\":1,\"123\":2,\"132\":1}}],[\"credit\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"count\",{\"1\":{\"110\":1}}],[\"compact\",{\"1\":{\"115\":1}}],[\"computer\",{\"1\":{\"32\":1,\"52\":1,\"83\":1},\"2\":{\"54\":1,\"75\":1,\"134\":1}}],[\"common\",{\"1\":{\"62\":1}}],[\"color\",{\"1\":{\"60\":1,\"88\":1}}],[\"column\",{\"1\":{\"37\":1}}],[\"cordts\",{\"1\":{\"45\":1}}],[\"corss\",{\"0\":{\"41\":1}}],[\"coin\",{\"1\":{\"22\":6}}],[\"consistency\",{\"0\":{\"124\":1}}],[\"convolution\",{\"1\":{\"71\":1}}],[\"contingency\",{\"1\":{\"88\":1}}],[\"context\",{\"1\":{\"36\":1}}],[\"contrast\",{\"1\":{\"132\":2}}],[\"contribution\",{\"0\":{\"27\":1,\"51\":1,\"72\":1,\"89\":1,\"109\":1,\"131\":1}}],[\"controller\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":7,\"25\":2,\"27\":1}}],[\"control\",{\"1\":{\"8\":1,\"82\":2,\"88\":1}}],[\"conflation\",{\"1\":{\"40\":1}}],[\"conference\",{\"1\":{\"32\":1}}],[\"confidence\",{\"0\":{\"14\":1}}],[\"conroller\",{\"1\":{\"24\":1}}],[\"curiosity\",{\"1\":{\"6\":1}}],[\"1m\",{\"1\":{\"107\":1}}],[\"15\",{\"1\":{\"70\":1}}],[\"15×15\",{\"1\":{\"22\":1}}],[\"1k\",{\"1\":{\"66\":1}}],[\"18\",{\"1\":{\"66\":1}}],[\"120\",{\"1\":{\"70\":1}}],[\"12\",{\"1\":{\"66\":1}}],[\"12326\",{\"1\":{\"24\":1}}],[\"1−λ\",{\"1\":{\"122\":1}}],[\"1−fc​\",{\"1\":{\"62\":1}}],[\"1−α\",{\"1\":{\"60\":1}}],[\"17\",{\"1\":{\"50\":1}}],[\"1774\",{\"1\":{\"24\":1}}],[\"13\",{\"1\":{\"47\":2,\"49\":2,\"50\":1}}],[\"19\",{\"1\":{\"45\":1,\"46\":1}}],[\"1187\",{\"1\":{\"24\":1}}],[\"11361\",{\"1\":{\"24\":1}}],[\"1177\",{\"1\":{\"24\":1}}],[\"14814\",{\"1\":{\"24\":1}}],[\"14\",{\"1\":{\"24\":1,\"67\":1}}],[\"16\",{\"1\":{\"24\":1,\"47\":1,\"49\":2,\"50\":1,\"72\":1}}],[\"1664\",{\"1\":{\"24\":1}}],[\"16926\",{\"1\":{\"24\":1}}],[\"160\",{\"1\":{\"17\":1}}],[\"108k\",{\"1\":{\"107\":1}}],[\"10​ifpistrueotherwise​\",{\"1\":{\"60\":1}}],[\"101\",{\"1\":{\"48\":1,\"128\":2}}],[\"100×max\",{\"1\":{\"107\":1}}],[\"100×scorehuman​−scorerandom​scoreagent​−scorerandom​​\",{\"1\":{\"107\":1}}],[\"100\",{\"1\":{\"25\":1,\"107\":1}}],[\"10362\",{\"1\":{\"24\":1}}],[\"10\",{\"1\":{\"23\":1,\"24\":3,\"50\":1}}],[\"1​​yk​​∀0≤k≤n−1∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"1​m=max\",{\"1\":{\"15\":1}}],[\"1​m=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"1\",{\"1\":{\"6\":1,\"8\":1,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"22\":2,\"50\":1,\"60\":1,\"66\":1,\"67\":2,\"86\":1,\"119\":1,\"121\":1,\"123\":1,\"124\":1}}],[\"如上面\",{\"1\":{\"70\":1}}],[\"如同前面看過的\",{\"1\":{\"115\":1}}],[\"如同前面提及\",{\"1\":{\"22\":1}}],[\"如同過去看過的\",{\"1\":{\"57\":1}}],[\"如\",{\"1\":{\"24\":1,\"63\":2,\"78\":2}}],[\"如果搭配\",{\"1\":{\"68\":1}}],[\"如果我們用更加強大的\",{\"1\":{\"57\":1}}],[\"如果我們想要訓練一個模型去做自駕車的街景物件偵測\",{\"1\":{\"33\":1}}],[\"如果出現道路或甚至機車\",{\"1\":{\"37\":1}}],[\"如果選擇較大\",{\"1\":{\"22\":1}}],[\"如果\",{\"1\":{\"15\":1}}],[\"如果每個\",{\"1\":{\"13\":1}}],[\"如此一來就能在\",{\"1\":{\"63\":1}}],[\"如此一來\",{\"1\":{\"13\":1,\"41\":1,\"103\":1,\"124\":1}}],[\"如下圖所示\",{\"1\":{\"61\":1}}],[\"如下\",{\"1\":{\"6\":1,\"19\":1,\"62\":1,\"63\":2,\"80\":1}}],[\"如何計算\",{\"1\":{\"123\":1}}],[\"如何讓\",{\"1\":{\"3\":1}}],[\"如何決定哪些\",{\"1\":{\"3\":1}}],[\"lces​\",{\"1\":{\"126\":1}}],[\"lcet​=−i=1∑h×w​k=1∑k​y^​t\",{\"1\":{\"119\":1}}],[\"lkd​=lces​\",{\"1\":{\"125\":1}}],[\"lkd​\",{\"1\":{\"125\":1,\"126\":1}}],[\"lklt​=kl\",{\"1\":{\"124\":1}}],[\"lklt​\",{\"1\":{\"124\":1}}],[\"lregt​=−i=1∑h×w​j=1∑k​logpt\",{\"1\":{\"124\":1}}],[\"lregt​\",{\"1\":{\"124\":1}}],[\"lˉ\",{\"1\":{\"102\":1,\"104\":2}}],[\"lv\",{\"1\":{\"100\":1,\"105\":2}}],[\"l=ls​+lt​+λfd​lfd​\",{\"1\":{\"63\":1}}],[\"l=5\",{\"1\":{\"6\":1}}],[\"lfd\",{\"1\":{\"63\":1}}],[\"ltotoal​\",{\"1\":{\"126\":1}}],[\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",{\"1\":{\"124\":1}}],[\"ltotal​\",{\"1\":{\"124\":1}}],[\"lt\",{\"1\":{\"60\":1}}],[\"lscet​=αlce​\",{\"1\":{\"123\":1}}],[\"ls​\",{\"1\":{\"63\":1}}],[\"ls\",{\"1\":{\"60\":1}}],[\"luc\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"lukas\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"layer\",{\"1\":{\"81\":2,\"102\":1,\"108\":2}}],[\"layout\",{\"1\":{\"36\":2}}],[\"labels\",{\"1\":{\"57\":1,\"60\":2,\"115\":1,\"116\":1,\"118\":1,\"119\":3,\"120\":2,\"123\":2}}],[\"labelled\",{\"1\":{\"40\":1}}],[\"labelling\",{\"0\":{\"37\":1},\"1\":{\"38\":1,\"40\":1,\"60\":1,\"115\":1}}],[\"labeling\",{\"1\":{\"37\":1}}],[\"labeled\",{\"1\":{\"37\":2}}],[\"label\",{\"0\":{\"113\":1,\"120\":1},\"1\":{\"33\":2,\"37\":4,\"41\":1,\"60\":7,\"70\":1,\"78\":1,\"115\":2,\"118\":1,\"120\":4,\"123\":1,\"124\":2,\"126\":1,\"131\":1,\"132\":1}}],[\"li​\",{\"1\":{\"80\":1}}],[\"linear\",{\"1\":{\"64\":1,\"82\":1,\"88\":1,\"102\":1}}],[\"liang\",{\"1\":{\"34\":1}}],[\"life\",{\"1\":{\"6\":2}}],[\"lebel\",{\"1\":{\"41\":1,\"123\":1}}],[\"level\",{\"0\":{\"18\":1},\"1\":{\"36\":3,\"61\":1}}],[\"length\",{\"1\":{\"15\":1,\"17\":2,\"23\":1}}],[\"learn\",{\"1\":{\"62\":1}}],[\"learner\",{\"1\":{\"9\":1,\"19\":1}}],[\"learning\",{\"0\":{\"64\":1,\"68\":1,\"76\":1,\"113\":1,\"124\":1},\"1\":{\"8\":2,\"28\":5,\"36\":1,\"37\":3,\"52\":5,\"57\":2,\"61\":2,\"64\":1,\"68\":1,\"78\":4,\"79\":3,\"82\":1,\"87\":1,\"89\":3,\"97\":1,\"110\":3,\"115\":3,\"116\":2,\"126\":1,\"132\":3},\"2\":{\"30\":1,\"91\":1,\"112\":1}}],[\"l\",{\"1\":{\"6\":1,\"8\":1,\"12\":1,\"13\":1,\"41\":1,\"97\":1,\"99\":1,\"100\":1,\"102\":2,\"104\":2}}],[\"local\",{\"1\":{\"36\":1,\"61\":1}}],[\"log\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"losses2\",{\"1\":{\"126\":1}}],[\"losses\",{\"1\":{\"126\":1}}],[\"loss\",{\"0\":{\"8\":1,\"123\":1},\"1\":{\"5\":1,\"8\":5,\"12\":4,\"41\":1,\"60\":3,\"63\":3,\"80\":1,\"96\":1,\"97\":1,\"99\":1,\"100\":3,\"102\":1,\"104\":1,\"108\":1,\"119\":1,\"123\":1,\"124\":4,\"125\":1}}],[\"long\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"17\":4,\"23\":2,\"25\":1}}],[\"h=f∘g\",{\"1\":{\"119\":1}}],[\"hackmd\",{\"1\":{\"132\":1}}],[\"hasselt\",{\"1\":{\"98\":1}}],[\"hado\",{\"1\":{\"98\":1}}],[\"hard\",{\"1\":{\"5\":1,\"10\":1,\"119\":2,\"123\":1}}],[\"hneat\",{\"1\":{\"88\":2}}],[\"hns=humanscore​−randomscore​agentscore​−randomscore​​\",{\"1\":{\"21\":1}}],[\"hns\",{\"1\":{\"21\":3}}],[\"history\",{\"1\":{\"84\":1}}],[\"hidden\",{\"1\":{\"81\":1}}],[\"high\",{\"0\":{\"18\":1},\"1\":{\"24\":2,\"51\":1,\"78\":1}}],[\"hf​\",{\"1\":{\"63\":1}}],[\"hϕ​\",{\"1\":{\"60\":1}}],[\"hsuan\",{\"1\":{\"36\":2,\"43\":1}}],[\"ht−1​\",{\"1\":{\"19\":1}}],[\"hyperparameters\",{\"1\":{\"50\":1}}],[\"hyperparameter\",{\"1\":{\"16\":1,\"21\":1,\"43\":1,\"63\":1,\"66\":1}}],[\"hoyer\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"horgan\",{\"1\":{\"19\":1}}],[\"horizon\",{\"1\":{\"14\":1}}],[\"hot\",{\"1\":{\"12\":1,\"60\":1}}],[\"hμ\",{\"1\":{\"12\":1}}],[\"h−1\",{\"1\":{\"8\":3}}],[\"h\",{\"1\":{\"8\":2,\"12\":4,\"41\":2,\"60\":1,\"63\":1,\"100\":2,\"119\":1,\"125\":2}}],[\"human\",{\"0\":{\"1\":1},\"1\":{\"21\":2,\"24\":2,\"27\":1,\"28\":1,\"88\":1,\"107\":2}}],[\"uda\",{\"0\":{\"40\":1,\"60\":1,\"64\":1},\"1\":{\"37\":4,\"40\":2,\"51\":1,\"56\":1,\"57\":4,\"58\":1,\"60\":1,\"61\":5,\"63\":1,\"66\":1,\"71\":1,\"72\":2,\"115\":1,\"116\":1,\"119\":1,\"124\":1,\"131\":2,\"132\":1}}],[\"unsuvervised\",{\"1\":{\"57\":1}}],[\"unsupervised\",{\"1\":{\"37\":1,\"58\":1,\"115\":1,\"116\":1,\"132\":2}}],[\"unity\",{\"1\":{\"47\":1}}],[\"university\",{\"1\":{\"32\":1,\"114\":1}}],[\"universal\",{\"1\":{\"7\":1}}],[\"unlabelled\",{\"1\":{\"40\":1}}],[\"unlabeled\",{\"1\":{\"37\":3,\"38\":1}}],[\"unlebelled\",{\"1\":{\"40\":1}}],[\"uncapped\",{\"1\":{\"25\":1}}],[\"undiscounted\",{\"1\":{\"21\":1}}],[\"uk​\",{\"1\":{\"16\":1}}],[\"uk​<ϵucb​​\",{\"1\":{\"16\":1}}],[\"uk​≥ϵucb​∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"ucb\",{\"0\":{\"14\":1,\"15\":1,\"16\":1},\"1\":{\"14\":5,\"15\":2,\"16\":1,\"28\":1}}],[\"uvfa\",{\"0\":{\"7\":1},\"1\":{\"5\":1,\"7\":1}}],[\"upernet\",{\"1\":{\"71\":1}}],[\"upper\",{\"0\":{\"14\":1}}],[\"up\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"us\",{\"0\":{\"0\":1}}],[\"模型採用\",{\"1\":{\"128\":1}}],[\"模型就越相信他是屬於\",{\"1\":{\"120\":1}}],[\"模型能夠得到的\",{\"1\":{\"87\":1}}],[\"模型很可能已經被\",{\"1\":{\"62\":1}}],[\"模型還是能夠順利學習\",{\"1\":{\"24\":1}}],[\"模型\",{\"1\":{\"3\":1,\"128\":1}}],[\"模型已經能夠在大多的\",{\"1\":{\"3\":1}}],[\"都有\",{\"1\":{\"118\":1}}],[\"都有正面的影響\",{\"1\":{\"107\":1}}],[\"都有相當大的改進\",{\"1\":{\"71\":1}}],[\"都需要\",{\"1\":{\"103\":1}}],[\"都對於\",{\"1\":{\"68\":1}}],[\"都可以有獲得提升\",{\"1\":{\"67\":1}}],[\"都可以使用\",{\"1\":{\"43\":1}}],[\"都被\",{\"1\":{\"63\":1}}],[\"都被其他\",{\"1\":{\"40\":1}}],[\"都具有\",{\"1\":{\"60\":1}}],[\"都不太好\",{\"1\":{\"49\":1}}],[\"都是平等對待所導致\",{\"1\":{\"129\":1}}],[\"都是平等的\",{\"1\":{\"123\":1}}],[\"都是會逐漸趨近於\",{\"1\":{\"108\":1}}],[\"都是要對\",{\"1\":{\"102\":1}}],[\"都是\",{\"1\":{\"48\":1,\"99\":1}}],[\"都是虛擬世界當中的影像\",{\"1\":{\"44\":1}}],[\"都是採用\",{\"1\":{\"13\":1,\"66\":1}}],[\"都會帶來\",{\"1\":{\"107\":1}}],[\"都會高過於原本的狀況\",{\"1\":{\"69\":1}}],[\"都會習慣使用\",{\"1\":{\"64\":1}}],[\"都會使用\",{\"1\":{\"63\":1}}],[\"都會對到\",{\"1\":{\"47\":1}}],[\"都會特別大\",{\"1\":{\"37\":1}}],[\"都會接收同樣的\",{\"1\":{\"12\":1}}],[\"都能夠透過\",{\"1\":{\"37\":1}}],[\"都能夠學習什麼時候該\",{\"1\":{\"13\":1}}],[\"都獲得比\",{\"1\":{\"27\":1}}],[\"都另外加上一個\",{\"1\":{\"21\":1}}],[\"都當成是工廠生產出來的機器人\",{\"1\":{\"13\":1}}],[\"都\",{\"1\":{\"3\":1}}],[\"也有介紹過同樣的\",{\"1\":{\"128\":1}}],[\"也有部分是源自於這樣的相似性帶來的好處\",{\"1\":{\"36\":1}}],[\"也額外定義\",{\"1\":{\"119\":1}}],[\"也是想要解決\",{\"1\":{\"115\":1}}],[\"也並不是每次加上\",{\"1\":{\"107\":1}}],[\"也可以拆成\",{\"1\":{\"119\":1}}],[\"也可以達到類似的效果\",{\"1\":{\"103\":1}}],[\"也可以是\",{\"1\":{\"60\":1}}],[\"也確實發現會平滑許多\",{\"1\":{\"87\":1}}],[\"也避免了上述提及的幾個問題\",{\"1\":{\"78\":1}}],[\"也變得能夠預測了\",{\"1\":{\"67\":1}}],[\"也會最大\",{\"1\":{\"69\":1}}],[\"也會期待其信心水平也要是高的\",{\"1\":{\"60\":1}}],[\"也會依據得到的\",{\"1\":{\"13\":1}}],[\"也跟最後評估的\",{\"1\":{\"50\":1}}],[\"也獲得不錯的成果\",{\"1\":{\"34\":1}}],[\"也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的\",{\"1\":{\"108\":1}}],[\"也說明了\",{\"1\":{\"25\":1}}],[\"也限制了數值範圍\",{\"1\":{\"21\":1}}],[\"也因為如此\",{\"1\":{\"13\":1}}],[\"也就可以更好地減輕\",{\"1\":{\"124\":1}}],[\"也就不需要再使用\",{\"1\":{\"104\":1,\"105\":1}}],[\"也就意味著需要\",{\"1\":{\"103\":1}}],[\"也就如下\",{\"1\":{\"100\":1}}],[\"也就形成\",{\"1\":{\"63\":1}}],[\"也就有更高的機會可以學更多次\",{\"1\":{\"62\":1}}],[\"也就是有些\",{\"1\":{\"124\":1}}],[\"也就是要讓底下的\",{\"1\":{\"80\":1}}],[\"也就是要找到\",{\"1\":{\"80\":1}}],[\"也就是在\",{\"1\":{\"80\":1}}],[\"也就是\",{\"1\":{\"17\":1,\"98\":1,\"100\":1,\"119\":1,\"123\":1}}],[\"也就是讓底下的期望值最大化\",{\"1\":{\"14\":1}}],[\"也就是說會在訓練的過程當中透過當下的預測給這些\",{\"1\":{\"115\":1}}],[\"也就是說最後的\",{\"1\":{\"108\":1}}],[\"也就是說對於一個參數\",{\"1\":{\"102\":1}}],[\"也就是說理想上每經過一輪更新\",{\"1\":{\"87\":1}}],[\"也就是說可以直接從\",{\"1\":{\"83\":1}}],[\"也就是說我們會期待產生出來的\",{\"1\":{\"60\":1}}],[\"也就是說我們對於\",{\"1\":{\"37\":1}}],[\"也就是說我現在面前有\",{\"1\":{\"14\":1}}],[\"也就是說能夠順利到達\",{\"1\":{\"22\":1}}],[\"也就是說\",{\"1\":{\"14\":1,\"60\":1,\"61\":1,\"62\":1,\"80\":1,\"108\":1}}],[\"也就是說這種做法的正確性是被確保的\",{\"1\":{\"12\":1}}],[\"也就是前面定義的\",{\"1\":{\"6\":1}}],[\"也就能夠得到\",{\"1\":{\"8\":1}}],[\"也提出了一個可以在所有\",{\"1\":{\"3\":1}}],[\"也許才有機會遇到\",{\"1\":{\"3\":1}}],[\"也需要嘗試越過那些障礙\",{\"1\":{\"3\":1}}],[\"的解決方案\",{\"1\":{\"129\":1}}],[\"的知識\",{\"1\":{\"125\":1}}],[\"的預測有改善\",{\"1\":{\"123\":1}}],[\"的預測結果要接近\",{\"1\":{\"41\":1}}],[\"的預測結果\",{\"1\":{\"36\":1}}],[\"的不平衡導致預測錯誤\",{\"1\":{\"123\":1}}],[\"的不同\",{\"1\":{\"36\":1}}],[\"的容忍程度\",{\"1\":{\"123\":1}}],[\"的所有\",{\"1\":{\"122\":1}}],[\"的中心點\",{\"1\":{\"121\":1}}],[\"的中心點就如同這裡的\",{\"1\":{\"120\":1}}],[\"的特徵中心點\",{\"1\":{\"121\":1}}],[\"的更新\",{\"1\":{\"120\":1}}],[\"的前提下\",{\"1\":{\"119\":1}}],[\"的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同\",{\"1\":{\"108\":1}}],[\"的數量\",{\"1\":{\"105\":2}}],[\"的形式\",{\"1\":{\"104\":2}}],[\"的計算本質上就是找中心點\",{\"1\":{\"122\":1}}],[\"的計算方式如下\",{\"1\":{\"122\":1}}],[\"的計算\",{\"1\":{\"102\":1}}],[\"的想法基本上是相同方向\",{\"1\":{\"102\":1}}],[\"的想法跟\",{\"1\":{\"102\":1}}],[\"的算式\",{\"1\":{\"100\":1}}],[\"的論文\",{\"1\":{\"100\":2}}],[\"的概念就如同火影忍者的影分身之術\",{\"1\":{\"100\":1}}],[\"的概念仍然是透過\",{\"1\":{\"99\":1}}],[\"的總和就能夠得到\",{\"1\":{\"99\":1}}],[\"的決定上採用了\",{\"1\":{\"97\":1,\"99\":1}}],[\"的限制\",{\"1\":{\"96\":1,\"99\":1}}],[\"的效果\",{\"1\":{\"96\":1}}],[\"的亂度越高越好\",{\"1\":{\"96\":1}}],[\"的時候都是透過增加\",{\"1\":{\"96\":1}}],[\"的各種\",{\"1\":{\"95\":1}}],[\"的位置以及類型\",{\"1\":{\"88\":1}}],[\"的平均去評估\",{\"1\":{\"87\":1}}],[\"的好壞就相對困難\",{\"1\":{\"87\":1}}],[\"的好壞\",{\"1\":{\"87\":1}}],[\"的目的是要讓整體的\",{\"1\":{\"80\":1}}],[\"的目標就是要讓整體的\",{\"1\":{\"80\":1}}],[\"的目標是在改採用更佳的\",{\"1\":{\"57\":1}}],[\"的目標是把兩個不同分佈的\",{\"1\":{\"33\":1}}],[\"的定義如下\",{\"1\":{\"80\":1,\"104\":1}}],[\"的改變而有巨大幅度的變化\",{\"1\":{\"78\":1,\"80\":1,\"84\":1}}],[\"的訓練資料具有高度相關性\",{\"1\":{\"78\":1,\"84\":1}}],[\"的訓練資料\",{\"1\":{\"78\":1}}],[\"的訓練上也是使用\",{\"1\":{\"60\":1}}],[\"的角度來看\",{\"1\":{\"78\":1}}],[\"的輸入去學習一直是一個很大的挑戰\",{\"1\":{\"78\":1}}],[\"的感官資料\",{\"1\":{\"78\":1}}],[\"的成功\",{\"1\":{\"88\":1}}],[\"的成功也放進\",{\"1\":{\"78\":1}}],[\"的成功帶進\",{\"1\":{\"72\":1}}],[\"的成本過高\",{\"1\":{\"57\":1}}],[\"的影響力\",{\"1\":{\"100\":1}}],[\"的影響\",{\"1\":{\"72\":1,\"124\":1}}],[\"的影響程度\",{\"1\":{\"6\":1,\"100\":1}}],[\"的表達能力可以更強\",{\"1\":{\"70\":1}}],[\"的表現都比起其他架構來得好許多\",{\"1\":{\"61\":1}}],[\"的圖片由於是透過車子上裝設攝影鏡頭去蒐集的\",{\"1\":{\"70\":1}}],[\"的變化就比較不與\",{\"1\":{\"69\":1}}],[\"的變化很大程度跟\",{\"1\":{\"69\":1}}],[\"的情況下\",{\"1\":{\"69\":1}}],[\"的情況下有很大的不同\",{\"1\":{\"62\":1}}],[\"的架構如同前面所述\",{\"1\":{\"66\":1}}],[\"的使用上如同過去我們看過的\",{\"1\":{\"66\":1}}],[\"的第\",{\"1\":{\"63\":1}}],[\"的實作比較簡單\",{\"1\":{\"60\":1}}],[\"的實驗\",{\"1\":{\"24\":1}}],[\"的產生方式可以是\",{\"1\":{\"60\":1}}],[\"的符號\",{\"1\":{\"60\":1}}],[\"的描述\",{\"1\":{\"60\":1}}],[\"的同時\",{\"1\":{\"57\":1}}],[\"的關聯性就能被連結起來\",{\"1\":{\"41\":1}}],[\"的核心做法是不單只是跟\",{\"1\":{\"41\":1}}],[\"的步驟\",{\"1\":{\"38\":1}}],[\"的一種\",{\"1\":{\"38\":1}}],[\"的技巧\",{\"1\":{\"38\":1,\"86\":1}}],[\"的例子\",{\"1\":{\"37\":1}}],[\"的認識嚴重缺乏\",{\"1\":{\"124\":1}}],[\"的認識\",{\"1\":{\"37\":1}}],[\"的資料拿進來使用\",{\"1\":{\"125\":1}}],[\"的資料分佈會隨著\",{\"1\":{\"78\":1,\"80\":1,\"84\":1}}],[\"的資料通常都會先\",{\"1\":{\"78\":1}}],[\"的資料\",{\"1\":{\"60\":2,\"126\":1}}],[\"的資料數量\",{\"1\":{\"60\":2}}],[\"的資料不存在任何\",{\"1\":{\"37\":1}}],[\"的資料上只有一些\",{\"1\":{\"37\":1}}],[\"的方式來處理\",{\"1\":{\"115\":1}}],[\"的方式\",{\"1\":{\"100\":1}}],[\"的方式是採用\",{\"1\":{\"8\":1}}],[\"的方法會使用\",{\"1\":{\"60\":1}}],[\"的方法是把套上\",{\"1\":{\"60\":1}}],[\"的方法上雖然任何\",{\"1\":{\"43\":1}}],[\"的方法\",{\"1\":{\"40\":1,\"61\":1,\"82\":1,\"94\":1,\"108\":1,\"131\":1}}],[\"的方法來降低這種問題\",{\"1\":{\"37\":1}}],[\"的方法解決了\",{\"1\":{\"37\":1}}],[\"的四個缺陷\",{\"1\":{\"25\":1}}],[\"的比較當中明顯看到在所有的成績都有所提升\",{\"1\":{\"25\":1}}],[\"的比例改變\",{\"1\":{\"13\":1}}],[\"的普遍性\",{\"1\":{\"25\":1}}],[\"的優劣\",{\"1\":{\"25\":1}}],[\"的重要性\",{\"1\":{\"22\":1}}],[\"的趨勢仍然是隨著\",{\"1\":{\"22\":1}}],[\"的狀況缺乏認知\",{\"1\":{\"34\":1}}],[\"的狀況下\",{\"1\":{\"22\":1}}],[\"的狀況\",{\"1\":{\"22\":2}}],[\"的做法就是照著\",{\"1\":{\"40\":1}}],[\"的做法之所以能夠成功\",{\"1\":{\"36\":1}}],[\"的做法\",{\"1\":{\"22\":1,\"103\":1}}],[\"的缺陷\",{\"1\":{\"22\":1}}],[\"的設計上也相當直覺\",{\"1\":{\"41\":1}}],[\"的設計是採用\",{\"1\":{\"17\":1}}],[\"的設定上對於目標被發現存在高估的問題\",{\"1\":{\"98\":1}}],[\"的設定上參考了許多過去的研究\",{\"1\":{\"43\":1}}],[\"的設定基本上跟\",{\"1\":{\"43\":1}}],[\"的設定下會大程度影響到最終\",{\"1\":{\"22\":1}}],[\"的設定取得的\",{\"1\":{\"22\":1}}],[\"的設定會透過\",{\"1\":{\"22\":1}}],[\"的設定詳閱論文的\",{\"1\":{\"21\":1}}],[\"的話會導致\",{\"1\":{\"82\":1}}],[\"的話\",{\"1\":{\"17\":1,\"78\":1}}],[\"的選用有關\",{\"1\":{\"69\":1}}],[\"的選項有更高機率被選擇到\",{\"1\":{\"14\":1}}],[\"的選擇相關\",{\"1\":{\"69\":1}}],[\"的選擇根據\",{\"1\":{\"19\":1}}],[\"的選擇應遠比\",{\"1\":{\"15\":1}}],[\"的選擇\",{\"1\":{\"10\":1}}],[\"的傾向\",{\"1\":{\"13\":1}}],[\"的存在\",{\"1\":{\"13\":1}}],[\"的機率\",{\"1\":{\"13\":1,\"69\":1,\"119\":1}}],[\"的版本是少了\",{\"1\":{\"47\":1}}],[\"的版本\",{\"1\":{\"12\":1,\"49\":1,\"80\":1}}],[\"的模型都跟\",{\"1\":{\"107\":1}}],[\"的模型都是採用如\",{\"1\":{\"57\":1}}],[\"的模型萃取出圖片的特徵\",{\"1\":{\"83\":1}}],[\"的模型得出來的結果\",{\"1\":{\"80\":1}}],[\"的模型\",{\"1\":{\"48\":1}}],[\"的模型對於\",{\"1\":{\"34\":1}}],[\"的模型雖然有許多\",{\"1\":{\"34\":1}}],[\"的模型會盡可能避開\",{\"1\":{\"22\":1}}],[\"的模型變成底下的樣子\",{\"1\":{\"12\":1}}],[\"的模型當成最後的結果\",{\"1\":{\"9\":1}}],[\"的參數更新\",{\"1\":{\"120\":1}}],[\"的參數加上\",{\"1\":{\"102\":1}}],[\"的參數\",{\"1\":{\"12\":2,\"100\":1}}],[\"的部分如下\",{\"1\":{\"130\":1}}],[\"的部分明顯可以看到最後的\",{\"1\":{\"129\":1}}],[\"的部分一如既往採用\",{\"1\":{\"128\":1}}],[\"的部分採用了\",{\"1\":{\"128\":1}}],[\"的部分在退步也是有幾項退步蠻多\",{\"1\":{\"107\":1}}],[\"的部分改用\",{\"1\":{\"104\":1,\"105\":1}}],[\"的部分作者另外的調整時選用的\",{\"1\":{\"66\":1}}],[\"的部分\",{\"1\":{\"61\":1}}],[\"的部分只能取得\",{\"1\":{\"61\":1}}],[\"的部分也可以發現到兩者的發展方向會稍有不同\",{\"1\":{\"22\":1}}],[\"的部分是分別給\",{\"1\":{\"12\":1}}],[\"的部分目的也是希望能夠促使\",{\"1\":{\"6\":1}}],[\"的作者認為是因為\",{\"1\":{\"10\":1}}],[\"的大小相差越來越懸殊\",{\"1\":{\"22\":1}}],[\"的大小下\",{\"1\":{\"22\":1}}],[\"的大小\",{\"1\":{\"10\":1,\"22\":1,\"86\":1,\"118\":1}}],[\"的問題似乎能夠得到改善\",{\"1\":{\"17\":1}}],[\"的問題\",{\"0\":{\"10\":1},\"1\":{\"10\":1,\"17\":1,\"57\":1,\"72\":1,\"115\":1}}],[\"的\",{\"1\":{\"8\":2,\"10\":1,\"12\":7,\"13\":2,\"16\":1,\"22\":2,\"24\":1,\"36\":2,\"41\":1,\"43\":1,\"46\":1,\"47\":1,\"49\":1,\"57\":1,\"61\":3,\"62\":1,\"63\":4,\"70\":1,\"78\":1,\"84\":2,\"94\":1,\"96\":1,\"97\":2,\"100\":1,\"102\":2,\"108\":2,\"115\":1,\"119\":1,\"121\":1,\"122\":2,\"125\":1,\"131\":1}}],[\"的分布往往分散\",{\"1\":{\"123\":1}}],[\"的分布上有做了一點調整\",{\"1\":{\"21\":1}}],[\"的分布也會變動\",{\"1\":{\"15\":1}}],[\"的分布會變動的話\",{\"1\":{\"15\":1}}],[\"的分布是固定的狀況下會使用\",{\"1\":{\"14\":1}}],[\"的分布\",{\"1\":{\"7\":1}}],[\"的研究\",{\"1\":{\"6\":1}}],[\"的範圍\",{\"1\":{\"6\":1}}],[\"的環境當中有更好的成效\",{\"1\":{\"5\":1}}],[\"的地方\",{\"1\":{\"3\":1}}],[\"的結果要接近\",{\"1\":{\"41\":1}}],[\"的結果就當作是他的\",{\"1\":{\"37\":1}}],[\"的結果\",{\"1\":{\"3\":1,\"22\":1}}],[\"是空的狀態\",{\"1\":{\"124\":1}}],[\"是由弱增強得到\",{\"1\":{\"124\":1}}],[\"是把一個可訓練參數拆成\",{\"1\":{\"108\":1}}],[\"是為了採用底下的特性方便後續\",{\"1\":{\"102\":1}}],[\"是使用\",{\"1\":{\"100\":1}}],[\"是被固定的參數\",{\"1\":{\"97\":1}}],[\"是上一個\",{\"1\":{\"97\":1}}],[\"是只有使用\",{\"1\":{\"48\":1}}],[\"是源自於即便\",{\"1\":{\"36\":1}}],[\"是每個\",{\"1\":{\"13\":1}}],[\"是在\",{\"1\":{\"12\":2,\"96\":1,\"104\":1,\"105\":1}}],[\"是一種\",{\"1\":{\"38\":1}}],[\"是一樣的\",{\"1\":{\"12\":1,\"50\":1}}],[\"是一個\",{\"1\":{\"16\":3,\"63\":1,\"119\":1}}],[\"是一個可以用來評估或是用在\",{\"1\":{\"8\":1}}],[\"是一個相當重要的\",{\"1\":{\"3\":1}}],[\"是不同的\",{\"1\":{\"6\":1}}],[\"是\",{\"1\":{\"6\":2,\"8\":1,\"43\":1,\"60\":1,\"61\":1,\"83\":1,\"105\":2,\"119\":1,\"121\":1}}],[\"是因為即便是在很多\",{\"1\":{\"3\":1}}],[\"正確\",{\"1\":{\"3\":1}}],[\"或是汽車比卡車更常見\",{\"1\":{\"37\":1}}],[\"或是\",{\"1\":{\"3\":1,\"14\":1,\"57\":2,\"64\":1,\"68\":1,\"82\":1,\"100\":1}}],[\"truncate\",{\"1\":{\"107\":1}}],[\"train\",{\"1\":{\"48\":2,\"63\":1,\"70\":1}}],[\"training\",{\"0\":{\"37\":1,\"55\":1,\"60\":1},\"1\":{\"38\":2,\"45\":1,\"46\":1,\"47\":1,\"52\":1,\"58\":1,\"60\":2,\"61\":2,\"73\":1,\"115\":2,\"116\":1,\"120\":2,\"129\":1}}],[\"transition\",{\"1\":{\"97\":1}}],[\"transformer\",{\"1\":{\"58\":1,\"61\":7,\"64\":1,\"70\":1,\"72\":1}}],[\"transformed\",{\"1\":{\"8\":2,\"12\":3}}],[\"tranheden\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"trajectory\",{\"1\":{\"19\":1}}],[\"trajectories\",{\"1\":{\"8\":1,\"12\":1,\"19\":1}}],[\"trace\",{\"1\":{\"17\":4,\"23\":4,\"25\":1}}],[\"table\",{\"1\":{\"97\":1}}],[\"target\",{\"0\":{\"113\":1,\"119\":1},\"1\":{\"8\":3,\"12\":1,\"33\":3,\"34\":1,\"36\":2,\"37\":7,\"40\":2,\"41\":4,\"60\":3,\"80\":1,\"84\":1,\"89\":1,\"97\":1,\"115\":3,\"118\":3,\"119\":1,\"121\":1,\"122\":1,\"124\":4,\"126\":1,\"132\":1}}],[\"t\",{\"1\":{\"80\":1,\"84\":1,\"100\":1,\"124\":8}}],[\"td\",{\"0\":{\"81\":1},\"1\":{\"79\":1,\"82\":1,\"84\":1}}],[\"twarm​\",{\"1\":{\"64\":1}}],[\"t​\",{\"1\":{\"62\":1}}],[\"to\",{\"0\":{\"40\":1,\"125\":1},\"1\":{\"40\":1,\"44\":1,\"50\":2,\"51\":2,\"52\":2,\"83\":1,\"110\":1}}],[\"tune\",{\"1\":{\"37\":1,\"126\":2}}],[\"temporal\",{\"1\":{\"110\":1}}],[\"temperature\",{\"1\":{\"62\":1,\"121\":1}}],[\"tensorflow\",{\"1\":{\"110\":1}}],[\"te\",{\"1\":{\"62\":1}}],[\"testing\",{\"1\":{\"61\":1}}],[\"testset\",{\"1\":{\"50\":1}}],[\"teacher\",{\"1\":{\"60\":2,\"125\":1,\"126\":2}}],[\"technology\",{\"1\":{\"32\":1,\"114\":1}}],[\"term\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"tsai\",{\"1\":{\"36\":2,\"43\":1}}],[\"ts\",{\"1\":{\"28\":1}}],[\"ting\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"ti\",{\"1\":{\"72\":1}}],[\"time\",{\"0\":{\"17\":1,\"23\":1}}],[\"tips\",{\"1\":{\"8\":1,\"12\":1,\"13\":1,\"16\":1,\"17\":1,\"22\":1,\"36\":1,\"40\":1,\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"102\":1}}],[\"thread\",{\"1\":{\"104\":1,\"105\":1}}],[\"through\",{\"0\":{\"17\":1,\"23\":1}}],[\"thing\",{\"0\":{\"63\":1,\"70\":1},\"1\":{\"61\":1,\"63\":2}}],[\"thq\",{\"1\":{\"8\":1}}],[\"thelimeydragon\",{\"1\":{\"3\":1}}],[\"the\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1}}],[\"t^q\",{\"1\":{\"8\":1}}],[\"t∈n​\",{\"1\":{\"8\":1}}],[\"找到寶藏可以加分\",{\"1\":{\"3\":1}}],[\"過於老舊的部分作者先透過一些實驗去尋找好的架構\",{\"1\":{\"61\":1}}],[\"過高\",{\"1\":{\"33\":1}}],[\"過去增加探索的方法大多都是在\",{\"1\":{\"96\":1}}],[\"過去會透過多次遊戲中\",{\"1\":{\"87\":1}}],[\"過去在\",{\"1\":{\"78\":1}}],[\"過去訓練\",{\"1\":{\"64\":1}}],[\"過去對於\",{\"1\":{\"14\":1}}],[\"過去的研究當中發現到如果是\",{\"1\":{\"82\":1}}],[\"過去的\",{\"1\":{\"3\":1}}],[\"過程中有許多陷阱\",{\"1\":{\"3\":1}}],[\"分開\",{\"1\":{\"61\":1}}],[\"分布相當不同時\",{\"1\":{\"10\":1}}],[\"分成了兩個部分\",{\"1\":{\"6\":1}}],[\"分鐘的時間探索\",{\"1\":{\"3\":1}}],[\"分別表示\",{\"1\":{\"100\":1,\"118\":3}}],[\"分別表示圖片的高寬\",{\"1\":{\"60\":1}}],[\"分別落在哪個\",{\"1\":{\"24\":1}}],[\"分別用\",{\"1\":{\"23\":1}}],[\"分別去針對\",{\"1\":{\"12\":1}}],[\"分別是\",{\"1\":{\"6\":1,\"66\":1,\"124\":1}}],[\"分別是這些遊戲\",{\"1\":{\"3\":1}}],[\"分別在\",{\"1\":{\"3\":1}}],[\"玩家要操作主角在\",{\"1\":{\"3\":1}}],[\"玩家要操作角色滑雪\",{\"1\":{\"3\":1}}],[\"number\",{\"0\":{\"103\":1},\"1\":{\"103\":2}}],[\"nfq\",{\"0\":{\"83\":1},\"1\":{\"79\":1,\"83\":2}}],[\"nt​\",{\"1\":{\"60\":1,\"118\":1}}],[\"ns​\",{\"1\":{\"60\":1,\"118\":1}}],[\"naive\",{\"0\":{\"40\":1},\"1\":{\"40\":2,\"60\":1}}],[\"nk​\",{\"1\":{\"14\":2,\"15\":2}}],[\"n−1\",{\"1\":{\"14\":1,\"16\":1}}],[\"n\",{\"1\":{\"14\":1,\"84\":1}}],[\"nn\",{\"1\":{\"8\":3,\"10\":1,\"12\":3,\"22\":1}}],[\"ngu+sep\",{\"1\":{\"24\":1}}],[\"ngu\",{\"0\":{\"9\":1,\"10\":1},\"1\":{\"5\":3,\"7\":2,\"8\":1,\"9\":2,\"10\":2,\"12\":2,\"13\":1,\"16\":1,\"17\":1,\"19\":1,\"22\":5,\"24\":1,\"25\":2}}],[\"net\",{\"1\":{\"95\":1,\"102\":1,\"104\":2}}],[\"networks\",{\"0\":{\"80\":1,\"92\":1},\"1\":{\"24\":1,\"25\":1,\"27\":1,\"28\":1,\"77\":1,\"79\":1}}],[\"network\",{\"0\":{\"55\":1,\"61\":1},\"1\":{\"8\":4,\"12\":3,\"22\":6,\"24\":2,\"36\":2,\"41\":1,\"43\":1,\"61\":2,\"63\":1,\"66\":1,\"73\":2,\"80\":4,\"82\":2,\"84\":4,\"87\":1,\"89\":1,\"94\":1,\"97\":4,\"99\":1,\"102\":2,\"104\":2,\"105\":2,\"115\":1,\"119\":1,\"120\":3}}],[\"next\",{\"1\":{\"84\":1}}],[\"neurips\",{\"1\":{\"77\":1},\"2\":{\"91\":1}}],[\"neural\",{\"1\":{\"28\":1,\"79\":1,\"80\":1,\"84\":1,\"94\":1,\"97\":3,\"99\":1}}],[\"needs\",{\"1\":{\"52\":1}}],[\"need\",{\"1\":{\"52\":1}}],[\"never\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"negative\",{\"1\":{\"3\":3,\"86\":1}}],[\"noise\",{\"0\":{\"96\":1},\"1\":{\"95\":1,\"96\":12,\"102\":8,\"103\":3,\"104\":1,\"105\":4,\"108\":2,\"110\":2,\"123\":1,\"124\":1}}],[\"noisynet\",{\"1\":{\"105\":1,\"107\":6,\"108\":1}}],[\"noisy\",{\"0\":{\"92\":1},\"1\":{\"95\":1,\"102\":1,\"104\":3,\"105\":1,\"116\":1}}],[\"non\",{\"1\":{\"82\":1}}],[\"notation\",{\"1\":{\"60\":1,\"118\":1}}],[\"notes\",{\"1\":{\"52\":1}}],[\"note\",{\"1\":{\"12\":1,\"100\":2,\"107\":1},\"2\":{\"29\":1,\"53\":1,\"74\":1,\"90\":1,\"111\":1,\"133\":1}}],[\"normalized\",{\"1\":{\"21\":2}}],[\"novelty\",{\"1\":{\"6\":1}}],[\"noveltyαt​\",{\"1\":{\"6\":1}}],[\"noveltyrtepisodic​\",{\"1\":{\"6\":1}}],[\"no\",{\"1\":{\"3\":1}}],[\"又有不少的\",{\"1\":{\"3\":1}}],[\"秒的\",{\"1\":{\"3\":1}}],[\"就能夠迫使模型對於這些略有不同的\",{\"1\":{\"124\":1}}],[\"就能夠比較好發揮作用\",{\"1\":{\"38\":1}}],[\"就需要看過整個\",{\"1\":{\"122\":1}}],[\"就被錯誤分類\",{\"1\":{\"115\":1}}],[\"就應該隨著訓練慢慢被忽視\",{\"1\":{\"108\":1}}],[\"就都是用這個\",{\"1\":{\"102\":1}}],[\"就只是這樣而已\",{\"1\":{\"102\":1}}],[\"就小到幾乎不存在了\",{\"1\":{\"98\":1}}],[\"就比較有系統性一些\",{\"1\":{\"96\":1}}],[\"就比較像是在亂試\",{\"1\":{\"96\":1}}],[\"就像是看到有殼的動物就當成是昆蟲\",{\"1\":{\"115\":1}}],[\"就像是\",{\"1\":{\"99\":1}}],[\"就像是可以換個角度去想其他人會怎麼做\",{\"1\":{\"96\":1}}],[\"就像是獵人裡面的凱特\",{\"1\":{\"96\":1}}],[\"就很不相同\",{\"1\":{\"87\":1}}],[\"就持續上一個做出的\",{\"1\":{\"86\":1}}],[\"就定義成\",{\"1\":{\"84\":1}}],[\"就基本上沒有\",{\"1\":{\"63\":1}}],[\"就不是單純的\",{\"1\":{\"60\":1}}],[\"就相當地雷同\",{\"1\":{\"36\":1}}],[\"就是上述的\",{\"1\":{\"120\":1}}],[\"就是在\",{\"1\":{\"96\":1}}],[\"就是用\",{\"1\":{\"80\":1}}],[\"就是用來描述一群資料他們的分布狀況\",{\"1\":{\"33\":1}}],[\"就是解決了使用更好的\",{\"1\":{\"61\":1}}],[\"就是希望\",{\"1\":{\"41\":1}}],[\"就是\",{\"1\":{\"36\":1,\"40\":1}}],[\"就會導致互相的不理解\",{\"1\":{\"34\":1}}],[\"就會導致單純在\",{\"1\":{\"33\":1}}],[\"就會因為\",{\"1\":{\"13\":1}}],[\"就會去環境當中互動\",{\"1\":{\"13\":1}}],[\"就會多\",{\"1\":{\"3\":1}}],[\"就有不同重要程度了\",{\"1\":{\"13\":1}}],[\"就做得頗差\",{\"1\":{\"10\":1}}],[\"就可以用\",{\"1\":{\"62\":1}}],[\"就可以結合起來形成新的\",{\"1\":{\"60\":1}}],[\"就可以再拿去\",{\"1\":{\"37\":1}}],[\"就可以得到相當好的影像分割結果\",{\"1\":{\"34\":1}}],[\"就可以得到單純\",{\"1\":{\"9\":1}}],[\"就可以透過\",{\"1\":{\"8\":1}}],[\"就跟\",{\"1\":{\"8\":1}}],[\"就通常完全沒辦法學習\",{\"1\":{\"3\":1}}],[\"途中要盡可能快速通過指定數量的\",{\"1\":{\"3\":1}}],[\"這三個\",{\"1\":{\"128\":1}}],[\"這跟前面提到只使用\",{\"1\":{\"40\":1}}],[\"這種包含了\",{\"1\":{\"61\":1}}],[\"這種相似的\",{\"1\":{\"40\":1}}],[\"這種\",{\"1\":{\"38\":2}}],[\"這種狀況下訓練模型就被稱為半監督式學習\",{\"1\":{\"37\":1}}],[\"這種差距被描述為\",{\"1\":{\"33\":1}}],[\"這類的\",{\"1\":{\"36\":1}}],[\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升\",{\"1\":{\"129\":1}}],[\"這樣的想法自然而然就出現了\",{\"1\":{\"78\":1}}],[\"這樣的結果有多少\",{\"1\":{\"60\":1}}],[\"這樣的問題只在\",{\"1\":{\"40\":1}}],[\"這樣的做法下每一個\",{\"1\":{\"103\":1}}],[\"這樣的做法有趣的是能夠將\",{\"1\":{\"38\":1}}],[\"這樣的做法之所以可行\",{\"1\":{\"36\":1}}],[\"這樣所需要的成本會過大\",{\"1\":{\"33\":1}}],[\"這就像是同理心\",{\"1\":{\"34\":1}}],[\"這一點尤其在\",{\"1\":{\"69\":1}}],[\"這一款遊戲\",{\"1\":{\"23\":1}}],[\"這一篇論文成功將\",{\"1\":{\"78\":1}}],[\"這一篇同樣也是先說明了\",{\"1\":{\"57\":1}}],[\"這一篇\",{\"1\":{\"3\":1,\"61\":1,\"115\":1}}],[\"這個參數會漸漸趨近於\",{\"1\":{\"108\":1}}],[\"這個論文提出的做法稱為\",{\"1\":{\"77\":1}}],[\"這個類別居然會隨著訓練時間預測結果越糟糕\",{\"1\":{\"70\":1}}],[\"這個\",{\"1\":{\"50\":1}}],[\"這個結果如果在取得\",{\"1\":{\"22\":1}}],[\"這個測量標準比較強調那些\",{\"1\":{\"21\":1}}],[\"這個問題\",{\"1\":{\"13\":1}}],[\"這裡使用的\",{\"1\":{\"124\":1}}],[\"這裡作者採用\",{\"1\":{\"123\":1}}],[\"這裡設為\",{\"1\":{\"121\":1}}],[\"這裡先定義一下接下來會用到的基本\",{\"1\":{\"118\":1}}],[\"這裡先簡單總結一下\",{\"1\":{\"67\":1}}],[\"這裡考慮有\",{\"1\":{\"80\":1}}],[\"這裡考慮到\",{\"1\":{\"60\":1}}],[\"這裡也加進來\",{\"1\":{\"64\":1}}],[\"這裡選用的\",{\"1\":{\"61\":1}}],[\"這裡會採用\",{\"1\":{\"60\":1}}],[\"這裡的距離是投射到高維空間之後\",{\"1\":{\"120\":1}}],[\"這裡的\",{\"1\":{\"60\":1,\"63\":1}}],[\"這裡的經驗指的是一個\",{\"1\":{\"15\":1}}],[\"這裡已經預設包含了\",{\"1\":{\"60\":1}}],[\"這裡要來實驗這一個做法實際上帶來多少影響\",{\"1\":{\"22\":1}}],[\"這裡就不贅述\",{\"1\":{\"21\":1}}],[\"這些普遍做得不錯的\",{\"1\":{\"48\":1}}],[\"這些\",{\"1\":{\"13\":1,\"63\":1}}],[\"這兩個\",{\"1\":{\"63\":1,\"69\":1}}],[\"這兩個問題\",{\"1\":{\"16\":1}}],[\"這兩者分別會讓\",{\"1\":{\"6\":1}}],[\"這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到\",{\"1\":{\"3\":1}}],[\"這款遊戲來說\",{\"1\":{\"3\":2}}],[\"以達成\",{\"1\":{\"96\":1}}],[\"以達到更好的訓練成效\",{\"1\":{\"12\":1}}],[\"以致於難以\",{\"1\":{\"62\":1}}],[\"以致於開始將研究的方向轉向如\",{\"1\":{\"57\":1}}],[\"以下就分別說明這三個部分的作法\",{\"1\":{\"61\":1}}],[\"以上\",{\"1\":{\"22\":1}}],[\"以\",{\"1\":{\"3\":2}}],[\"以及加上\",{\"1\":{\"107\":1}}],[\"以及畫面下方\",{\"1\":{\"70\":1}}],[\"以及訓練模型的\",{\"1\":{\"63\":1}}],[\"以及一般性都並不是很理想\",{\"1\":{\"60\":1}}],[\"以及一個對應的\",{\"1\":{\"120\":1}}],[\"以及一個\",{\"1\":{\"22\":1}}],[\"以及他對於\",{\"1\":{\"60\":1}}],[\"以及沒有的狀況\",{\"1\":{\"24\":1}}],[\"以及對應的\",{\"1\":{\"23\":1}}],[\"以及最傾向\",{\"1\":{\"22\":1}}],[\"以及\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"12\":2,\"22\":2,\"23\":2,\"24\":1,\"33\":1,\"36\":2,\"40\":1,\"41\":1,\"44\":1,\"61\":1,\"84\":2,\"86\":1,\"89\":2,\"100\":3,\"107\":2,\"109\":1,\"115\":1,\"119\":1,\"124\":1,\"126\":1}}],[\"v\",{\"1\":{\"99\":5}}],[\"volodymyr\",{\"1\":{\"77\":1,\"84\":1,\"87\":1,\"88\":1}}],[\"volvo\",{\"1\":{\"32\":1}}],[\"vgg\",{\"1\":{\"57\":1}}],[\"variational\",{\"1\":{\"110\":1}}],[\"varied\",{\"1\":{\"52\":1}}],[\"van\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1,\"98\":1}}],[\"validation\",{\"1\":{\"50\":4,\"87\":2}}],[\"value\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"19\":1,\"80\":1,\"81\":1,\"84\":1,\"97\":2,\"99\":1,\"100\":2,\"110\":1}}],[\"v2\",{\"1\":{\"43\":1,\"48\":1}}],[\"vime\",{\"1\":{\"110\":1}}],[\"visual\",{\"1\":{\"83\":1,\"132\":2}}],[\"vision\",{\"1\":{\"32\":1,\"52\":1,\"83\":1},\"2\":{\"54\":1,\"75\":1,\"134\":1}}],[\"virtual\",{\"1\":{\"47\":1}}],[\"viktor\",{\"1\":{\"38\":2}}],[\"via\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"52\":2,\"110\":1}}],[\"videos\",{\"1\":{\"96\":1}}],[\"video\",{\"1\":{\"3\":2}}],[\"veg\",{\"1\":{\"48\":1}}],[\"vector\",{\"1\":{\"12\":1}}],[\"venture\",{\"1\":{\"3\":1,\"24\":1}}],[\"2​\",{\"1\":{\"105\":1}}],[\"2∣st+i​\",{\"1\":{\"100\":1,\"105\":1}}],[\"25\",{\"1\":{\"52\":1}}],[\"255\",{\"1\":{\"3\":1}}],[\"24\",{\"1\":{\"52\":1}}],[\"24966\",{\"1\":{\"46\":1}}],[\"22480\",{\"1\":{\"24\":1}}],[\"21\",{\"1\":{\"24\":1}}],[\"26\",{\"1\":{\"24\":2}}],[\"2600\",{\"1\":{\"3\":2}}],[\"2975\",{\"1\":{\"45\":1}}],[\"29\",{\"1\":{\"24\":1}}],[\"2ϵ1+4ϵ\",{\"1\":{\"8\":1}}],[\"2\",{\"1\":{\"8\":1,\"12\":1,\"19\":1,\"67\":3,\"80\":1,\"97\":1,\"99\":1,\"100\":1,\"104\":4,\"132\":1}}],[\"2080\",{\"1\":{\"72\":1}}],[\"2017\",{\"1\":{\"96\":1}}],[\"2014\",{\"1\":{\"94\":1}}],[\"2013\",{\"1\":{\"77\":1}}],[\"2016\",{\"1\":{\"45\":1,\"47\":1}}],[\"2015\",{\"1\":{\"34\":1,\"84\":1,\"87\":1,\"88\":1,\"97\":1,\"98\":1,\"99\":1}}],[\"2018\",{\"1\":{\"19\":1,\"34\":1,\"36\":2,\"37\":1,\"43\":1,\"93\":1,\"107\":3,\"108\":1}}],[\"2022\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"2021\",{\"1\":{\"32\":1,\"37\":1,\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"2020\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"32\":1,\"38\":2}}],[\"200\",{\"1\":{\"22\":1}}],[\"20\",{\"1\":{\"3\":1,\"22\":1}}],[\"outlier\",{\"1\":{\"123\":1}}],[\"output\",{\"1\":{\"52\":1}}],[\"outperform\",{\"1\":{\"3\":2,\"71\":1,\"81\":1,\"88\":1}}],[\"outperforming\",{\"0\":{\"1\":1},\"1\":{\"28\":1}}],[\"olivier\",{\"1\":{\"94\":1}}],[\"olsson\",{\"1\":{\"38\":2}}],[\"object\",{\"1\":{\"88\":1}}],[\"observation\",{\"1\":{\"19\":1}}],[\"openai\",{\"1\":{\"96\":3,\"102\":1}}],[\"operation\",{\"1\":{\"8\":1}}],[\"operator\",{\"1\":{\"8\":1,\"12\":1}}],[\"optimal\",{\"1\":{\"80\":1,\"97\":1,\"99\":1}}],[\"oracle\",{\"1\":{\"61\":2,\"71\":1}}],[\"or\",{\"0\":{\"37\":1},\"1\":{\"47\":1}}],[\"offline\",{\"1\":{\"60\":1}}],[\"off\",{\"1\":{\"28\":1,\"82\":1,\"100\":2,\"109\":1}}],[\"of\",{\"0\":{\"13\":1},\"1\":{\"32\":2,\"114\":2}}],[\"overhead\",{\"1\":{\"104\":1,\"105\":1}}],[\"overfit\",{\"1\":{\"70\":1,\"120\":2}}],[\"overfitting\",{\"1\":{\"57\":2,\"61\":1,\"72\":1}}],[\"over\",{\"0\":{\"13\":1}}],[\"only\",{\"1\":{\"61\":1}}],[\"online\",{\"1\":{\"8\":1,\"12\":1,\"60\":2,\"100\":1,\"115\":1}}],[\"one\",{\"1\":{\"12\":1,\"60\":1}}],[\"on\",{\"1\":{\"3\":2,\"32\":1,\"43\":2,\"51\":1,\"52\":1,\"88\":1,\"100\":1,\"109\":1}}],[\"eπ\",{\"1\":{\"105\":1}}],[\"eπ​\",{\"1\":{\"14\":1}}],[\"e\",{\"1\":{\"104\":2}}],[\"e2​\",{\"1\":{\"84\":1}}],[\"encode\",{\"1\":{\"124\":1}}],[\"encoder\",{\"1\":{\"61\":6,\"66\":3,\"121\":2,\"124\":2}}],[\"enforcing\",{\"0\":{\"124\":1}}],[\"en​\",{\"1\":{\"84\":1}}],[\"enduro\",{\"1\":{\"86\":1}}],[\"end\",{\"1\":{\"83\":2}}],[\"entropy和kl\",{\"1\":{\"132\":1}}],[\"entropy\",{\"1\":{\"41\":1,\"60\":1,\"94\":1,\"95\":1,\"96\":1,\"100\":3,\"105\":1,\"119\":1,\"123\":2,\"132\":1}}],[\"ema\",{\"1\":{\"60\":1,\"122\":1}}],[\"evolution\",{\"1\":{\"110\":1}}],[\"everything\",{\"1\":{\"52\":1}}],[\"evaluation\",{\"0\":{\"50\":1}}],[\"evaluator\",{\"1\":{\"21\":1}}],[\"early\",{\"1\":{\"50\":1}}],[\"efficient\",{\"1\":{\"28\":1}}],[\"extractor\",{\"1\":{\"119\":1,\"124\":1}}],[\"extrinsic\",{\"1\":{\"6\":1,\"9\":1,\"12\":3,\"19\":1,\"22\":4}}],[\"extension\",{\"1\":{\"73\":1}}],[\"exp\",{\"1\":{\"121\":1,\"124\":2}}],[\"experiments\",{\"0\":{\"107\":1}}],[\"experience\",{\"1\":{\"9\":2,\"10\":1,\"13\":4,\"16\":1,\"19\":2,\"28\":2,\"84\":9,\"89\":1,\"97\":2,\"100\":1}}],[\"exponential\",{\"1\":{\"60\":1,\"122\":1}}],[\"explore\",{\"1\":{\"13\":1}}],[\"exploration\",{\"0\":{\"13\":1,\"24\":1,\"92\":1,\"96\":1},\"1\":{\"3\":1,\"5\":1,\"6\":2,\"7\":1,\"10\":2,\"13\":1,\"14\":1,\"22\":5,\"28\":1,\"94\":2,\"95\":1,\"96\":5,\"99\":1,\"100\":1,\"102\":1,\"108\":1,\"109\":1,\"110\":4}}],[\"exploit\",{\"1\":{\"13\":1}}],[\"exploitation\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"13\":1,\"14\":1,\"22\":5}}],[\"episode\",{\"1\":{\"6\":2,\"13\":1,\"19\":2,\"21\":1,\"22\":3,\"78\":1,\"102\":2,\"103\":1,\"107\":1}}],[\"eye\",{\"1\":{\"3\":2,\"24\":1}}],[\"et​=\",{\"1\":{\"84\":1}}],[\"eth\",{\"1\":{\"56\":1}}],[\"et\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"43\":1,\"45\":1,\"46\":1,\"47\":1,\"77\":1,\"84\":1,\"87\":1,\"88\":1,\"93\":1,\"97\":1,\"99\":1,\"107\":3,\"108\":1}}],[\"mdp\",{\"1\":{\"80\":1}}],[\"mnih\",{\"1\":{\"77\":1,\"84\":1,\"87\":1,\"88\":1}}],[\"mmsegmentation\",{\"1\":{\"66\":1}}],[\"mthings\",{\"1\":{\"63\":2}}],[\"mpi\",{\"1\":{\"56\":1}}],[\"mscoco\",{\"1\":{\"43\":1}}],[\"m\",{\"1\":{\"38\":3}}],[\"moco\",{\"1\":{\"132\":1}}],[\"momentum\",{\"1\":{\"121\":1,\"132\":2}}],[\"modified\",{\"1\":{\"115\":1}}],[\"models\",{\"1\":{\"63\":1,\"129\":1}}],[\"model\",{\"0\":{\"125\":1},\"1\":{\"37\":1,\"48\":1,\"60\":4,\"61\":1,\"80\":1,\"81\":1,\"82\":1,\"98\":1,\"125\":3,\"126\":6,\"131\":1}}],[\"monte\",{\"1\":{\"102\":1}}],[\"montezuma\",{\"1\":{\"3\":2,\"24\":1}}],[\"mohammad\",{\"1\":{\"93\":1,\"107\":3,\"108\":1}}],[\"moving\",{\"1\":{\"60\":1,\"122\":2}}],[\"microsoft\",{\"1\":{\"114\":1}}],[\"mit\",{\"1\":{\"61\":2,\"66\":1,\"70\":1}}],[\"miou\",{\"1\":{\"49\":1,\"57\":1,\"61\":1,\"67\":5,\"71\":1,\"129\":1,\"130\":1}}],[\"mix\",{\"1\":{\"41\":1}}],[\"mixup\",{\"1\":{\"38\":1}}],[\"mixing\",{\"0\":{\"38\":1,\"40\":1},\"1\":{\"38\":3,\"40\":2,\"41\":2,\"43\":2}}],[\"mixed\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"40\":1,\"52\":1}}],[\"mini\",{\"1\":{\"122\":2}}],[\"min\",{\"1\":{\"6\":1,\"15\":1,\"21\":1}}],[\"median\",{\"1\":{\"107\":1}}],[\"medium\",{\"1\":{\"33\":2,\"100\":1}}],[\"meire\",{\"1\":{\"93\":1,\"107\":3,\"108\":1}}],[\"memory\",{\"1\":{\"84\":2,\"100\":1}}],[\"mean\",{\"1\":{\"25\":3,\"102\":2,\"107\":1}}],[\"methods\",{\"1\":{\"110\":1}}],[\"method\",{\"1\":{\"51\":1}}],[\"methodology\",{\"0\":{\"11\":1,\"39\":1,\"59\":1,\"84\":1,\"101\":1,\"117\":1}}],[\"meta\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":8,\"25\":2,\"27\":1}}],[\"matthieu\",{\"1\":{\"94\":1}}],[\"marius\",{\"1\":{\"45\":1}}],[\"mask\",{\"1\":{\"38\":3,\"43\":1,\"63\":1}}],[\"majchrowska\",{\"1\":{\"37\":1}}],[\"map做出預測\",{\"1\":{\"61\":1}}],[\"map\",{\"1\":{\"36\":1,\"38\":3,\"61\":1}}],[\"maps\",{\"1\":{\"36\":2,\"61\":1}}],[\"mab\",{\"1\":{\"14\":2,\"28\":1}}],[\"maximizing\",{\"1\":{\"110\":1}}],[\"maxb∈a​q\",{\"1\":{\"98\":1}}],[\"max​q\",{\"1\":{\"80\":1}}],[\"maxc\",{\"1\":{\"60\":1}}],[\"max\",{\"1\":{\"6\":2}}],[\"multi\",{\"1\":{\"14\":1,\"81\":1,\"105\":1}}],[\"muzero\",{\"1\":{\"3\":3,\"25\":3}}],[\"不知道\",{\"1\":{\"119\":1}}],[\"不要想太多\",{\"1\":{\"102\":1}}],[\"不難發現到確實都存在高估的狀況\",{\"1\":{\"98\":1}}],[\"不需要事先經過其他的分解\",{\"1\":{\"89\":1}}],[\"不需要看太遠\",{\"1\":{\"7\":1}}],[\"不是\",{\"1\":{\"83\":1}}],[\"不是那麼地\",{\"1\":{\"10\":1,\"16\":1}}],[\"不採用\",{\"1\":{\"61\":1}}],[\"不確定性低\",{\"1\":{\"14\":1}}],[\"不確定性高\",{\"1\":{\"14\":1}}],[\"不會被固定下來\",{\"1\":{\"13\":1}}],[\"不穩定\",{\"1\":{\"12\":1}}],[\"不過進步主要在\",{\"1\":{\"107\":1}}],[\"不過並沒有保證收斂\",{\"1\":{\"94\":1}}],[\"不過作者發現在他們的模型得出來的結果往往會是很不穩定的\",{\"1\":{\"87\":1}}],[\"不過\",{\"1\":{\"83\":1}}],[\"不過從\",{\"1\":{\"78\":1}}],[\"不過也考慮到\",{\"1\":{\"63\":1}}],[\"不過過去使用\",{\"1\":{\"61\":1}}],[\"不過直覺上\",{\"1\":{\"57\":1}}],[\"不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索\",{\"1\":{\"94\":1}}],[\"不過這裡的成功只停止在雙陸棋上\",{\"1\":{\"81\":1}}],[\"不過這裡最主要都是使用\",{\"1\":{\"43\":1}}],[\"不過這種情況下一個直覺的問題是\",{\"1\":{\"33\":1}}],[\"不過像是馬路\",{\"1\":{\"36\":1}}],[\"不過如果遇到新的\",{\"1\":{\"34\":1}}],[\"不過實際上訓練時因為拆開來訓練\",{\"1\":{\"12\":1}}],[\"不過在計算\",{\"1\":{\"12\":1}}],[\"不過可惜的是\",{\"1\":{\"3\":1}}],[\"不同的地方在於他並不是直接去學習\",{\"1\":{\"99\":1}}],[\"不同的地方在於\",{\"1\":{\"83\":1}}],[\"不同的環境下需要的\",{\"1\":{\"6\":1}}],[\"不同\",{\"1\":{\"10\":1,\"13\":2,\"36\":1,\"61\":1,\"123\":1}}],[\"不太好\",{\"1\":{\"3\":1,\"34\":1}}],[\"sce\",{\"1\":{\"123\":1}}],[\"science\",{\"1\":{\"114\":1}}],[\"scalable\",{\"1\":{\"110\":1}}],[\"scale\",{\"1\":{\"61\":1}}],[\"scorebaseline​\",{\"1\":{\"107\":1}}],[\"scorehuman​\",{\"1\":{\"107\":1}}],[\"scores\",{\"1\":{\"21\":2}}],[\"src\",{\"1\":{\"61\":1}}],[\"sw\",{\"1\":{\"48\":1,\"49\":1}}],[\"swear\",{\"1\":{\"3\":1}}],[\"skipping\",{\"1\":{\"86\":1}}],[\"skiiing\",{\"1\":{\"10\":1}}],[\"skiing\",{\"1\":{\"3\":5,\"24\":1}}],[\"sky\",{\"1\":{\"48\":1,\"63\":1}}],[\"sb​\",{\"1\":{\"38\":2}}],[\"symmetric\",{\"1\":{\"123\":1,\"132\":1}}],[\"synthia\",{\"0\":{\"47\":1,\"49\":1,\"130\":1},\"1\":{\"44\":2,\"49\":1,\"50\":1,\"66\":1,\"128\":1,\"130\":1}}],[\"synthetic\",{\"1\":{\"44\":1,\"46\":1,\"47\":1}}],[\"synethic\",{\"1\":{\"33\":1,\"34\":1,\"36\":1}}],[\"sylwia\",{\"1\":{\"37\":1}}],[\"ssl\",{\"1\":{\"37\":1,\"51\":1}}],[\"spoiler\",{\"1\":{\"61\":1}}],[\"sparse\",{\"1\":{\"78\":1}}],[\"space\",{\"0\":{\"96\":1},\"1\":{\"52\":1,\"86\":2,\"95\":1,\"96\":8,\"102\":2,\"110\":1}}],[\"spatial\",{\"1\":{\"36\":2}}],[\"sprechmann\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"shift\",{\"1\":{\"33\":3,\"37\":2,\"41\":1}}],[\"sarsa\",{\"1\":{\"88\":2}}],[\"sa​\",{\"1\":{\"38\":3}}],[\"sampling\",{\"0\":{\"31\":1,\"41\":1,\"62\":1,\"69\":1},\"1\":{\"52\":1,\"61\":1}}],[\"sample\",{\"1\":{\"10\":1,\"12\":2,\"13\":1,\"16\":1,\"19\":1,\"62\":2,\"69\":2,\"102\":1}}],[\"sampled\",{\"1\":{\"8\":1}}],[\"safe\",{\"1\":{\"28\":1}}],[\"success\",{\"1\":{\"120\":1,\"123\":1,\"124\":1}}],[\"supervise\",{\"1\":{\"37\":2}}],[\"supervised\",{\"0\":{\"125\":1},\"1\":{\"37\":1,\"52\":5,\"57\":2,\"61\":1,\"115\":2}}],[\"summary\",{\"0\":{\"25\":1,\"67\":1},\"1\":{\"70\":1}}],[\"surround\",{\"1\":{\"24\":1}}],[\"small\",{\"1\":{\"23\":1,\"63\":3}}],[\"seaquest\",{\"1\":{\"86\":1}}],[\"separable\",{\"1\":{\"71\":1}}],[\"separation\",{\"1\":{\"61\":1}}],[\"separate\",{\"1\":{\"22\":6,\"24\":1,\"25\":1,\"27\":1}}],[\"seed\",{\"1\":{\"62\":1,\"69\":3}}],[\"segformer\",{\"1\":{\"61\":3,\"68\":1}}],[\"segmentation\",{\"0\":{\"55\":1,\"113\":1},\"1\":{\"34\":1,\"36\":2,\"37\":1,\"38\":1,\"41\":1,\"43\":1,\"52\":5,\"56\":1,\"57\":3,\"58\":1,\"61\":2,\"63\":1,\"73\":2,\"115\":1,\"118\":1,\"119\":2,\"123\":1,\"128\":2,\"131\":1,\"132\":1}}],[\"set\",{\"1\":{\"50\":5,\"87\":2}}],[\"settings\",{\"0\":{\"21\":1}}],[\"sematic\",{\"1\":{\"58\":1}}],[\"semantic\",{\"0\":{\"55\":1,\"113\":1},\"1\":{\"34\":1,\"36\":3,\"37\":1,\"38\":4,\"52\":4,\"56\":1,\"57\":3,\"61\":2,\"63\":1,\"73\":2,\"115\":1,\"119\":2,\"123\":1,\"131\":1,\"132\":1}}],[\"semi\",{\"1\":{\"37\":2,\"52\":5,\"57\":1,\"115\":1}}],[\"self\",{\"0\":{\"37\":1,\"60\":1,\"125\":1},\"1\":{\"52\":1,\"58\":1,\"60\":2,\"61\":1,\"115\":1,\"116\":1,\"129\":1}}],[\"sequence\",{\"1\":{\"12\":1}}],[\"sequences\",{\"1\":{\"8\":1}}],[\"simclrv2\",{\"1\":{\"125\":1,\"128\":1}}],[\"simple\",{\"1\":{\"51\":1,\"110\":1}}],[\"simplified\",{\"0\":{\"16\":1}}],[\"single\",{\"1\":{\"104\":1}}],[\"si​\",{\"1\":{\"100\":2}}],[\"silver\",{\"1\":{\"77\":1,\"84\":1,\"87\":1,\"88\":1,\"98\":1}}],[\"sidewalk\",{\"1\":{\"40\":1}}],[\"size\",{\"0\":{\"17\":1,\"23\":1},\"1\":{\"63\":1}}],[\"sliding\",{\"0\":{\"15\":1,\"16\":1},\"1\":{\"15\":1,\"16\":1}}],[\"s\",{\"1\":{\"12\":2,\"80\":4,\"84\":1,\"97\":4,\"99\":16,\"104\":8}}],[\"s=1∏t​cs​\",{\"1\":{\"8\":1}}],[\"stage\",{\"1\":{\"120\":2}}],[\"state\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"6\":1,\"12\":4,\"19\":1,\"78\":1,\"80\":1,\"84\":3,\"87\":1,\"97\":1,\"99\":1,\"100\":1}}],[\"st+i​\",{\"1\":{\"100\":3,\"105\":4}}],[\"st+1​\",{\"1\":{\"84\":1}}],[\"stuff\",{\"1\":{\"63\":1}}],[\"student\",{\"1\":{\"60\":2,\"125\":2,\"126\":4}}],[\"structure\",{\"0\":{\"113\":1,\"124\":1},\"1\":{\"115\":1,\"126\":1,\"132\":1}}],[\"structured\",{\"1\":{\"52\":1}}],[\"strong\",{\"1\":{\"52\":1}}],[\"strategies\",{\"0\":{\"55\":1},\"1\":{\"28\":1,\"61\":2,\"73\":1,\"110\":1}}],[\"stop\",{\"1\":{\"50\":1}}],[\"stephan\",{\"1\":{\"46\":1}}],[\"steps\",{\"1\":{\"22\":1}}],[\"steven\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"st​\",{\"1\":{\"7\":1,\"84\":1}}],[\"soft\",{\"1\":{\"119\":1,\"120\":2,\"124\":1}}],[\"softmax\",{\"1\":{\"62\":1,\"119\":1,\"121\":2}}],[\"solution\",{\"1\":{\"108\":2}}],[\"solaris\",{\"1\":{\"3\":3,\"10\":1,\"23\":1,\"24\":1}}],[\"some\",{\"0\":{\"50\":1}}],[\"source\",{\"1\":{\"33\":3,\"34\":1,\"36\":2,\"37\":2,\"41\":3,\"48\":3,\"49\":1,\"57\":1,\"60\":3,\"61\":2,\"62\":1,\"70\":1,\"115\":1,\"118\":3,\"120\":2,\"125\":2,\"126\":3}}],[\"sota\",{\"1\":{\"3\":1,\"51\":1,\"71\":3,\"129\":1,\"130\":1,\"131\":1}}],[\"和一層\",{\"1\":{\"81\":1}}],[\"和\",{\"1\":{\"3\":2,\"6\":1,\"10\":1,\"12\":3,\"15\":1,\"36\":1,\"37\":1,\"63\":1,\"69\":1,\"84\":1,\"94\":1,\"99\":3,\"100\":1,\"103\":1,\"104\":2,\"118\":4,\"124\":1}}],[\"5​\",{\"1\":{\"105\":1}}],[\"55\",{\"1\":{\"50\":1}}],[\"5595\",{\"1\":{\"24\":1}}],[\"53\",{\"1\":{\"50\":1}}],[\"50\",{\"1\":{\"24\":3}}],[\"5\",{\"1\":{\"3\":1,\"67\":2}}],[\"52\",{\"1\":{\"3\":1}}],[\"51\",{\"1\":{\"3\":1}}],[\"57\",{\"1\":{\"3\":2,\"107\":1}}],[\"r−v\",{\"1\":{\"100\":3}}],[\"r=r\",{\"1\":{\"97\":1}}],[\"r+γq\",{\"1\":{\"98\":1,\"99\":1,\"104\":2}}],[\"r+γmaxb∈a​q\",{\"1\":{\"98\":1}}],[\"r+γb∈amax​q\",{\"1\":{\"97\":1,\"104\":2}}],[\"r+γa\",{\"1\":{\"80\":1}}],[\"rgb\",{\"1\":{\"89\":1}}],[\"row\",{\"1\":{\"67\":4}}],[\"road\",{\"1\":{\"40\":1,\"48\":1,\"49\":1,\"63\":1}}],[\"rcs\",{\"0\":{\"62\":1,\"69\":1},\"1\":{\"61\":1,\"67\":2,\"69\":3}}],[\"raw\",{\"1\":{\"89\":1}}],[\"rate\",{\"0\":{\"64\":1,\"68\":1},\"1\":{\"61\":1,\"64\":1,\"66\":1,\"68\":1}}],[\"rates\",{\"1\":{\"61\":1}}],[\"rare\",{\"0\":{\"62\":1,\"69\":1},\"1\":{\"61\":1,\"62\":3,\"67\":1,\"69\":1}}],[\"randomized\",{\"1\":{\"110\":1}}],[\"randomness\",{\"1\":{\"94\":1}}],[\"random\",{\"0\":{\"103\":1},\"1\":{\"22\":1,\"62\":1,\"69\":3,\"103\":4,\"104\":1,\"105\":1,\"107\":1}}],[\"r\",{\"1\":{\"12\":2,\"46\":1,\"97\":1,\"99\":1,\"104\":4}}],[\"rightarrow\",{\"0\":{\"129\":1,\"130\":1}}],[\"richter\",{\"1\":{\"46\":1}}],[\"rider\",{\"1\":{\"24\":1,\"40\":1,\"69\":1,\"86\":1}}],[\"ri\",{\"1\":{\"12\":3}}],[\"rtx\",{\"1\":{\"72\":1}}],[\"rt−1i​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt−1e​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt​=t\",{\"1\":{\"80\":1}}],[\"rt​∣st​=s\",{\"1\":{\"80\":1}}],[\"rt​\",{\"1\":{\"8\":1,\"80\":1,\"84\":1}}],[\"rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+\",{\"1\":{\"7\":1}}],[\"rti​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rti​=rtepisodic​⋅min\",{\"1\":{\"6\":1}}],[\"rte​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rtβi​​=rte​+βi​rti​\",{\"1\":{\"6\":1}}],[\"review\",{\"1\":{\"96\":1}}],[\"revenge\",{\"1\":{\"3\":2,\"24\":1}}],[\"regularization\",{\"1\":{\"94\":1,\"95\":1,\"96\":1,\"100\":1}}],[\"regularize\",{\"1\":{\"63\":1}}],[\"reply\",{\"1\":{\"100\":1}}],[\"replay\",{\"1\":{\"9\":1,\"17\":3,\"19\":3,\"23\":1,\"28\":2,\"84\":6,\"89\":1,\"97\":3,\"100\":1}}],[\"representation\",{\"1\":{\"88\":1,\"116\":1,\"132\":2}}],[\"research\",{\"1\":{\"114\":1}}],[\"researchgate\",{\"1\":{\"70\":1,\"71\":1}}],[\"residual\",{\"1\":{\"79\":1}}],[\"resnet\",{\"1\":{\"48\":1,\"57\":1,\"128\":2}}],[\"resnet101\",{\"1\":{\"43\":2}}],[\"results\",{\"0\":{\"20\":1,\"42\":1,\"65\":1,\"85\":1,\"106\":1,\"127\":1}}],[\"real\",{\"1\":{\"34\":1,\"36\":1,\"44\":1}}],[\"read\",{\"2\":{\"30\":1,\"54\":1,\"75\":1,\"91\":1,\"112\":1,\"134\":1}}],[\"rel=oracleuda​\",{\"1\":{\"61\":1}}],[\"rel\",{\"1\":{\"61\":1}}],[\"release\",{\"1\":{\"32\":1}}],[\"related\",{\"0\":{\"4\":1,\"35\":1,\"58\":1,\"79\":1,\"95\":1,\"116\":1}}],[\"reinforcement\",{\"0\":{\"76\":1},\"1\":{\"28\":3,\"79\":1,\"89\":1,\"110\":3},\"2\":{\"30\":1,\"91\":1,\"112\":1}}],[\"recurrent\",{\"1\":{\"28\":2}}],[\"returns\",{\"1\":{\"21\":1}}],[\"return\",{\"1\":{\"14\":8,\"15\":1,\"22\":2,\"24\":1,\"80\":3,\"99\":1,\"100\":1}}],[\"retrace\",{\"1\":{\"8\":6,\"12\":2,\"24\":1}}],[\"re\",{\"1\":{\"12\":3,\"62\":1}}],[\"reward\",{\"0\":{\"6\":1},\"1\":{\"3\":7,\"5\":1,\"6\":10,\"8\":1,\"9\":2,\"12\":7,\"13\":2,\"14\":1,\"15\":2,\"19\":2,\"22\":6,\"78\":2,\"80\":2,\"84\":1,\"86\":4,\"87\":2,\"97\":1}}],[\"r2d2+sep\",{\"1\":{\"24\":1}}],[\"r2d2\",{\"1\":{\"3\":3,\"9\":1,\"17\":1,\"23\":1,\"24\":2,\"25\":3}}],[\"rl\",{\"0\":{\"8\":1},\"1\":{\"3\":4,\"5\":2,\"6\":1,\"8\":1,\"78\":8,\"80\":4,\"81\":1,\"83\":1,\"84\":2,\"87\":1,\"89\":1,\"94\":1,\"95\":1,\"100\":1,\"109\":1}}],[\"例如下圖\",{\"1\":{\"115\":1}}],[\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的\",{\"1\":{\"37\":1}}],[\"例如在\",{\"1\":{\"10\":1,\"94\":1}}],[\"例如\",{\"1\":{\"3\":1,\"94\":1}}],[\"guez\",{\"1\":{\"98\":1}}],[\"go\",{\"1\":{\"94\":1}}],[\"gool\",{\"1\":{\"56\":2,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":2,\"71\":1}}],[\"google\",{\"1\":{\"2\":1,\"93\":1}}],[\"gheshlaghi\",{\"1\":{\"93\":1,\"107\":3,\"108\":1}}],[\"github\",{\"1\":{\"73\":1}}],[\"give\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"gpu\",{\"1\":{\"72\":1}}],[\"gta\",{\"1\":{\"61\":2}}],[\"gta5\",{\"0\":{\"46\":1,\"48\":1,\"129\":1},\"1\":{\"44\":2,\"46\":1,\"50\":1,\"51\":1,\"66\":1,\"128\":1,\"129\":1}}],[\"gθ​\",{\"1\":{\"60\":1}}],[\"geist\",{\"1\":{\"94\":1}}],[\"germanros\",{\"1\":{\"47\":1}}],[\"generator\",{\"1\":{\"36\":1}}],[\"general\",{\"1\":{\"10\":1,\"16\":1,\"21\":1,\"25\":1}}],[\"gradient\",{\"1\":{\"82\":1}}],[\"gridworld\",{\"1\":{\"22\":1}}],[\"greedy\",{\"1\":{\"13\":1,\"16\":1,\"19\":1,\"84\":1,\"94\":1,\"95\":1,\"96\":2,\"97\":1,\"99\":1,\"104\":1}}],[\"g\",{\"1\":{\"21\":1,\"119\":1}}],[\"gap\",{\"1\":{\"115\":2}}],[\"gaussian\",{\"1\":{\"60\":1,\"96\":1,\"102\":1,\"103\":2,\"104\":1,\"105\":3}}],[\"gan\",{\"1\":{\"36\":1}}],[\"gammon\",{\"0\":{\"81\":1},\"1\":{\"79\":1,\"84\":1}}],[\"gamma\",{\"1\":{\"24\":2}}],[\"gamer\",{\"1\":{\"3\":1}}],[\"game\",{\"1\":{\"3\":2}}],[\"games\",{\"1\":{\"3\":3,\"24\":1,\"25\":3,\"27\":1,\"86\":1,\"107\":1}}],[\"gate\",{\"1\":{\"3\":1}}],[\"gates\",{\"1\":{\"3\":1}}],[\"在絕大多數的類別當中也是比起過去的做法還要強\",{\"1\":{\"129\":1}}],[\"在絕大多數並非是最佳的結果上都不會離最佳太遠\",{\"1\":{\"48\":1,\"49\":1}}],[\"在兩個\",{\"1\":{\"128\":1}}],[\"在那些充滿噪點的\",{\"1\":{\"120\":1}}],[\"在一個\",{\"1\":{\"120\":1}}],[\"在實務上為了避免像是\",{\"1\":{\"99\":1}}],[\"在過往的研究可以發現到說往往我們在設計讓\",{\"1\":{\"96\":1}}],[\"在過去的\",{\"1\":{\"94\":1}}],[\"在幾乎所有的遊戲當中都\",{\"1\":{\"88\":1}}],[\"在經過\",{\"1\":{\"84\":1}}],[\"在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示\",{\"1\":{\"70\":1}}],[\"在上圖的橘色線是原本的模型隨著訓練後對於不同\",{\"1\":{\"70\":1}}],[\"在沒有使用\",{\"1\":{\"69\":1}}],[\"在加上不同的調整後得出的結果\",{\"1\":{\"67\":1}}],[\"在訓練後期才出現\",{\"1\":{\"62\":1}}],[\"在這裡採用\",{\"1\":{\"104\":1,\"105\":1}}],[\"在這裡能夠提供更好的幫助\",{\"1\":{\"61\":1}}],[\"在這一篇論文當中主要探討的是過去\",{\"1\":{\"57\":1}}],[\"在這邊我們在意的是評估的部分\",{\"1\":{\"8\":1}}],[\"在邊界上往往會出現誤差的問題解決\",{\"1\":{\"38\":1}}],[\"在限定幾款遊戲有特別出色的成效\",{\"1\":{\"25\":1}}],[\"在所有\",{\"1\":{\"25\":1}}],[\"在搭配了\",{\"1\":{\"24\":1}}],[\"在下表當中可以看到\",{\"1\":{\"24\":1}}],[\"在最傾向\",{\"1\":{\"22\":1}}],[\"在不同\",{\"1\":{\"22\":1,\"62\":1}}],[\"在每個\",{\"1\":{\"22\":1,\"24\":1}}],[\"在每一個\",{\"1\":{\"13\":1,\"102\":1}}],[\"在時間\",{\"1\":{\"14\":1,\"84\":1,\"100\":1}}],[\"在計算上分別都只會拿自己的\",{\"1\":{\"12\":1}}],[\"在目標的\",{\"1\":{\"8\":1}}],[\"在整個訓練過程當中沒有踏足過的狀態\",{\"1\":{\"6\":1}}],[\"在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響\",{\"1\":{\"3\":1}}],[\"在剩下的遊戲當中這些\",{\"1\":{\"3\":1}}],[\"在\",{\"1\":{\"3\":1,\"6\":2,\"14\":1,\"17\":1,\"21\":1,\"22\":1,\"23\":2,\"24\":2,\"33\":1,\"36\":1,\"37\":1,\"38\":2,\"40\":1,\"41\":1,\"43\":2,\"44\":1,\"56\":1,\"57\":1,\"60\":1,\"61\":1,\"62\":1,\"66\":2,\"71\":1,\"78\":1,\"80\":1,\"84\":1,\"87\":1,\"98\":1,\"100\":1,\"103\":1,\"120\":1,\"123\":1,\"129\":1,\"130\":1,\"131\":1}}],[\"iclr\",{\"1\":{\"93\":1,\"96\":1},\"2\":{\"112\":1}}],[\"icml\",{\"1\":{\"2\":1},\"2\":{\"30\":1}}],[\"iou\",{\"1\":{\"69\":4,\"70\":1}}],[\"iverson\",{\"1\":{\"60\":1}}],[\"i=0∑k​eπ\",{\"1\":{\"105\":1}}],[\"i=0∑k​∇ζπ​​log\",{\"1\":{\"105\":1}}],[\"i=0∑k​∇θπ​​log\",{\"1\":{\"100\":1,\"105\":1}}],[\"i=1nt​​\",{\"1\":{\"60\":2}}],[\"i=1ns​​\",{\"1\":{\"60\":2}}],[\"i=t+1∏s​ci​\",{\"1\":{\"8\":1}}],[\"i\",{\"1\":{\"60\":17,\"62\":1,\"63\":14,\"100\":1,\"119\":7,\"120\":7,\"121\":6,\"122\":3,\"123\":3,\"124\":7}}],[\"issue\",{\"1\":{\"124\":1}}],[\"issues\",{\"0\":{\"50\":1}}],[\"is\",{\"0\":{\"33\":1}}],[\"improving\",{\"0\":{\"55\":1},\"1\":{\"61\":1,\"73\":2}}],[\"improvement\",{\"1\":{\"24\":1,\"107\":2}}],[\"images\",{\"1\":{\"45\":1,\"46\":1,\"47\":1}}],[\"imagenet\",{\"0\":{\"63\":1,\"70\":1},\"1\":{\"43\":1,\"61\":1,\"63\":4,\"66\":1}}],[\"image\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":2,\"22\":3,\"23\":2,\"24\":2,\"25\":2,\"33\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":5,\"40\":2,\"41\":2,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"56\":1,\"58\":1,\"61\":5,\"67\":2,\"68\":1,\"69\":1,\"70\":3,\"71\":2,\"84\":2,\"87\":2,\"88\":2,\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"107\":3,\"108\":1,\"115\":1,\"120\":1,\"129\":2,\"130\":1}}],[\"initialize\",{\"1\":{\"105\":1}}],[\"independent\",{\"1\":{\"103\":1,\"105\":2}}],[\"invaders\",{\"1\":{\"86\":2}}],[\"input\",{\"1\":{\"83\":1,\"105\":2}}],[\"introduce\",{\"1\":{\"51\":1}}],[\"intrinsic\",{\"0\":{\"6\":1},\"1\":{\"5\":1,\"6\":4,\"9\":1,\"12\":3,\"19\":1,\"22\":3}}],[\"in\",{\"1\":{\"28\":2,\"51\":1,\"52\":1}}],[\"informatics\",{\"1\":{\"56\":1}}],[\"information\",{\"0\":{\"2\":1,\"32\":1,\"56\":1,\"77\":1,\"93\":1,\"114\":1},\"1\":{\"61\":1,\"110\":1}}],[\"info\",{\"1\":{\"3\":1,\"34\":1,\"36\":1,\"37\":4,\"40\":1,\"48\":1,\"49\":1,\"60\":1,\"84\":1,\"94\":1,\"96\":1,\"99\":1,\"100\":1}}],[\"py​\",{\"1\":{\"125\":1,\"126\":1}}],[\"psuedo\",{\"1\":{\"131\":1}}],[\"ps​\",{\"1\":{\"125\":1,\"126\":1}}],[\"pseudo\",{\"0\":{\"37\":1,\"113\":1,\"120\":1},\"1\":{\"37\":3,\"38\":1,\"40\":1,\"41\":1,\"60\":5,\"70\":1,\"115\":3,\"118\":1,\"119\":1,\"120\":5,\"123\":3,\"124\":2,\"126\":1,\"132\":1}}],[\"p+q\",{\"1\":{\"103\":1}}],[\"pq+q\",{\"1\":{\"103\":1}}],[\"playing\",{\"0\":{\"76\":1}}],[\"p\",{\"1\":{\"60\":1,\"62\":2,\"105\":2}}],[\"pt​∥pt\",{\"1\":{\"125\":1}}],[\"pt​\",{\"1\":{\"119\":1,\"123\":2,\"125\":1}}],[\"pt\",{\"1\":{\"60\":2,\"119\":2,\"120\":2,\"125\":1}}],[\"ptifall\",{\"1\":{\"3\":1}}],[\"prototypes\",{\"1\":{\"131\":1}}],[\"prototype\",{\"0\":{\"122\":1},\"1\":{\"120\":4,\"122\":3,\"123\":1,\"124\":1}}],[\"prototypical\",{\"0\":{\"113\":1,\"120\":1},\"1\":{\"124\":1,\"126\":1,\"132\":1}}],[\"probability\",{\"1\":{\"97\":1,\"119\":1}}],[\"proda\",{\"1\":{\"60\":1,\"66\":1,\"124\":1,\"126\":2,\"128\":1,\"131\":1,\"132\":1}}],[\"progress\",{\"1\":{\"28\":1}}],[\"preliminary\",{\"0\":{\"118\":1}}],[\"preprocess\",{\"1\":{\"84\":1}}],[\"pretrain\",{\"1\":{\"66\":1,\"125\":1}}],[\"pretrained\",{\"1\":{\"43\":1,\"63\":1,\"128\":1}}],[\"prediction\",{\"1\":{\"37\":2}}],[\"prioritized\",{\"1\":{\"28\":1}}],[\"private\",{\"1\":{\"3\":2,\"24\":1}}],[\"posts\",{\"0\":{\"135\":1}}],[\"positive\",{\"1\":{\"3\":3,\"86\":1}}],[\"pooling\",{\"1\":{\"63\":1}}],[\"pole\",{\"1\":{\"47\":1}}],[\"policies\",{\"0\":{\"13\":1}}],[\"policy有什么区别\",{\"1\":{\"110\":1}}],[\"policy\",{\"1\":{\"8\":3,\"10\":2,\"12\":4,\"13\":5,\"16\":1,\"24\":1,\"27\":1,\"28\":1,\"78\":1,\"80\":2,\"82\":2,\"84\":1,\"87\":1,\"88\":1,\"94\":1,\"96\":1,\"100\":6,\"109\":2,\"110\":1}}],[\"pong\",{\"1\":{\"24\":1,\"86\":1}}],[\"pan\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"part\",{\"1\":{\"110\":1}}],[\"parameterization\",{\"0\":{\"12\":1,\"22\":1}}],[\"parameter\",{\"0\":{\"96\":1},\"1\":{\"8\":1,\"95\":1,\"96\":7,\"102\":2,\"110\":2}}],[\"pablo\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"paper\",{\"1\":{\"3\":2,\"37\":1,\"49\":1,\"50\":1,\"57\":1,\"61\":1,\"66\":1,\"73\":1,\"83\":1,\"96\":1,\"115\":1},\"2\":{\"30\":1,\"54\":1,\"75\":1,\"91\":1,\"112\":1,\"134\":1}}],[\"perceptron\",{\"1\":{\"81\":1}}],[\"perturbations\",{\"1\":{\"52\":1}}],[\"person\",{\"1\":{\"40\":1,\"48\":1}}],[\"period\",{\"1\":{\"17\":2,\"23\":1}}],[\"per\",{\"1\":{\"6\":1}}],[\"performance\",{\"1\":{\"3\":2,\"21\":1,\"22\":1,\"23\":1,\"48\":2,\"49\":1,\"51\":1,\"60\":1,\"61\":1,\"62\":1,\"67\":4,\"68\":1,\"71\":1,\"72\":1,\"87\":1,\"115\":1}}],[\"penalty\",{\"1\":{\"3\":1}}],[\"pietquin\",{\"1\":{\"94\":1}}],[\"pixels\",{\"1\":{\"70\":2}}],[\"pixel\",{\"1\":{\"36\":1,\"63\":1,\"88\":1,\"119\":1}}],[\"pitfall\",{\"1\":{\"3\":3,\"24\":1}}],[\"piot\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"93\":1,\"107\":3,\"108\":1}}],[\"puigdomènech\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"by\",{\"0\":{\"124\":1}}],[\"b∈rq\",{\"1\":{\"103\":1}}],[\"breakout\",{\"1\":{\"86\":1}}],[\"bracket\",{\"1\":{\"60\":1}}],[\"bo\",{\"1\":{\"114\":1,\"115\":1,\"120\":1,\"129\":1}}],[\"bottleneck\",{\"1\":{\"63\":1}}],[\"bound\",{\"0\":{\"14\":1}}],[\"b5\",{\"1\":{\"61\":2,\"66\":1,\"70\":1}}],[\"blur\",{\"1\":{\"60\":1}}],[\"bicycle\",{\"1\":{\"69\":2}}],[\"bias\",{\"1\":{\"62\":1,\"103\":1}}],[\"binary\",{\"1\":{\"38\":1,\"43\":1}}],[\"bilal\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"93\":1,\"107\":3,\"108\":1}}],[\"better\",{\"1\":{\"96\":2,\"110\":1}}],[\"bert\",{\"1\":{\"86\":1}}],[\"beat\",{\"1\":{\"51\":1}}],[\"beam\",{\"1\":{\"24\":1,\"86\":1}}],[\"best\",{\"1\":{\"50\":3,\"88\":1}}],[\"behaviour\",{\"1\":{\"28\":1}}],[\"benifit\",{\"1\":{\"24\":1}}],[\"benchmarks\",{\"1\":{\"44\":1,\"128\":1}}],[\"benchmark\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1}}],[\"bus\",{\"1\":{\"63\":1}}],[\"build\",{\"1\":{\"48\":1}}],[\"budden\",{\"1\":{\"19\":1}}],[\"buffer\",{\"1\":{\"9\":1,\"17\":1,\"19\":3,\"97\":2,\"100\":1}}],[\"b\",{\"1\":{\"12\":3,\"38\":2,\"97\":1,\"98\":2,\"99\":1,\"104\":4}}],[\"balanced\",{\"1\":{\"52\":1}}],[\"baseline\",{\"1\":{\"50\":1,\"88\":1,\"99\":1,\"107\":3}}],[\"based\",{\"1\":{\"43\":1,\"52\":1,\"61\":3,\"110\":1}}],[\"basic\",{\"0\":{\"2\":1,\"32\":1,\"56\":1,\"77\":1,\"93\":1,\"114\":1}}],[\"backbone\",{\"1\":{\"43\":1,\"48\":1,\"57\":3,\"61\":5,\"63\":1,\"72\":1,\"128\":2}}],[\"backprop\",{\"0\":{\"17\":1,\"23\":1}}],[\"bandit\",{\"0\":{\"13\":1},\"1\":{\"14\":1,\"16\":1,\"24\":3,\"25\":1}}],[\"batch\",{\"1\":{\"12\":4,\"122\":2}}],[\"badia\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"analysis\",{\"0\":{\"108\":1}}],[\"and\",{\"0\":{\"55\":1,\"113\":1},\"1\":{\"28\":1,\"61\":1,\"73\":1,\"114\":1,\"132\":1}}],[\"ai​∣si​\",{\"1\":{\"100\":1}}],[\"a3c\",{\"0\":{\"100\":1,\"105\":1},\"1\":{\"95\":1,\"96\":1,\"100\":7,\"105\":3,\"107\":2,\"110\":1}}],[\"azar\",{\"1\":{\"93\":1,\"107\":3,\"108\":1}}],[\"a∼ρ\",{\"1\":{\"80\":1}}],[\"avgpool\",{\"1\":{\"63\":1}}],[\"average\",{\"1\":{\"24\":2,\"27\":1,\"60\":1,\"63\":1,\"122\":2}}],[\"augumented\",{\"1\":{\"60\":1}}],[\"augumentation\",{\"1\":{\"38\":2,\"124\":2}}],[\"augmentation\",{\"1\":{\"52\":1}}],[\"approximation\",{\"1\":{\"79\":1,\"102\":1}}],[\"approximators\",{\"1\":{\"82\":1}}],[\"approximator\",{\"1\":{\"7\":1,\"82\":2,\"88\":1}}],[\"apply\",{\"1\":{\"33\":1,\"38\":2,\"51\":1}}],[\"applications\",{\"1\":{\"32\":1}}],[\"appendix\",{\"1\":{\"21\":1,\"126\":1}}],[\"argb∈amax​q\",{\"1\":{\"99\":1,\"104\":2}}],[\"arthur\",{\"1\":{\"98\":1,\"100\":1}}],[\"architectures\",{\"0\":{\"55\":1},\"1\":{\"61\":1,\"73\":2}}],[\"architecture\",{\"0\":{\"18\":1,\"61\":1}}],[\"arm\",{\"1\":{\"14\":1}}],[\"am​=a\",{\"1\":{\"14\":2,\"15\":2}}],[\"ak​=⎩⎨⎧​kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"16\":1}}],[\"ak​=\",{\"1\":{\"14\":1,\"15\":1}}],[\"ak​\",{\"1\":{\"14\":2}}],[\"advantage\",{\"1\":{\"99\":2,\"100\":2}}],[\"adversarial\",{\"1\":{\"36\":1,\"37\":1,\"52\":1}}],[\"adaptsegnet\",{\"1\":{\"128\":1}}],[\"adapt\",{\"1\":{\"52\":1,\"61\":1}}],[\"adaptation\",{\"0\":{\"31\":1},\"1\":{\"52\":4,\"58\":1}}],[\"adaption\",{\"0\":{\"33\":1,\"41\":1},\"1\":{\"33\":2,\"37\":1,\"115\":1,\"124\":1},\"2\":{\"54\":1,\"75\":1,\"134\":1}}],[\"adapting\",{\"1\":{\"28\":1}}],[\"adaptive\",{\"0\":{\"13\":1,\"24\":1,\"55\":1,\"113\":1},\"1\":{\"61\":1,\"73\":2,\"132\":1}}],[\"adrià\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"actors\",{\"0\":{\"19\":1},\"1\":{\"19\":1}}],[\"actor\",{\"1\":{\"9\":1,\"13\":13,\"16\":1,\"24\":1,\"27\":1,\"100\":1,\"110\":1}}],[\"action\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"3\":1,\"7\":1,\"8\":1,\"9\":1,\"12\":4,\"13\":1,\"14\":3,\"15\":1,\"16\":1,\"19\":2,\"78\":1,\"80\":2,\"84\":2,\"86\":2,\"96\":5,\"97\":4,\"99\":3}}],[\"asia\",{\"1\":{\"114\":1}}],[\"as\",{\"1\":{\"110\":1}}],[\"asynchronous\",{\"1\":{\"110\":2}}],[\"asb​\",{\"1\":{\"12\":3}}],[\"as​∣xs​\",{\"1\":{\"8\":2}}],[\"assignment\",{\"1\":{\"3\":1,\"10\":1,\"17\":2,\"124\":2}}],[\"a\",{\"0\":{\"13\":1},\"1\":{\"8\":4,\"12\":5,\"14\":10,\"15\":8,\"16\":2,\"38\":2,\"51\":1,\"80\":4,\"97\":5,\"99\":18,\"100\":3,\"104\":8,\"105\":2,\"110\":1,\"115\":1,\"120\":2}}],[\"a∣xt+1​\",{\"1\":{\"8\":2}}],[\"at+i​\",{\"1\":{\"100\":1,\"105\":2}}],[\"at+i​∣st+i​\",{\"1\":{\"100\":1,\"105\":2}}],[\"attention\",{\"1\":{\"61\":1}}],[\"at\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"at−1​\",{\"1\":{\"9\":1,\"19\":1}}],[\"at​=a\",{\"1\":{\"80\":1}}],[\"at​\",{\"1\":{\"7\":2,\"8\":8,\"84\":1}}],[\"atari\",{\"0\":{\"1\":1,\"76\":1},\"1\":{\"3\":5,\"25\":3,\"27\":1,\"28\":1,\"86\":1,\"107\":1}}],[\"agents\",{\"1\":{\"110\":1}}],[\"agent\",{\"0\":{\"9\":1},\"1\":{\"3\":1,\"5\":2,\"6\":3,\"15\":1,\"21\":1,\"22\":4,\"78\":1,\"84\":2,\"86\":1,\"87\":2,\"94\":1,\"96\":1,\"100\":1}}],[\"agent57\",{\"0\":{\"1\":1},\"1\":{\"10\":1,\"12\":2,\"13\":3,\"14\":1,\"16\":1,\"21\":1,\"22\":1,\"23\":1,\"25\":5,\"28\":2}}],[\"alternative\",{\"1\":{\"110\":1}}],[\"alogorithm\",{\"1\":{\"100\":1}}],[\"alpha\",{\"1\":{\"94\":1}}],[\"alignment\",{\"0\":{\"36\":1},\"1\":{\"36\":1,\"129\":1}}],[\"algorithms\",{\"1\":{\"79\":1}}],[\"algorithm\",{\"0\":{\"14\":1},\"1\":{\"126\":1}}],[\"al\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"43\":1,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"77\":1,\"84\":1,\"87\":1,\"88\":1,\"93\":1,\"97\":1,\"99\":1,\"107\":3,\"108\":1}}],[\"about\",{\"0\":{\"0\":1,\"50\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
