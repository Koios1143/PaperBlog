export const data = JSON.parse("{\"key\":\"v-5b18c8c4\",\"path\":\"/posts/Noisy%20Networks%20for%20Exploration.html\",\"title\":\"Noisy Networks for Exploration\",\"lang\":\"en-US\",\"frontmatter\":{\"date\":\"2024-02-03T00:00:00.000Z\",\"category\":[\"Note\"],\"tag\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"],\"description\":\"Noisy Networks for Exploration Basic Information 2018 ICLR Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind 問題描述 在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"head\":[[\"meta\",{\"property\":\"og:url\",\"content\":\"https://mister-hope.github.io/posts/Noisy%20Networks%20for%20Exploration.html\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"Paper Blog\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"Noisy Networks for Exploration\"}],[\"meta\",{\"property\":\"og:description\",\"content\":\"Noisy Networks for Exploration Basic Information 2018 ICLR Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind 問題描述 在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Paper Read\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Reinforcement Learning\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"ICLR\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2024-02-03T00:00:00.000Z\"}],[\"script\",{\"type\":\"application/ld+json\"},\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Article\\\",\\\"headline\\\":\\\"Noisy Networks for Exploration\\\",\\\"image\\\":[\\\"\\\"],\\\"datePublished\\\":\\\"2024-02-03T00:00:00.000Z\\\",\\\"dateModified\\\":null,\\\"author\\\":[]}\"]]},\"headers\":[{\"level\":2,\"title\":\"Basic Information\",\"slug\":\"basic-information\",\"link\":\"#basic-information\",\"children\":[]},{\"level\":2,\"title\":\"問題描述\",\"slug\":\"問題描述\",\"link\":\"#問題描述\",\"children\":[]},{\"level\":2,\"title\":\"Related Works\",\"slug\":\"related-works\",\"link\":\"#related-works\",\"children\":[{\"level\":3,\"title\":\"Parameter Space Noise for Exploration\",\"slug\":\"parameter-space-noise-for-exploration\",\"link\":\"#parameter-space-noise-for-exploration\",\"children\":[]},{\"level\":3,\"title\":\"DQN\",\"slug\":\"dqn\",\"link\":\"#dqn\",\"children\":[]},{\"level\":3,\"title\":\"Double-DQN\",\"slug\":\"double-dqn\",\"link\":\"#double-dqn\",\"children\":[]},{\"level\":3,\"title\":\"Dueling DQN\",\"slug\":\"dueling-dqn\",\"link\":\"#dueling-dqn\",\"children\":[]},{\"level\":3,\"title\":\"A3C\",\"slug\":\"a3c\",\"link\":\"#a3c\",\"children\":[]}]},{\"level\":2,\"title\":\"Methodology\",\"slug\":\"methodology\",\"link\":\"#methodology\",\"children\":[{\"level\":3,\"title\":\"基本想法\",\"slug\":\"基本想法\",\"link\":\"#基本想法\",\"children\":[]},{\"level\":3,\"title\":\"減少產 random number 時間\",\"slug\":\"減少產-random-number-時間\",\"link\":\"#減少產-random-number-時間\",\"children\":[]},{\"level\":3,\"title\":\"DQN & Dueling DQN\",\"slug\":\"dqn-dueling-dqn\",\"link\":\"#dqn-dueling-dqn\",\"children\":[]},{\"level\":3,\"title\":\"Distributed A3C\",\"slug\":\"distributed-a3c\",\"link\":\"#distributed-a3c\",\"children\":[]}]},{\"level\":2,\"title\":\"Results\",\"slug\":\"results\",\"link\":\"#results\",\"children\":[{\"level\":3,\"title\":\"Experiments\",\"slug\":\"experiments\",\"link\":\"#experiments\",\"children\":[]},{\"level\":3,\"title\":\"Analysis\",\"slug\":\"analysis\",\"link\":\"#analysis\",\"children\":[]}]},{\"level\":2,\"title\":\"Contribution\",\"slug\":\"contribution\",\"link\":\"#contribution\",\"children\":[]},{\"level\":2,\"title\":\"值得一看的文章們\",\"slug\":\"值得一看的文章們\",\"link\":\"#值得一看的文章們\",\"children\":[]}],\"readingTime\":{\"minutes\":13.37,\"words\":4010},\"filePathRelative\":\"posts/Noisy Networks for Exploration.md\",\"localizedDate\":\"February 3, 2024\",\"excerpt\":\"<h1> Noisy Networks for Exploration</h1>\\n<h2> Basic Information</h2>\\n<ul>\\n<li>2018 ICLR</li>\\n<li>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind</li>\\n</ul>\\n<h2> 問題描述</h2>\\n<p>在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 <code>ϵ-greedy</code> 和 <code>entropy regularization</code> 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。</p>\",\"autoDesc\":true}")

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
