export default "{\"documentCount\":93,\"nextId\":93,\"documentIds\":{\"0\":\"v-184f4da6\",\"1\":\"v-3caeec67\",\"2\":\"v-3caeec67#basic-information\",\"3\":\"v-3caeec67#問題描述\",\"4\":\"v-3caeec67#related-works\",\"5\":\"v-3caeec67#never-give-up\",\"6\":\"v-3caeec67#intrinsic-reward\",\"7\":\"v-3caeec67#uvfa\",\"8\":\"v-3caeec67#rl-loss\",\"9\":\"v-3caeec67#ngu-agent\",\"10\":\"v-3caeec67#ngu-的問題\",\"11\":\"v-3caeec67#methodology\",\"12\":\"v-3caeec67#state-action-value-function-parameterization\",\"13\":\"v-3caeec67#adaptive-exploration-over-a-family-of-policies-bandit\",\"14\":\"v-3caeec67#upper-confidence-bound-algorithm-ucb\",\"15\":\"v-3caeec67#sliding-window-ucb\",\"16\":\"v-3caeec67#simplified-sliding-window-ucb\",\"17\":\"v-3caeec67#backprop-through-time-window-size\",\"18\":\"v-3caeec67#high-level-architecture\",\"19\":\"v-3caeec67#actors\",\"20\":\"v-3caeec67#results\",\"21\":\"v-3caeec67#settings\",\"22\":\"v-3caeec67#state-action-value-function-parameterization-1\",\"23\":\"v-3caeec67#backprop-through-time-window-size-1\",\"24\":\"v-3caeec67#adaptive-exploration\",\"25\":\"v-3caeec67#summary\",\"26\":\"v-3caeec67#discussion\",\"27\":\"v-3caeec67#contribution\",\"28\":\"v-3caeec67#值得一看的文章們\",\"29\":\"v-3caeec67@0\",\"30\":\"v-3caeec67@1\",\"31\":\"v-c0336012\",\"32\":\"v-c0336012#basic-information\",\"33\":\"v-c0336012#what-is-domain-adaption\",\"34\":\"v-c0336012#問題描述\",\"35\":\"v-c0336012#related-works\",\"36\":\"v-c0336012#domain-alignment\",\"37\":\"v-c0336012#pseudo-labelling-or-self-training\",\"38\":\"v-c0336012#mixing\",\"39\":\"v-c0336012#methodology\",\"40\":\"v-c0336012#naive-mixing-to-uda\",\"41\":\"v-c0336012#domain-adaption-via-corss-domain-mixed-sampling-dacs\",\"42\":\"v-c0336012#results\",\"43\":\"v-c0336012#實驗設定\",\"44\":\"v-c0336012#dataset\",\"45\":\"v-c0336012#cityscapes\",\"46\":\"v-c0336012#gta5\",\"47\":\"v-c0336012#synthia\",\"48\":\"v-c0336012#gta5-cityscapes\",\"49\":\"v-c0336012#synthia-cityscapes\",\"50\":\"v-c0336012#some-issues-about-evaluation\",\"51\":\"v-c0336012#contribution\",\"52\":\"v-c0336012#值得一看的文章們\",\"53\":\"v-c0336012@0\",\"54\":\"v-c0336012@1\",\"55\":\"v-32d63a0d\",\"56\":\"v-32d63a0d#basic-information\",\"57\":\"v-32d63a0d#問題描述\",\"58\":\"v-32d63a0d#related-works\",\"59\":\"v-32d63a0d#q-networks\",\"60\":\"v-32d63a0d#td-gammon\",\"61\":\"v-32d63a0d#收斂性相關研究\",\"62\":\"v-32d63a0d#nfq\",\"63\":\"v-32d63a0d#methodology\",\"64\":\"v-32d63a0d#results\",\"65\":\"v-32d63a0d#實驗設定\",\"66\":\"v-32d63a0d#評估方式\",\"67\":\"v-32d63a0d#比較基準\",\"68\":\"v-32d63a0d#contribution\",\"69\":\"v-32d63a0d@0\",\"70\":\"v-32d63a0d@1\",\"71\":\"v-5b18c8c4\",\"72\":\"v-5b18c8c4#basic-information\",\"73\":\"v-5b18c8c4#問題描述\",\"74\":\"v-5b18c8c4#related-works\",\"75\":\"v-5b18c8c4#parameter-space-noise-for-exploration\",\"76\":\"v-5b18c8c4#dqn\",\"77\":\"v-5b18c8c4#double-dqn\",\"78\":\"v-5b18c8c4#dueling-dqn\",\"79\":\"v-5b18c8c4#a3c\",\"80\":\"v-5b18c8c4#methodology\",\"81\":\"v-5b18c8c4#基本想法\",\"82\":\"v-5b18c8c4#減少產-random-number-時間\",\"83\":\"v-5b18c8c4#dqn-dueling-dqn\",\"84\":\"v-5b18c8c4#distributed-a3c\",\"85\":\"v-5b18c8c4#results\",\"86\":\"v-5b18c8c4#experiments\",\"87\":\"v-5b18c8c4#analysis\",\"88\":\"v-5b18c8c4#contribution\",\"89\":\"v-5b18c8c4#值得一看的文章們\",\"90\":\"v-5b18c8c4@0\",\"91\":\"v-5b18c8c4@1\",\"92\":\"v-e1e3da16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2],\"1\":[6],\"2\":[2,13],\"3\":[1,115],\"4\":[2],\"5\":[3,18],\"6\":[2,75],\"7\":[1,46],\"8\":[2,114],\"9\":[2,41],\"10\":[2,58],\"11\":[1],\"12\":[5,132],\"13\":[9,91],\"14\":[6,83],\"15\":[3,56],\"16\":[4,52],\"17\":[5,30],\"18\":[3,13],\"19\":[1,80],\"20\":[1],\"21\":[1,56],\"22\":[5,137],\"23\":[5,39],\"24\":[2,144],\"25\":[1,63],\"26\":[1],\"27\":[1,22],\"28\":[1,41],\"29\":[null,null,1],\"30\":[null,null,5],\"31\":[8],\"32\":[2,19],\"33\":[4,49],\"34\":[1,49],\"35\":[2],\"36\":[2,77],\"37\":[6,108],\"38\":[1,66],\"39\":[1],\"40\":[4,54],\"41\":[9,67],\"42\":[1],\"43\":[1,37],\"44\":[1,15],\"45\":[1,13],\"46\":[1,19],\"47\":[1,28],\"48\":[3,48],\"49\":[3,39],\"50\":[4,52],\"51\":[1,19],\"52\":[1,52],\"53\":[null,null,1],\"54\":[null,null,7],\"55\":[6],\"56\":[2,16],\"57\":[1,60],\"58\":[2,16],\"59\":[2,85],\"60\":[2,20],\"61\":[1,32],\"62\":[1,25],\"63\":[1,94],\"64\":[1],\"65\":[1,52],\"66\":[1,50],\"67\":[1,49],\"68\":[1,19],\"69\":[null,null,1],\"70\":[null,null,5],\"71\":[4],\"72\":[2,13],\"73\":[1,40],\"74\":[2,19],\"75\":[5,78],\"76\":[1,70],\"77\":[2,43],\"78\":[2,92],\"79\":[1,129],\"80\":[1],\"81\":[1,86],\"82\":[4,48],\"83\":[2,55],\"84\":[2,71],\"85\":[1],\"86\":[1,73],\"87\":[1,61],\"88\":[1,12],\"89\":[1,49],\"90\":[null,null,1],\"91\":[null,null,5],\"92\":[1]},\"averageFieldLength\":[2.3400624500085123,53.49612204406415,0.4634876708933956],\"storedFields\":{\"0\":{\"h\":\"About us\"},\"1\":{\"h\":\"Agent57: Outperforming the Atari Human Benchmark\"},\"2\":{\"h\":\"Basic Information\",\"t\":[\"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, et al. @ Google DeepMind\",\"2020 ICML\"]},\"3\":{\"h\":\"問題描述\",\"t\":[\"在 RL 當中，Atari games 是一個相當重要的 benchmark。過去的 RL 模型已經能夠在大多的 atari games 當中獲得相當不錯的 performance，例如 MuZero、R2D2，分別在 57 個遊戲當中有 51 和 52 個遊戲是 outperform 人類的。不過可惜的是，在剩下的遊戲當中這些 SoTA 就通常完全沒辦法學習。\",\"Info\",\"稍微翻了一下 MuZero 以及 R2D2 兩篇 paper 的結果，分別是這些遊戲 performance 不太好。\",\"MuZero \",\"montezuma revenge, pitfall, private eye, skiing, solaris, venture\",\"R2D2 \",\"montezuma revenge, pitfall, private eye, skiing, solaris\",\"那麼，剩下這些遊戲有怎樣的共通點呢？\",\"skiing 和 solaris 這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到 reward，在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響。\",\"Skiing game on Atari 2600. Video from TheLimeyDragon\",\"以 Skiing 這款遊戲來說，玩家要操作角色滑雪，途中要盡可能快速通過指定數量的 gates。每忽略一個 gate 就會多 5 秒的 penalty。Reward 會一直到遊戲的最後依照最後通過的時間決定。\",\"剩下的四款遊戲則是因為環境太大，又有不少的 negative reward，需要相當大量的探索之後才能得到 positive reward。\",\"Pitfall game on Atari 2600. Video from The No Swear Gamer\",\"以 Ptifall 這款遊戲來說，玩家要操作主角在 20 分鐘的時間探索 255 個遊戲場景，去找到藏在地圖當中的寶藏。過程中有許多陷阱，找到寶藏可以加分，最後分數越多越好。\",\"從這些觀察當中可以得到兩個待改善的地方\",\"long-term credit assignment 如何決定哪些 action 應該要給 positive 或是 negative reward\",\"exploration 如何讓 agent 能夠盡可能去正確探索環境 \",\"之所以說\\\"正確\\\"，是因為即便是在很多 negative reward 的地方，也需要嘗試越過那些障礙，也許才有機會遇到 positive reward。\",\"這一篇 paper 希望改善這兩個對 RL 相當重要的問題，也提出了一個可以在所有 57 Atari games 都 outperform 人類的 RL 模型。\"]},\"4\":{\"h\":\"Related Works\"},\"5\":{\"h\":\"Never Give Up\",\"t\":[\"Never Give Up(NGU) 目的也是希望能夠讓 RL agent 能夠在上述 hard-exploration 的環境當中有更好的成效。具體來說 NGU 包含了幾個重要的部分。\",\"Intrinsic Reward\",\"UVFA\",\"RL Loss\",\"NGU Agent\"]},\"6\":{\"h\":\"Intrinsic Reward\",\"t\":[\"在 Intrinsic Reward 的部分目的也是希望能夠促使 agent 多多探索，他們將 reward 分成了兩個部分，分別是 per-episode noveltyrtepisodic​ 以及 life-long noveltyαt​。這兩者分別會讓 agent 鼓勵去探索那些在 episode 當中、在整個訓練過程當中沒有踏足過的狀態。而整體 intrinsic Reward 如下。\",\"rti​=rtepisodic​⋅min{max{αt​,1},L}(L=5)\",\"min 和 max 只是用來限制 life-long novelty 的範圍，避免太大或是太小。\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"而整體的 reward 依照過去 curiosity-driven exploration 的研究，設定如下。\",\"rtβi​​=rte​+βi​rti​\",\"rte​ 是 Extrinsic Reward，在 RL 當中就是環境給予的 reward\",\"rti​ 是 Intrinsic Reward，也就是前面定義的 reward\",\"βi​ 用來調整兩種 reward 的影響程度\",\"不同的環境下需要的 exploration 以及 exploitation 是不同的。當 β 比較大的時候，intrinsic reward 會使得 agent 比較傾向去試試看那些不熟的 state，反之則會去走那些比較熟悉的。\"]},\"7\":{\"h\":\"UVFA\",\"t\":[\"NGU 接下來用 Universal Value Function Approximator, UVFA 去近似 action value function Q。\",\"Q(st​,at​,βi​)=E[rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+…∣st​,at​,βi​]\",\"針對不同的 βi​，NGU 會選擇不同的 γ。\",\"βi​ 大，傾向 exploration，不需要看太遠，γ 選小一些\",\"βi​ 小，傾向 exploitation，需要看遠一些，γ 選大一些\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"左邊是 β 選擇的分布，右邊是 γ 的分布。\"]},\"8\":{\"h\":\"RL Loss\",\"t\":[\"既然有 NN 去逼近，那也就會有 Loss。NGU 計算 Loss 的方式是採用 Transformed Retrace Double Q-learning Loss。\",\"Retrace 是一個可以用來評估或是用在 control 上的 RL 演算法。在這邊我們在意的是評估的部分，Retrace 可以幫助我們去評估如果我們 follow policy μ，在目標的 policy π 的 action value function Qπ 可以拿到多少 Reward。\",\"首先定義從 policy μ 當中取得的 trajectories τ\",\"τ=(xt​,at​,rt​,xt+1​)t∈N​\",\"考慮有限的 sampled sequences，定義 Retrace operator\",\"T^Q(xt​,at​)=Q(xt​,at​)+s=t∑t+k−1​γs−t(i=t+1∏s​ci​)δs​\",\"其中\",\"δt​cs​​=rt​+γa∈A∑​π(a∣xt+1​)Q(xt+1​,a)−Q(xt​,at​)=λmin(1,μ(as​∣xs​)π(as​∣xs​)​)​\",\"實際上訓練的 NN 會有兩個，就跟 DQN 一樣，一個是 target network，一個是 online network。Target network 就可以透過 Retrace operation 去得到目標 yt​^​\",\"yt​^​=T^Q(xt​,at​;θ−)\",\"θ− 是 target network 的 parameter。\",\"有了目標，也就能夠得到 Loss\",\"L(xt​,at​,θ)=(Q(xt​,at​,θ)−yt​^​)2\",\"Tips\",\"上面提及的是單純的 Retrace Double Q-learning Loss，實際上還會為了讓 NN 更好學習，改成 Transformed 版本。\",\"ThQ(x,a)=Eμ​[h(h−1(Q(x,a))+t≥0∑​γt(s=1∏t​cs​)δth​)]\",\"其中\",\"δth​=rt​+γa∈A∑​π(a∣xt+1​)h−1(Q(xt+1​,a)−h−1Q(xt​,at​))\",\"∀z∈R,h(z)∀z∈R,h−1(z)​=sgn(z)(∣z∣+1​−1)+ϵz=sgn(z)((2ϵ1+4ϵ(∣z∣+1+ϵ)​−1​)−1)​\",\"但數學有點太難，我還沒有理解這一段做了什麼。\"]},\"9\":{\"h\":\"NGU Agent\",\"t\":[\"NGU 基本上使用了 R2D2，只不過輸入上會丟\",\"Action at−1​\",\"Extrinsic Reward rt−1e​\",\"Intrinsic Reward rt−1i​\",\"βi​\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"NGU 採用分散式學習，有許多的 actor 使用不同的 βi​ 取得不同的 experience 丟在 replay buffer，然後再讓 learner 使用 experience 去更新參數學習。\",\"最後只需要設定 β=0，就可以得到單純 exploitation 的模型當成最後的結果。\"]},\"10\":{\"h\":\"NGU 的問題\",\"t\":[\"實作上 NGU 有時會很不穩定、難以收斂，尤其當 rti​ 和 rte​ 的大小、分布相當不同時 \",\"Agent57 的作者認為是因為 NGU 只用了一個 NN 去學習導致\",\"不是那麼地 general \",\"解決了一些 hard-exploration 的問題，卻在一些簡單的問題做得很差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"每種 policy(不同 βi​ 的選擇) sample 的 experience 數量相同 \",\"有些 policy 對於學習是並沒有幫助的，但是卻跟其他人有同樣的影響力\",\"有些環境需要更多的 exploration，有些則不需要\",\"無法好好處理 long-term credit assignment 問題 \",\"例如在 skiiing 以及 solaris 就做得頗差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\"]},\"11\":{\"h\":\"Methodology\"},\"12\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"Agent57 首先針對 State-Action Value Function 拆開來，用兩個 NN 分別去針對 Extrinsic 以及 Intrinsic Reward 處理。\",\"Q(x,a,j;θ)=Q(x,a,j;θe)+βj​Q(x,a,j;θi)\",\"x: state\",\"a: action\",\"j: 表示使用的是哪一個 policy 的 one-hot vector\",\"θe: 近似 Extrinsic Reward re 的 NN\",\"θi: 近似 Intrinsic Reward ri 的 NN\",\"θ: θe∪θi\",\"兩個 Q-Network 都會接收同樣的 state 和 action，並且也是 follow 相同的 policy π。\",\"π(x)=arga∈Amax​Q(x,a,j;θ)\",\"兩個模型都是使用 Transformed Retrace Loss，跟 NGU 是一樣的，不過在計算 Loss 時 reward 的部分是分別給 re 和 ri。\",\"細節上，因為是一次更新 B 個 batch，每個 batch sample 的 sequence 大小為 H，因此 Loss 會有兩組總和。\",\"L(D,θ,θ−,π,μ,r,h)=b=0∑B−1​s=t∑t+H−1​(Q(xsb​,asb​;θ)−T^r,hμ,π​Q(xsb​,asb​;θ−))2\",\"D 表示從 μ sample 出來的 trajectories\",\"θ 為 online network 的參數\",\"θ− 為 target network 的參數\",\"π 為目標 policy\",\"μ 為當前 policy\",\"r 表示 reward，上面的差異就是這裡傳入的分別是 re 和 ri\",\"h 為 Transformed Retrace Operator 的 h\",\"xsb​ 是在 batch b、時間 s 的 state\",\"asb​ 是在 batch b、時間 s 的 action\",\"於是 Agent57 的模型變成底下的樣子。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"Note: 雖然兩個模型都會把 intrinsic 以及 extrinsic reward 輸入進去，但 Loss 在計算上分別都只會拿自己的。\",\"Tips\",\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子，也就是說這種做法的正確性是被確保的(無論是否有使用 Transformed 的版本)。\",\"不過實際上訓練時因為拆開來訓練，能夠使模型更好去學習各自的 reward，以達到更好的訓練成效。\",\"透過拆開訓練，解決了 NGU 不穩定、難以收斂的問題。\"]},\"13\":{\"h\":\"Adaptive Exploration over a Family of Policies (Bandit)\",\"t\":[\"「每種 policy sample 的 experience 數量相同」這個問題 Agent57 透過加上 Meta-controller 來解決。\",\"Tips\",\"如果每個 actor 都能夠學習什麼時候該 exploit、什麼時候該 explore，選擇出現傾向，不同 policy 就有不同重要程度了\",\"舉一個例子來說，NGU 會把每個 actor 都當成是工廠生產出來的機器人，每一個 actor 一開始都是一樣的。\",\"接下來依照你的需求不同，你分別把這幾個 actor 加上不同的偏好，有些傾向 exploration，有些傾向 exploitation。\",\"這些 actor 就會去環境當中互動，蒐集一些 experience 給你學習。\",\"另一方面，Agent57 的 actor 天生就有一些自己的偏好，有人天生愛探險，有人天生愛保險。\",\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的 reward。\",\"最後你一樣可以透過這些 actor 蒐集的 experience 去學習。但是 policy 不會被固定下來，具有更高的靈活性。\",\"照著這樣的想法，Agent57 讓每個 actor 前面都加上一組 Meta-controller，在每一個 episode 開始之前，透過它決定接下來要使用的 (βj​,γj​)。此外，Meta-controller 也會依據得到的 reward 去調整選擇不同 j 的機率。\",\"如此一來，每個 actor 就會因為 Meta-controller 的存在，產生出選擇 policy 的傾向，進而使得整體訓練採用的 experience 中 policy 的比例改變。\",\"Warning\",\"細節上，每個 actor 選擇 action 都是採用 ϵl​-greedy，其中的 l 表示不同的 actor。亦即，不同 actor 採用不同的 ϵ 大小，也因為如此，Meta-controller 是每個 actor 各有一個。\"]},\"14\":{\"h\":\"Upper Confidence Bound Algorithm (UCB)\",\"t\":[\"Agent57 把 Meta-controller 簡單設計成一個 Multi-Arm Bandit (MAB) 問題，也就是說我現在面前有 N 個 action {0,…,N−1} 可以選擇，在時間 k 你選擇 Ak​，目標是在整個 horizon K 當中你可以得到最好的 return，也就是讓底下的期望值最大化。\",\"Eπ​[k=0∑K−1​Rk​(Ak​)]\",\"過去對於 MAB 在 reward 的分布是固定的狀況下會使用 UCB 來解決它。基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界，把這個上界當成是它預期的 return，選擇其中最大的當成這次的選擇。\",\"未知/嘗試次數少的選擇 (不確定性高，要傾向 exploration) \",\"平均 Return 低 ➡️ UCB 高 ➡️ 探索機率高\",\"平均 Return 高 ➡️ UCB 更高 ➡️ 探索機率更高\",\"已知/嘗試次數多的選擇 (不確定性低，要傾向 exploitation) \",\"平均 Return 低 ➡️ UCB 低 ➡️ 嘗試機率低\",\"平均 Return 高 ➡️ UCB 高 ➡️ 嘗試機率高\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a)+βNk−1​(a)log(k−1)​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a)μ^​k​(a)​=m=0∑k−1​1{Am​=a}​=Nk​(a)1​m=0∑k−1​Rk​(a)1{Am​=a}​​\",\"也就是說\",\"Nk​(a) 用來表示一個 action a 至今被嘗試的次數\",\"μ^​k​(a) 用來表示一個 action a 至今平均的 Return\",\"從式子當中也可以觀察到，確實它會傾向讓 平均 Return 高 或是 嘗試次數少 的選項有更高機率被選擇到。\"]},\"15\":{\"h\":\"Sliding-Window UCB\",\"t\":[\"然而，如果 reward 的分布會變動的話，單純的 UCB 並不會是一個好的選項，因為過去的經驗即便在現實狀況改變仍然有大影響力。而隨著 agent 更新、行為模式改變，reward 的分布也會變動。\",\"這裡的經驗指的是一個 action 採取的次數以及得到的 Return 平均 (Nk​(a) 和 μ^​k​(a))。\",\"因此 Sliding-Window UCB 加上了一個 window length τ∈N∗ 來限制要考慮多久之前的經驗。\",\"τ 的選擇應遠比 K 小。\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)log(min(k−1,τ))​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a,τ)μ^​k​(a,τ)​=m=max(0,k−τ)∑k−1​1{Am​=a}​=Nk​(a,τ)1​m=max(0,k−τ)∑k−1​Rk​(a)1{Am​=a}​​\",\"僅僅是加上 τ 而已，剩餘的都是相同的。\"]},\"16\":{\"h\":\"Simplified Sliding-Window UCB\",\"t\":[\"最後，Agent57 對 Sliding-Window UCB 做了兩個小修正\",\"log 對於結果並不會有影響，可以移除\",\"多加上 ϵ-greedy\",\"Ak​=⎩⎨⎧​kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)1​​Yk​​∀0≤k≤N−1∀N≤k≤K−1,Uk​≥ϵUCB​∀N≤k≤K−1,Uk​<ϵUCB​​\",\"其中\",\"ϵUCB​ 是一個 hyperparameter\",\"Uk​ 是一個 [0,1] 之間均勻分布的隨機值\",\"Yk​ 是一個 {0,…,N−1} 之間均勻分布的隨機 action\",\"Tips\",\"透過 Bandit，每個 actor 能夠調整自己的 (γ,β)，解決了 NGU「不是那麼地 general」、「每種 policy sample 的 experience 數量相同」這兩個問題。\"]},\"17\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"原先 R2D2 在 Replay buffer 的設計是採用 trace length 80 搭配 replay period 40，作者在實驗當中發現如果採用 trace length 160 搭配 replay period 80，也就是 long trace 的話，對於 long-term credit assignment 的問題似乎能夠得到改善。\",\"Tips\",\"透過 long trace 解決了 NGU「無法好好處理 long-term credit assignment」的問題。\"]},\"18\":{\"h\":\"High-level architecture\",\"t\":[\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"19\":{\"h\":\"Actors\",\"t\":[\"每個 episode 開始前，透過各自的 Meta-Controller 選擇出一組 (γj​,βj​)\",\"透過上一個 trajectory (xt​,rt−1e​,rt−1i​,at−1​,ht−1​) 估計當前 state-action value Q(xt​,⋅,j,θl​)\",\"透過 ϵl​-greedy 選擇 action\",\"計算 intrinsic reward rti​\",\"環境中取得 observation xt+1​, extrinsic reward rte​\",\"若已經又經過 400 個 frames，更新模型參數\",\"重複 2 直到 episode 結束\",\"將 trajectories 交給 replay buffer\",\"ϵl​ 的選擇根據 Dan Horgan, John Quan, David Budden, et al. (2018) 如下\",\"ϵl​=ϵ1+αL−11​\",\"其他部分基本上都跟 NGU 相同。總之，Actors 去跟環境互動，取得 experience 之後交給 replay buffer，Learner 會從 replay buffer 當中 sample 一些 experience 學習，然後繼續跟環境互動。\"]},\"20\":{\"h\":\"Results\"},\"21\":{\"h\":\"Settings\",\"t\":[\"Agent57 在 γ 的分布上有做了一點調整，範圍變成 [0.99,0.9999]，具體來說如下圖\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"其他 Hyperparameter 的設定詳閱論文的 Appendix G，這裡就不贅述。\",\"對於每個實驗的 Agent 都另外加上一個 Evaluator 去紀錄訓練過程當中的 undiscounted episode returns。\",\"此外，他們並不是採用 Human Normalized Scores (HNS)，而是 Capped Human Normalized Scores (CHNS)，這個測量標準比較強調那些 HNS 比較差的結果，也限制了數值範圍，因此會比較能夠好好評估 general performance。\",\"CHNS=max{min{HNS,1},0}\",\"其中\",\"HNS=Humanscore​−Randomscore​Agentscore​−Randomscore​​\"]},\"22\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"我們透過 intrinsic 以及 extrinsic 拆開來解決 NGU 的缺陷，這裡要來實驗這一個做法實際上帶來多少影響。\",\"作者建構一個簡單的 15×15 Gridworld random coin。在每個 episode 開始之前他們把一個 agent 以及一個 coin 隨機地放在地圖上的任意格子。Agent 能夠上、下、左、右移動，並且每個 episode 最多 200 個 steps。當 Agent 走到 coin 會得到 reward 1，然後結束這個 episode。\",\"接著作者比較 NGU 以及 NGU 加上 separate network 的做法。如同前面提及 βj​ 如果選擇較大，由於 intrinsic reward 有較大的影響，agent 會偏向 exploration，反之則是 exploitation。細節上，βj​ 的設定會透過 β 來調整整體 βj​ 的大小。\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"作者比較兩個模型在不同 β 的大小下，各自最傾向 exploration (βj​=maxj​βj​) 以及最傾向 exploitation (βj​=0) 的設定取得的 Extrinsic Reward。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"x 軸表示 β (注意並不是 βj​)\",\"y 軸表示 extrinsic reward\",\"紫色圓點表示 βj​=0，最傾向 exploitation 的狀況\",\"綠色圓點表示 βj​=maxj​βj​，最傾向 exploration 的狀況\",\"從結果可以發現到 NGU 在不同 β 的設定下會大程度影響到最終 exploitation 的結果，即便這個環境設定是相當簡單的，最終 Return 的趨勢仍然是隨著 β 越大變得越小。\",\"另一方面，加上了 separate network 的狀況下 exploitation 的 return 基本上都相當接近 1.0，也就是說能夠順利到達 coin 所在的位置。\",\"在 exploration 的部分也可以發現到兩者的發展方向會稍有不同。但整體來說兩者都能在最後趨近於 0.0。\",\"由此可見，當 β 提升，由於 intrinsic reward 與 extrinsic reward 的大小相差越來越懸殊，導致 NGU 並無法好好只透過一個 NN 去學習，進而影響到結果，較不具有彈性。相對的，增加 separate network 確實能夠帶來相當好的效益。\",\"此外，作者也發現如果把 Agent57 的 separate network 移除，performance 會掉 20% 以上，可見 separate network 的重要性。\",\"作者也發現到 separate network 在最傾向 exploration 的模型會盡可能避開 coin，反之會走出最短路。\",\"Tips\",\"值得一提的是，這個結果如果在取得 coin 之後仍然不會停止的話就不會出現。\"]},\"23\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"在 trace length 以及對應的 replay period 有多少影響呢？\",\"作者將 R2D2 以及 Agent57 分別用 small trace 以及 long trace 來比較，作者認為在這兩者都有一個共通點：Long trace 會導致訓練前期較為緩慢，但最後能取得更好的 performance。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"在 10 個比較難的遊戲當中測試的結果\",\"尤其在 Solaris 這一款遊戲，可以看到比較明顯的結果。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"24\":{\"h\":\"Adaptive Exploration\",\"t\":[\"最後是針對 Meta-Controller 的實驗。作者將 R2D2+sep. network 以及 NGU+sep. network 拿來比較加上 Meta-Conroller 以及沒有的狀況。\",\"在 10 個比較困難的遊戲當中，可以發現到加上 Meta-Controller(圖片中以 bandit 表示)後可以得到更好的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"此外，從上面的圖片中也可以觀察到這樣的 improvement 在 NGU 當中是小許多的。可以認為 separate networks 跟 meta-controller 之間有一些重疊的 benifit\",\"另一方面，有了 meta-controller 之後，即便 discount factor γ 異常地大(如 γ=0.9999)，模型還是能夠順利學習。在下表當中可以看到 high gamma 的 R2D2，在搭配了 meta-controller 之後得到的成效在 10 款比較困難的遊戲當中有些甚至是能夠比 Average Human 還要強。\",\"Games\",\"R2D2(Retrace) high gamma\",\"Average Human\",\"beam rider\",\"349971.96 ± 5595.38\",\"16926.50\",\"freeway\",\"32.84 ± 0.06\",\"29.60\",\"montezuma revenge\",\"1664.89 ± 1177.26\",\"4753.30\",\"pitfall\",\"0.00 ± 0.00\",\"6463.70\",\"pong\",\"21.00 ± 0.00\",\"14.60\",\"private eye\",\"22480.31 ± 10362.99\",\"69571.30\",\"skiing\",\"-4596.26 ± 601.04\",\"-4336.90\",\"solaris\",\"14814.76 ± 11361.16\",\"12326.70\",\"surround\",\"10.00 ± 0.00\",\"6.50\",\"venture\",\"1774.89 ± 83.79\",\"1187.50\",\"因此作者認為 meta-controller 提供了更大的普遍性，即便在參數比較異常的狀況下仍然能有很不錯的學習成果。\",\"最後，作者也觀察了在幾款遊戲訓練過程中當中 Meta-Controller 在每個 bandit 選擇中最大的 return 分別落在哪個 bandit，可以發現到不同的遊戲會有不同的偏好。從這裡也可以了解到實際上讓每個 actor 自己調整 policy、適應不同的環境，實際上是有幫助的。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"25\":{\"h\":\"Summary\",\"t\":[\"最後比較 R2D2、NGU、Agent57、MuZero 在所有 Atari games 的優劣，可以發現到 MuZero 雖然在 uncapped mean 有最好的結果，但是在 capped mean 卻是最差的。顯示了 MuZero 在限定幾款遊戲有特別出色的成效，但並不 general。\",\"同時也可以看到 Agent57 有最大的 Capped Mean 100，亦即 Agent57 能夠在所有的 Atari games 當中獲得比人類平均還要好的成果，除了展現驚人的成果以外，也說明了 Agent57 的普遍性。\",\"同時也能在 R2D2 與 R2D2 bandit 的比較當中明顯看到在所有的成績都有所提升，再次說明了 Meta-Controller 帶來的效益。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"最後，Agent57 透過 separate networks、Meta-Controller、long trace 解決了 NGU 的四個缺陷，最終在所有的 Atari games 當中都獲得了超過人類的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"26\":{\"h\":\"Discussion\"},\"27\":{\"h\":\"Contribution\",\"t\":[\"提出透過 separate networks 解決訓練不穩定、難以收斂的問題\",\"提出 Meta-Controller 來讓每個 actor 自適應不同環境，使模型具有更好的普遍性，並且不同 policy 得以有不同程度的影響\",\"第一個能夠在所有 Atari games 都獲得比 Average Human 更好的成效\"]},\"28\":{\"h\":\"值得一看的文章們\",\"t\":[\"Agent57: Outperforming the human Atari benchmark\",\"Recurrent Neural Networks in Reinforcement Learning\",\"MAB - UCB <> TS 基本概念\",\"Safe and efficient off-policy reinforcement learning.\",\"Never Give Up: Learning Directed Exploration Strategies\",\"Adapting Behaviour for Learning Progress\",\"Agent57\",\"Distributed Prioritized Experience Replay\",\"Recurrent experience replay in distributed reinforcement learning\"]},\"29\":{\"c\":[\"Note\"]},\"30\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICML\"]},\"31\":{\"h\":\"DACS: Domain Adaptation via Cross-domain Mixed Sampling\"},\"32\":{\"h\":\"Basic Information\",\"t\":[\"2020 Release\",\"2021 WACV(Winter Conference on Applications of Computer Vision)\",\"Chalmers University of Technology(查爾摩斯理工大學)與 Volvo Cars 共同發表\"]},\"33\":{\"h\":\"What is Domain Adaption\",\"t\":[\"Image from Medium\",\"所謂的 Domain 就是用來描述一群資料他們的分布狀況。\",\"Domain Adaption 的目標是把兩個不同分佈的 Domain (Source Domain 以及 Target Domain) 投射到同一個平面上，使得同類型的資料會相近，反之則相遠。\",\"舉一個在 CV 上的例子。如果我們想要訓練一個模型去做自駕車的街景物件偵測，很多時候我們並不會直接去蒐集真實的資料，像是直接有一台車會去蒐集真實街景資料，這樣所需要的成本會過大。時常我們會訓練在合成資料上(synethic data)，然後再應用在真實的世界當中。\",\"Image from Medium\",\"不過這種情況下一個直覺的問題是，在 虛擬世界(Source Domain) 上也許我們能夠對各種物件去做標記 label，但是對於真實世界(Target Domain)往往會有許多我們沒有的 label、環境與虛擬世界有差距，這種差距被描述為 Domain Shift。當兩個 Domain 相差過大，Domain Shift 過高，就會導致單純在 Source Domain 上訓練的模型難以直接 apply 到 Target Domain 上。\",\"因此，Domain Adaption 想解決的就是盡可能地將 Domain Shift 降低，讓我們得以用較低的成本在虛擬環境中訓練模型，然後應用在真實的環境當中。\"]},\"34\":{\"h\":\"問題描述\",\"t\":[\"近年來透過 CNN 處理 semantic segmentation(影像分割) 的模型雖然有許多，也獲得不錯的成果，不過如果遇到新的 domain，往往就會 work 不太好，尤其是從 synethic data 轉變到 real data 上的時候。\",\"問題在於不同的 domain，各自的 domain distribution 會不同。只訓練在 source domain 的模型對於 target domain 的狀況缺乏認知，導致預測失準。\",\"Info\",\"這就像是同理心，因為缺乏對他人的理解，擅自用自己的思維解讀，就會導致互相的不理解。\",\"Image from Liang-ChiehChen et al. (2015)\",\"可以發現單純用 CNN 就可以得到相當好的影像分割結果。\",\"Image from Yiheng Zhang et al. (2018)\",\"直接把訓練在虛擬環境的模型應用在真實環境，結果相當糟糕。\"]},\"35\":{\"h\":\"Related Works\"},\"36\":{\"h\":\"Domain Alignment\",\"t\":[\"透過 adversarial learning (對抗式學習) 去拉近 source domain 以及 target domain。\",\"我們可以想成現在 Segmentation Network 就是 GAN 的 Generator，然後會有一個 Discriminator 去判別現在給我的究竟是 source domain 還是 target domain 的預測結果。\",\"Image from Yi-Hsuan Tsai et al. (2018)\",\"兩個 Domain 中各取圖片，經過相同的 Segmentation Network，將產出的 semantic maps 做對抗式學習\",\"Info\",\"依照 alignment 的不同，可以分成 pixel level, feature map level, semantic level 等不同的做法。\",\"這樣的做法之所以可行，是源自於即便 domain 不同，在 semantic maps 上的 spatial layout 以及 local context 通常並不會差太多。\",\"DACS 的做法之所以能夠成功，也有部分是源自於這樣的相似性帶來的好處。\",\"Tips\",\"同樣以自駕車的例子來說，即便 synethic data 和 real data 的 domain 有相當大的差異，不過像是馬路、汽車、行人都還是會跟地板黏在一起，其他像是路燈、號誌、天空之類的就通常會像是在半空中。這類的 spatial layout 就相當地雷同。\",\"Image from Yi-Hsuan Tsai et al. (2018)\"]},\"37\":{\"h\":\"pseudo labelling (or self-training)\",\"t\":[\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"最初是為了解決 半監督式學習(Semi-Supervise Learning, SSL) 而被提出的。\",\"Info\",\"所謂的半監督式學習也就是說 target domain 的資料上只有一些 labeled data，其他絕大多都是 unlabeled data，這種狀況下訓練模型就被稱為半監督式學習。\",\"而半監督式學習困難的點在於雖然對於 Target Domain 有部分的認知，但是並不全面。\",\"一個簡單的方法是想辦法給這些 unlabeled data 一些 pseudo label。那我們就可以用 supervise learning 的方法解決了。\",\"舉例來說，先在 labeled data 上訓練一個模型，透過這個模型我們就有辦法給 unlabeled data 做 prediction，而 prediction 的結果就當作是他的 pseudo label，就可以再拿去 fine-tune model 了。\",\"Image from Sylwia Majchrowska et al. (2021)\",\"但主要的問題來自於 Domain Shift，畢竟 Source Domain 和 Target Domain 還是存在差異的，並不是所有的 Target Data 都能夠透過 Source Data 去轉移出來。\",\"尤其在 Unsupervised Domain Adaption(UDA) 來說是相當大的問題，在 UDA 當中通常 Domain Shift 都會特別大。\",\"Info\",\"所謂的 UDA 也就是說我們對於 Target Domain 的資料不存在任何 label。換句話說，我們對於 Target Domain 缺乏 label 上的認知。\",\"對於 UDA 來說由於缺乏對於 Target Domain 的認識，一個常見的問題是產出的結果通常會傾向去預測結果為常見的 class。\",\"Info\",\"對陌生人的認識，往往先從貼標籤開始。\",\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的 class，如果出現道路或甚至機車，有可能就被誤判成人行道。或是汽車比卡車更常見，導致卡車時常被預測成汽車。\",\"Image from Yang Zou et al (2018)\",\"看 column 4，只有 pseudo labeling 的例子\",\"Info\",\"雖然已經有 paper 提出如 CBST 的方法來降低這種問題，但在邊界上往往還是難以有好的結果。\"]},\"38\":{\"h\":\"Mixing\",\"t\":[\"Mixing 基本上就是從 training image 拿出兩張，透過一些方式混在一起，產生一個新的 training image。最初被用於把 unlabeled image 混合成新的圖片，是一種 data augumentation 的技巧。\",\"像是 Mixup 這種 data augumentation 方法也是屬於 Mixing 的一種。\",\"DACS 當中使用的是 ClassMix 這種 Mixing 方法。\",\"具體來說，ClassMix 的步驟\",\"把兩個圖片 (A,B) 先轉成 semantic map (SA​,SB​)\",\"把 SA​ 其中一半的 classes 對應的 semantic map 做出一個 binary mask (M)\",\"把 mask M apply 在 A 上，跟 B 合成出 XA​。\",\"把 mask M apply 在 SA​ 上，跟 SB​ 合成出 XA​ 對應的 semantic map YA​\",\"Image from Viktor Olsson et al. (2020)\",\"這樣的做法有趣的是能夠將 semantic segmentation 在邊界上往往會出現誤差的問題解決。\",\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清。但透過剪貼則可以造成不同環境的突兀感，進而解決這個問題。因此這時候 pseudo labelling 就能夠比較好發揮作用。\",\"Image from Viktor Olsson et al. (2020)\"]},\"39\":{\"h\":\"Methodology\"},\"40\":{\"h\":\"Naive Mixing to UDA\",\"t\":[\"最 Naive 的做法就是照著 ClassMix 的方法，將 unlebelled dataset Mixing 成新的 dataset，把 labelled dataset 以及 mixed dataset 拿去訓練。\",\"Info\",\"在 UDA 當中，unlabelled dataset 就是 target domain dataset。\",\"Image from Wilhelm Tranheden at al.\",\"但是這種做法實際上效果很糟糕。像是 sidewalk 被預測成 road，rider 被預測成 person 之類的，許多的 class 都被其他 class 覆蓋。這樣的問題只在 target domain 上會發生，這跟前面提到只使用 pseudo labelling to UDA 會造成的問題是吻合的。\",\"Image from Wilhelm Tranheden at al.\",\"單純的 Naive Mixing 往往在邊界上會有許多誤判的 class\",\"Tips\",\"這種相似的 class 相鄰而導致的誤判被稱為 class conflation\"]},\"41\":{\"h\":\"Domain Adaption via Corss-domain mixed Sampling (DACS)\",\"t\":[\"DACS 的核心做法是不單只是跟 Target Domain 去 mixing，而是將 Source 跟 Target 一起 Mix。如此一來， Target Domain 以及 Source Domain 的關聯性就能被連結起來，降低 Domain Shift。\",\"Image from Wilhelm Tranheden at al.\",\"詳細的步驟具體來說\",\"從 Source Domain (DS​) 取出圖片與 lebel (XS​,YS​)\",\"從 Target Domain (DT​) 取出圖片 XT​\",\"透過 segmentation network fθ​ 取得 XT​ 的 pseudo label YT​^​\",\"將 (XS​,YS​),(XT​,YT​^​) 經過 ClassMix 得到 (XM​,YM​)\",\"把 (XS​,YS​),(XM​,YM​) 拿去訓練。\",\"Image from Wilhelm Tranheden at al.\",\"在 Loss 的設計上也相當直覺，就是希望 XS​ 的預測結果要接近 YS​，XM​ 的結果要接近 YM​。\",\"H: Cross-Entropy\",\"λ: 調整 Mixing 部分的影響程度\",\"L(θ)=E[H(fθ​(XS​),YS​)+λH(fθ​(XM​),YM​)]\"]},\"42\":{\"h\":\"Results\"},\"43\":{\"h\":\"實驗設定\",\"t\":[\"在 segmentation network 的設定上參考了許多過去的研究，選擇採用 DeepLab v2 搭配 ResNet101 作為 backbone。\",\"ResNet101 是 pretrained on ImageNet 跟 MSCOCO。而 Hyperparameter 的設定基本上跟 Yi-Hsuan Tsai et al. (2018) 一樣。\",\"在 Mixing 的方法上雖然任何 based on binary mask 的 Mixing 都可以使用，不過這裡最主要都是使用 ClassMix。\"]},\"44\":{\"h\":\"Dataset\",\"t\":[\"在 synthetic-to-real 有一些常見的 benchmarks。\",\"GTA5 -> Cityscapes\",\"SYNTHIA -> Cityscapes\",\"GTA5 以及 SYNTHIA 都是虛擬世界當中的影像，而 Cityscapes 則是現實世界當中的影像。\"]},\"45\":{\"h\":\"Cityscapes\",\"t\":[\"照片是在城市當中開車拍下的各種照片\",\"Image from Marius Cordts et al. (2016)\",\"2975 training images\",\"19 classes\"]},\"46\":{\"h\":\"GTA5\",\"t\":[\"照片是在 GTA5 下拍攝的\",\"Image from Stephan R. Richter et al.\",\"24966 synthetic training images\",\"19 classes \",\"可對應到 Cityscapes 的 classes\"]},\"47\":{\"h\":\"SYNTHIA\",\"t\":[\"照片是在 Unity 建構的 virtual city 下拍攝\",\"Image from GermanRos et al. (2016)\",\"9400 synthetic training images\",\"16(or 13) classes \",\"都會對到 Cityscapes 的 classes\",\"13 個 classes 的版本是少了 Wall, Fence, Pole\"]},\"48\":{\"h\":\"GTA5 -> Cityscapes\",\"t\":[\"Image from Wilhelm Tranheden at al.\",\"其他的 Model 都是 DeepLab-v2，他們選擇其中 Performance 最好的，但 Backbone 並不一定要是 ResNet 101\",\"Image from Wilhelm Tranheden at al.\",\"Source 是只有使用 source domain 去 train 的模型\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，只對簡單的 class 像是 Road, Build, Veg, Sky, Person, Car 這些普遍做得不錯的 class 有還不錯的 Performance\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 有點偏以及 Train 真的很糟\"]},\"49\":{\"h\":\"SYNTHIA -> Cityscapes\",\"t\":[\"考慮到 SYNTHIA 有些 paper 使用 16 個 classes，有些是 13 個 class 的版本，所以在數據上 mIoU 有兩列分別表示 13 個平均跟 16 個的平均。\",\"Image from Wilhelm Tranheden at al.\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，甚至對 Road 的 Performance 都不太好\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 頗偏\"]},\"50\":{\"h\":\"Some issues about evaluation\",\"t\":[\"他們認為在其他的 paper 有不少人最後給的結果之所以那麼好看是因為\",\"Cityscapes 並沒有 testset\",\"他們選擇用 validation set 判斷要不要 early stop，這個 validation set 也跟最後評估的 set 是一樣的\",\"針對 validation set 挑選 hyperparameters (?)\",\"所以他們認為這樣不太公平，畢竟在 Validation set 做得很棒不能直接表達在整體會表達很棒。 他們也試著用相同的手段訓練模型，然後拿到了\",\"GTA5 \",\"Baseline: 35.68% (+2.83%)\",\"DACS: 53.84% (+1.7%) (BEST)\",\"SYNTHIA \",\"DACS (13 classes): 55.98% (+1.17%) (1.02% to BEST)\",\"DACS (16 classes): 49.10% (+0.76%) (0.7% to BEST)\"]},\"51\":{\"h\":\"Contribution\",\"t\":[\"Apply SSL method on ClassMix to UDA\",\"Introduce a simple framework with high-performance\",\"Beat SOTA in GTA5 to Cityscape\"]},\"52\":{\"h\":\"值得一看的文章們\",\"t\":[\"【Day 24】半監督式學習（Semi-supervised Learning）（上）\",\"【Day 25】半監督式學習（Semi-supervised Learning）（下）\",\"Notes on “DACS: Domain Adaptation via Cross-domain Mixed Sampling”\",\"物件偵測的領域自適應 (Domain Adaptation)\",\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"Domain Adaptation in Computer Vision: Everything You Need to Know\",\"Semi-supervised semantic segmentation needs strong, varied perturbations\",\"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\",\"Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training\",\"Learning to Adapt Structured Output Space for Semantic Segmentation\"]},\"53\":{\"c\":[\"Note\"]},\"54\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"WACV\"]},\"55\":{\"h\":\"Playing Atari with Deep Reinforcement Learning\"},\"56\":{\"h\":\"Basic Information\",\"t\":[\"2013 NeurIPS\",\"Volodymyr Mnih, Koray Kavukcuoglu David Silver et al.\",\"這個論文提出的做法稱為 DQN(Deep Q-Networks)\"]},\"57\":{\"h\":\"問題描述\",\"t\":[\"過去在 RL 領域當中把一些 high-dimensional 的感官資料（如：視覺影像、語音資料等）作為 agent 的輸入去學習一直是一個很大的挑戰。然而我們也看到近幾年 Deep Learning 已經能夠在這種資料上去擷取特徵，進而去完成許多複雜的任務。\",\"所以「能不能把 Deep Learning 的成功也放進 RL 當中呢？」這樣的想法自然而然就出現了。\",\"不過從 Deep Learning 的角度來看 RL 的話，會有幾個明顯的問題。\",\"RL 的訓練資料（如：Reward）需要透過與環境互動取得，但數值範圍往往很 sparse，而且也往往會經過一段時間的延遲才取得 與 Deep Learning 相較之下，DL 的資料通常都會先 Label 好，可以直接把資料之間的關聯建構起來。\",\"RL 的訓練資料具有高度相關性 在 DL 當中我們會預設資料之間是沒有什麼相依性的，但在 RL 當中同一個 episode 的 state、action、reward 之間都會具有相當高的相關性。\",\"RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化 DL 往往假設資料的分布會維持住。\",\"這一篇論文成功將 CNN 應用在 RL 上，也避免了上述提及的幾個問題。\"]},\"58\":{\"h\":\"Related Works\",\"t\":[\"Q-Networks\",\"TD-gammon\",\"收斂性相關研究 \",\"Residual algorithms: Reinforcement learning with function approximation.\",\"Q-learning\",\"Neural fitted Q-learning (NFQ)\"]},\"59\":{\"h\":\"Q-Networks\",\"t\":[\"在 RL 當中我們會透過 MDP 去 model 整個問題，而 RL 的目標就是要讓整體的 reward 總和最大化。\",\"定義 optimal action value function Q∗ 如下\",\"Q∗(s,a)=πmax​E[Rt​∣st​=s,at​=a,π]\",\"也就是在 state s 採取 action a 並 follow policy π 得到的最大 return。其中 Return 的定義如下，這裡考慮有 discount 的版本。\",\"Rt​=t′=t∑T​γt′−trt′​\",\"γ 為 discount factor\",\"rt​ 表示在時間 t 取得的 reward\",\"既然 RL 的目的是要讓整體的 return 最大化，也就是要找到 Q∗ 了。Q-Network 就是用 Neural Network 來近似 Q∗，也就是要讓底下的 Loss 最小化。\",\"Li​(θi​)yi​​=Es,a∼ρ(⋅)​[(yi​−Q(s,a;θi​))2]=Es′∼ε​[r+γa′max​Q(s′,a′;θi−1​)]​\",\"需要特別注意到對於 θi​ 來說，他要去近似的是 θi−1​ 的模型得出來的結果，也就是說，Q-Network 透過固定訓練的目標(Target Network)，解決了前面提及的第三個問題「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"60\":{\"h\":\"TD-gammon\",\"t\":[\"一個 model-free RL 算法，透過一個 multi-layer perceptron 和一層 hidden layer 去預測 value function，成功在雙陸棋上面 outperform 人類。\",\"不過這裡的成功只停止在雙陸棋上，並無法繼續擴充到其他的領域。\"]},\"61\":{\"h\":\"收斂性相關研究\",\"t\":[\"過去的研究當中發現到如果是 model-free 搭配 non-linear function approximators 或是 off-policy learning 的話會導致 Q-network 發散，無法收斂。\",\"後續的研究中則發現到 Q-network 無法收斂的問題可以透過 gradient TD 舒緩，並且證明了底下兩個狀況是可以確保收斂。\",\"固定 policy，使用非線性的 approximator\",\"使用線性的 approximator 去學 control\",\"然而這些研究都並未能夠給出用非線性去學 control 的方法。\"]},\"62\":{\"h\":\"NFQ\",\"t\":[\"跟這一篇 paper 最相近的一個研究，他們會先透過 Computer Vision 的模型萃取出圖片的特徵，然後再把這些特徵丟去給 RL 訓練。\",\"不過 DQN 與 NFQ 不同的地方在於 DQN 是 end-to-end，也就是說可以直接從 visual input 去訓練，而 NFQ 不是。\"]},\"63\":{\"h\":\"Methodology\",\"t\":[\"在 TD-gammon 當中我們看到了使用 Neural Network 去學習 value function 有還不錯的成效，DQN 稍微修改了這個做法，將 Q-Network 和 Experience Replay 結合起來。\",\"Experience Replay 會將 agent 跟環境的互動過程當中的 experience 記錄在 replay memory D 當中。當要去更新模型的時候，我們是從 replay memory 當中取得隨機幾筆去更新。\",\"Info\",\"在時間 t 的 experience 包含了 state, action, reward, next state\",\"et​=(st​,at​,rt​,st+1​)\",\"因此 experience 就定義成\",\"D=e1​,e2​,…,eN​\",\"在經過 experience replay 之後，agent 會透過 ϵ-greedy 去選擇 action。\",\"實作上 experience 只會儲存最後 N 筆，並且 history 當中的 frames 只會取出最後 4 個，拿出來做一些 preprocess ϕ(s) 之後作為實際上儲存進 experience 的 state。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"透過 Q-Network 中的 Target Network 以及 Experience Replay，DQN 順利避免了最初提及的兩個問題「RL 的訓練資料具有高度相關性」以及「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"64\":{\"h\":\"Results\"},\"65\":{\"h\":\"實驗設定\",\"t\":[\"實驗做在 7 個 Atari games，Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest 以及 Space Invaders。\",\"每個遊戲的 reward 一開始都不太相同，實驗上調整了 reward 的大小，使得所有 positive reward 固定為 1；negative reward 為 −1，其餘則為 0 表示不影響。\",\"並且會使用 frame skipping 的技巧，讓 agent 只會每經過 k 個 frames 才會去擷取畫面，並且做出相對應的 action。至於那些被忽略的 frames，就持續上一個做出的 action。除了 Space Invaders 因為遊戲當中的雷射會跑很快，所以設定 k=3，其他遊戲則都是 k=4。\"]},\"66\":{\"h\":\"評估方式\",\"t\":[\"在 Deep Learning 當中如果要評估一個 Network 的好壞，可以單純透過觀察模型在 validation set 上的 performance 即可，但是在 RL 當中並沒有 validation set，因此評估一個 agent 的好壞就相對困難。\",\"過去會透過多次遊戲中 agent 獲得的 reward 平均去評估，也就是說理想上每經過一輪更新，模型能夠得到的 reward 應該要慢慢變大。不過作者發現在他們的模型得出來的結果往往會是很不穩定的。作者推測是因為權重即便只有小的變化也會對 policy distribution 有大的影響，導致接下來會經過的 state 就很不相同。\",\"因此作者改成 Q 的平均去評估，也確實發現會平滑許多。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\"]},\"67\":{\"h\":\"比較基準\",\"t\":[\"最後將 DQN 跟幾個 Baseline 去比較\",\"Sarsa \",\"On-control policy\",\"Linear approximator\",\"人工提取 features\",\"Contingency \",\"跟 Sarsa 類似，但有包含了部分的學習過程\",\"HNeat Best \",\"訓練的過程包含了一點專家系統的概念\",\"事先標記好 object 的位置以及類型\",\"HNeat Pixel \",\"訓練的過程包含了一點專家系統的概念\",\"事先處理好了 8 color channel representation\",\"Human\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"最後得出來的結果，DQN 在幾乎所有的遊戲當中都 outperform 所有的算法，證明了 DQN 的成功。\"]},\"68\":{\"h\":\"Contribution\",\"t\":[\"成功結合 Deep Learning 以及 Reinforcement Learning\",\"直接從 raw RGB 當作輸入，不需要事先經過其他的分解\",\"透過 Experience Replay 以及 Target Network 解決過去 Deep Learning 結合 RL 時訓練不佳的問題\"]},\"69\":{\"c\":[\"Note\"]},\"70\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"NeurIPS\"]},\"71\":{\"h\":\"Noisy Networks for Exploration\"},\"72\":{\"h\":\"Basic Information\",\"t\":[\"2018 ICLR\",\"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind\"]},\"73\":{\"h\":\"問題描述\",\"t\":[\"在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"例如在 Alpha Go 當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",\"Info\",\"論文中有提及在 Matthieu Geist, Olivier Pietquin (2014) 有提及一個使用 Neural Network 的方法，不過並沒有保證收斂，因此仍然沒有解決問題。\",\"因此這一篇論文提出一個方法試圖去消除 exploration 效率與品質的問題。\"]},\"74\":{\"h\":\"Related Works\",\"t\":[\"ϵ-greedy、Entropy Regularization\",\"Parameter Space Noise for Exploration\",\"用來加上 Noisy-Net 的各種 RL 架構 \",\"DQN\",\"Double-DQN\",\"Dueling DQN\",\"A3C\"]},\"75\":{\"h\":\"Parameter Space Noise for Exploration\",\"t\":[\"2017 年由 OpenAI 發表在 ICLR 的 paper。其方法與這一篇可說是大同小異。\",\"在過往的研究可以發現到說往往我們在設計讓 agent 有更多的 exploration 的時候都是透過增加 noise 來達成。\",\"Info\",\"舉例而言，ϵ-greedy 就是在 action space 上增加了 noise，讓選擇更多樣，以達成 exploration。\",\"而在 A3C 當中加上 Entropy Regularization，是在 Loss 上鼓勵 policy 的亂度越高越好，達到鼓勵 exploration 的效果。\",\"Image from OpenAI - Better exploration with parameter noise\",\"核心的概念很簡單，過去增加探索的方法大多都是在 action space 上增加 noise，而這裡則選擇在 parameter space 上增加 noise，並且達到了很棒的效果。\",\"Tips\",\"ϵ-greedy 就像是獵人裡面的凱特，行動之前需要先看運氣抽接下來使用的武器，即便自己知道當下用哪一個 action 比較好，卻會受到 ϵ 的限制。\",\"而在 parameter space 加上 noise 就像是可以換個角度去想其他人會怎麼做，試著用那一個人的做法走過一次，得到不同的經驗。\",\"相較之下，action space 加 noise 就比較像是在亂試，反之在 parameter space 上加 noise 就比較有系統性一些。\",\"具體來說就是他們試圖在 parameter 上加上 Gaussian Noise。\",\"θ~=θ+N(0,σ2I)\",\"Action Space Noise\",\"Parameter Space Noise\",\"Videos from OpenAI - Better exploration with parameter noise\",\"Warning\",\"底下的內容只是單純的 Review\"]},\"76\":{\"h\":\"DQN\",\"t\":[\"透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"於是他們定義了底下的 Loss function 去試圖得到 Q∗。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"D 是上一個 replay buffer 的 transition distribution \",\"state s\",\"action a\",\"reward r=R(s,a)\",\"probability y∼P(⋅∣s,a)\",\"θ− 是被固定的參數\",\"Tips\",\"DQN 帶來了幾個好處\",\"Experience Replay 透過儲存 Experience，更新參數是從 replay buffer 中隨意挑一筆，降低了資料之間的相關性，讓 Neural Network 訓練避免偏差。\",\"Target Network 避免了訓練目標經常地變動造成訓練效果差\",\"使用 Neural Network 替代 action value function 避免了 Q-Learning 的 table 維度過大訓練困難的問題\"]},\"77\":{\"h\":\"Double-DQN\",\"t\":[\"在 DQN 當中我們需要同時訓練兩個 model，也就是 θ 與 θ−。然而 DQN 的設定上對於目標被發現存在高估的問題，因此 Double-DQN 提出了解決這個高估問題的方法。\",\"原始 DQN 目標\",\"Double-DQN 目標\",\"r+γmaxb∈A​Q(y,b;θ−)\",\"r+γQ(y,maxb∈A​Q(y,b;θ);θ−)\",\"高估的狀況如底下的綠線。紫線是目標函數，橘線是綠線與紫線的誤差，不難發現到確實都存在高估的狀況。\",\"然而使用了 Double-DQN 之後的誤差(藍線)就小到幾乎不存在了。\",\"Image from Hado van Hasselt, Arthur Guez, David Silver (2015)\",\"Tips\",\"Double-DQN 帶來的幾個好處\",\"讓 DQN 高估的問題消失，有更好的效果\"]},\"78\":{\"h\":\"Dueling DQN\",\"t\":[\"Dueling DQN 的概念仍然是透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"與 DQN 不同的地方在於他並不是直接去學習 Q∗，而是另外定義了一個 Advantage functionA。\",\"A(s,a)=Q(s,a)−V(s)\",\"V(s) 就像是 baseline，表示著在當前這個 state s 底下你預期可以拿到多好的 return，所以 A(s,a) 意義上就是在看每個 action 有多好多壞。\",\"透過 V 和 A 的總和就能夠得到 Q。上圖就是在最後分開成兩個輸出結果 V 和 A，最後合併成 Q。\",\"Q(s,a;θ)=V(s;θV​)+A(s,a;θA​)\",\"Info\",\"在實務上為了避免像是 V(s) 都是 0，實際上跟 DQN 沒有差異的問題，因此細節上是還會對 A(s,a) 加上總和為 0 的限制。\",\"Q(s,a,θ,α,β)=V(a,θ,α)+​A(s,a,θ,β)−∣A∣1​a′∈∣A∣∑​A(s,a′,θ,β)​\",\"α,β 只是調整 V 和 A 兩部分影響程度的參數。\",\"把 Dueling DQN 搭配 Double DQN 之後可以得到底下的 Loss。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"Tips\",\"Dueling DQN 帶來了幾個好處\",\"使用 Advantage function 增加模型的更新與 exploration。\"]},\"79\":{\"h\":\"A3C\",\"t\":[\"在 DQN 當中使用了 Experience Reply 去避免訓練資料上的強關聯性，然而存在幾個缺點\",\"需要額外的 memory 去儲存 replay buffer\",\"需要 off-policy alogorithm，對於 online RL 來說可能導致收斂不穩定以及緩慢等問題\",\"A3C 的概念就如同火影忍者的影分身之術，讓每個分身在各自的環境當中訓練，訓練成效也就翻倍。\",\"Image from Arthur Juliani@Medium\",\"A3C 是使用 advantage actor-critic 的方式，會直接去學 policy 以及 value function。因此在參數上也就包含了兩項 θπ​ 以及 θv​ 分別表示 policy 以及 value function 的參數。考慮在時間 t 往後看 k 步的更新，參照 A3C 的論文，兩個參數的 Loss 計算分別如下。\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"A：Advantage function 也就是 A3C 當中的 R−V\",\"H：Entropy function 根據 A3C 的論文，加上這一項能夠促使模型更好 exploration\",\"β：調整 A 和 H 的影響程度\",\"Qi​：在時間 i 時於 state st+i​ 執行 policy π 的 return\",\"Note\",\"紅色的部分也就是前面提及的 entropy regularization\",\"最後整體的 Loss 也就如下\",\"L(θ)=Lπ(θπ​)+λLV(θv​)\",\"λ 用來調整兩個 Loss 的影響力\",\"Info\",\"Note：原始 A3C 論文中更新一步，並且沒有使用 entropy 的算式\",\"θ′θv′​​:dθ←dθ+∇θ′​logπ(ai​∣si​;θ′)(R−V(si​;θv′​)):dθv​←dθv​+∂(R−V(si​;θv′​))2/∂θv′​​\",\"Tips\",\"A3C 帶來了幾個好處\",\"降低訓練資料之間的關聯性 畢竟每個 agent 訓練的環境都不同，得到的資料也就不同\",\"能夠使用 on-policy 或是 off-policy，增加通用性\",\"可以平行化加速訓練\",\"穩定地訓練\"]},\"80\":{\"h\":\"Methodology\"},\"81\":{\"h\":\"基本想法\",\"t\":[\"Noisy-Net 的想法跟 Parameter Space Noise for Exploration 的想法基本上是相同方向，都是要對 parameter space 去加上 noise。\",\"作法上，對於每個可訓練的參數拆解成 ζ=(μ,σ)，然後再透過 zero-mean 的 ϵ 增加 noise。也就是說對於一個參數 θ 我們會寫成：\",\"θ=defμ+σ⊙ϵ\",\"所以對於一個 Linear Layer 來說\",\"y=wx+b⇒y=(μw+σw⊙ϵw)x+(μb+σb⊙ϵb)\",\"就只是這樣而已，不要想太多！\",\"Tips\",\"刻意挑 zero-mean 的 noise 是為了採用底下的特性方便後續 Loss 的計算。\",\"Lˉ(ζ)=E[L(θ)]\",\"因此\",\"∇Lˉ(ζ)=∇E[L(θ)]=E[∇μ,Σ​L(μ+Σ⊙ϵ)]\",\"Σ 包含了所有 σ\",\"加上了 Monte-Carlo approximation 之後，可以用單一的 sample ξ 去近似\",\"∇Lˉ(ζ)≈∇μ,Σ​L(μ+Σ⊙ξ)\",\"跟 OpenAI 提出的方法略為不同的地方在於他並不是直接對 network 的參數加上 Gaussian Noise，而是給了參數 ϵw,ϵb 去決定要加怎樣的 noise。\",\"在每一個 episode 開始之前先把參數加上 noise，接下來這一整個 episode 就都是用這個 network 去訓練，意即在過程中不會對 noise 做調整。\"]},\"82\":{\"h\":\"減少產 random number 時間\",\"t\":[\"這樣的做法下每一個 episode 都需要 random noise 在 weight 和 bias 上。假如 w∈Rq×p,b∈Rq，那麼 ϵw∈Rq×p,ϵb∈Rq，也就意味著需要 random 出 pq+q 個數值。\",\"上面基本的做法作者稱他為 Independent Gaussian noise，而接下來作者給出一個 Factorised Gaussian noise 的做法。\",\"基本上就是將 random number 拆分\",\"ϵi,jw​ϵjb​​=f(ϵi​)f(ϵj​)=f(ϵj​)​\",\"其中 f(x)=sgn(x)∣x∣​,ϵi​∈Rq,ϵj​∈Rp。\",\"如此一來，只需要產出 p+q 個 random number 也可以達到類似的效果。\"]},\"83\":{\"h\":\"DQN & Dueling DQN\",\"t\":[\"由於 DQN 和 Dueling DQN 是在 single-thread 上訓練，因此上述的 Random Overhead 會比較大，在這裡採用 Factorised Gaussian noise。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 ϵ-greedy 了。\",\"原本的 DQN 對 Loss 的定義如下。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γb∈Amax​Q(y,b,ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\",\"最外層的期望值是對 ϵ 和 ϵ′\",\"同樣地也可以對 Dueling DQN 做修改。原本的定義為\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γQ(y,argb∈Amax​Q(y,b,ϵ′′;ζ),ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\"]},\"84\":{\"h\":\"Distributed A3C\",\"t\":[\"由於 A3C 是在 multi-thread 上訓練，因此不太需要考慮上述的 Random Overhead，在這裡採用 Independent Gaussian noise 即可。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 Entropy function 了。\",\"原本的 A3C\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"NoisyNet-A3C\",\"∇ζπ​​Lπ(ζπ​)LV(ζv​)​=−E[Eπ[i=0∑k​∇ζπ​​log(π(at+i​∣st+i​;ζπ​,ϵ))A(st+i​,at+i​;ζv​,ϵ)]]=E[i=0∑k​Eπ[(Qi​−V(st+i​;ζv​,ϵ))∣st+i​]]2​\",\"Noise initialize details\",\"Independent Gaussian noise\",\"μi,j​∼U[−p3​​,+p3​​]\",\"p 是 input 的數量\",\"σi,j​=0.017\",\"Factorised Gaussian noise\",\"μi,j​∼U[−p1​​,+p1​​]\",\"p 是 input 的數量\",\"σi,j​=p​0.5​\"]},\"85\":{\"h\":\"Results\"},\"86\":{\"h\":\"Experiments\",\"t\":[\"實驗是做在 57 Atari games 上。每 1M 個 frames 評估一次，episode 每 108K frames 會 truncate 一次。將沒有做任何修正的 DQN、Dueling DQN、A3C 作為 Baseline。\",\"首先把 Baseline 以及加上 NoisyNet 的模型都跟 Human 比較，底下是用來評估優劣的評分方式。\",\"100×ScoreHuman​−ScoreRandom​Scoreagent​−ScoreRandom​​\",\"Note\",\"最後得出的結果為 0：跟 Random 一樣糟 最後得出的結果為 100：跟 Human 一樣好\",\"可以從分數上明顯看出來加上了 NoisyNet 後對於 Mean 以及 Median 都有正面的影響。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"接下來評估加上 NoisyNet 帶來的影響力，評分方式會也跟 Baseline 比較。\",\"100×max(ScoreHuman​,ScoreBaseline​)−ScoreRandom​Scoreagent​−ScoreBaseline​​\",\"可以看到在大多數的遊戲加上了 NoisyNet 之後的結果都有些進步。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"不過進步主要在 DQN 以及 Dueling 上較為顯著。A3C 的部分在退步也是有幾項退步蠻多，也並不是每次加上 NoisyNet 都會帶來 improvement。\",\"從訓練中的曲線也可以明顯看到 NoisyNet 可以帶來很不錯的 improvement。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"87\":{\"h\":\"Analysis\",\"t\":[\"為了進一步去釐清這樣的做法為什麼是可行、合理的，作者進一步去研究。\",\"回顧一下我們加上 Noise 的方法，是把一個可訓練參數拆成 ζ=(μ,σ)，再額外多一個 Noise ϵ。\",\"θ=defμ+σ⊙ϵ\",\"理想上，我們最後的 Loss 應該要能夠好好收斂，也就是說最後的 solution 應該要是 deterministic。那麼這裡加上的 ϵ 就應該隨著訓練慢慢被忽視，作用只在於訓練的前中期提供 exploration。因此，我們也就會期待 σ 這個參數會漸漸趨近於 0 了！\",\"定義底下的平均\",\"Σˉ=Nweights​1​i∑​∣σiw​∣\",\"作者發現在每一個遊戲當中最後一個 Layer 的 Σˉ 都是會逐漸趨近於 0 的，然而若觀察倒數第二個 Layer 卻並不一定了，有些甚至是遞增的。也就是說，其實 NoisyNet 並不會都得出 deterministic solution。\",\"此外，透過 Σˉ 的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同，也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"88\":{\"h\":\"Contribution\",\"t\":[\"提供一個簡單又有效的 Exploration 方式\",\"能夠在 on-policy 以及 off-policy 上適用\",\"能夠輕易地套用在所有的 RL 算法當中\"]},\"89\":{\"h\":\"值得一看的文章們\",\"t\":[\"强化学习中on-policy 与off-policy有什么区别？\",\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\",\"Asynchronous Methods for Deep Reinforcement Learning\",\"Deep Exploration via Randomized Value Functions\",\"Kalman Temporal Differences\",\"VIME: Variational Information Maximizing Exploration\",\"Parameter Space Noise for Exploration\",\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"Better exploration with parameter noise\",\"强化学习中的探索与利用（count-based)\"]},\"90\":{\"c\":[\"Note\"]},\"91\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"]},\"92\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"强化学习中的探索与利用\",{\"1\":{\"89\":1}}],[\"强化学习中on\",{\"1\":{\"89\":1}}],[\"与off\",{\"1\":{\"89\":1}}],[\"方式\",{\"1\":{\"88\":1}}],[\"方法\",{\"1\":{\"38\":1}}],[\"方法也是屬於\",{\"1\":{\"38\":1}}],[\"理想上\",{\"1\":{\"87\":1}}],[\"再額外多一個\",{\"1\":{\"87\":1}}],[\"再次說明了\",{\"1\":{\"25\":1}}],[\"回顧一下我們加上\",{\"1\":{\"87\":1}}],[\"合理的\",{\"1\":{\"87\":1}}],[\"合成出\",{\"1\":{\"38\":2}}],[\"評分方式會也跟\",{\"1\":{\"86\":1}}],[\"評估一次\",{\"1\":{\"86\":1}}],[\"評估方式\",{\"0\":{\"66\":1}}],[\"現在替換成\",{\"1\":{\"83\":2}}],[\"現在\",{\"1\":{\"83\":1,\"84\":1}}],[\"拆分\",{\"1\":{\"82\":1}}],[\"拆開來解決\",{\"1\":{\"22\":1}}],[\"拆開來\",{\"1\":{\"12\":1}}],[\"出\",{\"1\":{\"82\":1}}],[\"出來的\",{\"1\":{\"12\":1}}],[\"假如\",{\"1\":{\"82\":1}}],[\"減少產\",{\"0\":{\"82\":1}}],[\"意即在過程中不會對\",{\"1\":{\"81\":1}}],[\"意義上就是在看每個\",{\"1\":{\"78\":1}}],[\"≈∇μ\",{\"1\":{\"81\":1}}],[\"ξ\",{\"1\":{\"81\":1}}],[\"∇ζπ​​lπ\",{\"1\":{\"84\":1}}],[\"∇μ\",{\"1\":{\"81\":1}}],[\"∇lˉ\",{\"1\":{\"81\":2}}],[\"∇θπ​​lπ\",{\"1\":{\"79\":1,\"84\":1}}],[\"ζv​\",{\"1\":{\"84\":3}}],[\"ζπ​\",{\"1\":{\"84\":2}}],[\"ζ−\",{\"1\":{\"83\":2}}],[\"ζ\",{\"1\":{\"81\":3,\"83\":5}}],[\"ζ=\",{\"1\":{\"81\":1,\"87\":1}}],[\"刻意挑\",{\"1\":{\"81\":1}}],[\"σˉ\",{\"1\":{\"87\":2}}],[\"σˉ=nweights​1​i∑​∣σiw​∣\",{\"1\":{\"87\":1}}],[\"σi\",{\"1\":{\"84\":2}}],[\"σ​l\",{\"1\":{\"81\":2}}],[\"σ\",{\"1\":{\"81\":3,\"87\":2}}],[\"σ2i\",{\"1\":{\"75\":1}}],[\"穩定地訓練\",{\"1\":{\"79\":1}}],[\"∂θv\",{\"1\":{\"79\":1}}],[\"論文中更新一步\",{\"1\":{\"79\":1}}],[\"論文中有提及在\",{\"1\":{\"73\":1}}],[\"紅色的部分也就是前面提及的\",{\"1\":{\"79\":1}}],[\"執行\",{\"1\":{\"79\":1}}],[\"根據\",{\"1\":{\"79\":1}}],[\"參照\",{\"1\":{\"79\":1}}],[\"步的更新\",{\"1\":{\"79\":1}}],[\"往後看\",{\"1\":{\"79\":1}}],[\"往往假設資料的分布會維持住\",{\"1\":{\"57\":1}}],[\"往往在邊界上會有許多誤判的\",{\"1\":{\"40\":1}}],[\"往往先從貼標籤開始\",{\"1\":{\"37\":1}}],[\"往往就會\",{\"1\":{\"34\":1}}],[\"往往會有許多我們沒有的\",{\"1\":{\"33\":1}}],[\"∈∣a∣∑​a\",{\"1\":{\"78\":1}}],[\"α\",{\"1\":{\"78\":3}}],[\"αt​\",{\"1\":{\"6\":1}}],[\"沒有差異的問題\",{\"1\":{\"78\":1}}],[\"底下是用來評估優劣的評分方式\",{\"1\":{\"86\":1}}],[\"底下你預期可以拿到多好的\",{\"1\":{\"78\":1}}],[\"底下的內容只是單純的\",{\"1\":{\"75\":1}}],[\"藍線\",{\"1\":{\"77\":1}}],[\"橘線是綠線與紫線的誤差\",{\"1\":{\"77\":1}}],[\"紫線是目標函數\",{\"1\":{\"77\":1}}],[\"紫色圓點表示\",{\"1\":{\"22\":1}}],[\"原本的定義為\",{\"1\":{\"83\":1}}],[\"原本的\",{\"1\":{\"83\":1,\"84\":1}}],[\"原始\",{\"1\":{\"77\":1,\"79\":1}}],[\"原先\",{\"1\":{\"17\":1}}],[\"維度過大訓練困難的問題\",{\"1\":{\"76\":1}}],[\"替代\",{\"1\":{\"76\":1}}],[\"避免了\",{\"1\":{\"76\":1}}],[\"避免了訓練目標經常地變動造成訓練效果差\",{\"1\":{\"76\":1}}],[\"避免太大或是太小\",{\"1\":{\"6\":1}}],[\"帶來的影響力\",{\"1\":{\"86\":1}}],[\"帶來的幾個好處\",{\"1\":{\"77\":1}}],[\"帶來的效益\",{\"1\":{\"25\":1}}],[\"帶來了幾個好處\",{\"1\":{\"76\":1,\"78\":1,\"79\":1}}],[\"∼d​\",{\"1\":{\"76\":1,\"78\":1,\"83\":4}}],[\"∼ε​\",{\"1\":{\"59\":1}}],[\"加\",{\"1\":{\"75\":1}}],[\"加上這一項能夠促使模型更好\",{\"1\":{\"79\":1}}],[\"加上總和為\",{\"1\":{\"78\":1}}],[\"加上了\",{\"1\":{\"22\":1,\"81\":1}}],[\"加上了一個\",{\"1\":{\"15\":1}}],[\"加上\",{\"1\":{\"22\":1,\"75\":1}}],[\"加上不同的偏好\",{\"1\":{\"13\":1}}],[\"試著用那一個人的做法走過一次\",{\"1\":{\"75\":1}}],[\"核心的概念很簡單\",{\"1\":{\"75\":1}}],[\"達到鼓勵\",{\"1\":{\"75\":1}}],[\"發表在\",{\"1\":{\"75\":1}}],[\"發散\",{\"1\":{\"61\":1}}],[\"年由\",{\"1\":{\"75\":1}}],[\"架構\",{\"1\":{\"74\":1}}],[\"效率與品質的問題\",{\"1\":{\"73\":1}}],[\"等\",{\"1\":{\"73\":1}}],[\"等不同的做法\",{\"1\":{\"36\":1}}],[\"證明了\",{\"1\":{\"67\":1}}],[\"事先處理好了\",{\"1\":{\"67\":1}}],[\"事先標記好\",{\"1\":{\"67\":1}}],[\"類似\",{\"1\":{\"67\":1}}],[\"人工提取\",{\"1\":{\"67\":1}}],[\"人類\",{\"1\":{\"60\":1}}],[\"人類的\",{\"1\":{\"3\":2}}],[\"獲得的\",{\"1\":{\"66\":1}}],[\"即可\",{\"1\":{\"66\":1,\"84\":1}}],[\"即便自己知道當下用哪一個\",{\"1\":{\"75\":1}}],[\"即便在參數比較異常的狀況下仍然能有很不錯的學習成果\",{\"1\":{\"24\":1}}],[\"即便\",{\"1\":{\"24\":1,\"36\":1}}],[\"即便這個環境設定是相當簡單的\",{\"1\":{\"22\":1}}],[\"至於那些被忽略的\",{\"1\":{\"65\":1}}],[\"至今平均的\",{\"1\":{\"14\":1}}],[\"至今被嘗試的次數\",{\"1\":{\"14\":1}}],[\"才會去擷取畫面\",{\"1\":{\"65\":1}}],[\"順利避免了最初提及的兩個問題\",{\"1\":{\"63\":1}}],[\"ϕ\",{\"1\":{\"63\":1}}],[\"筆\",{\"1\":{\"63\":1}}],[\"包含了所有\",{\"1\":{\"81\":1}}],[\"包含了\",{\"1\":{\"63\":1}}],[\"包含了幾個重要的部分\",{\"1\":{\"5\":1}}],[\"記錄在\",{\"1\":{\"63\":1}}],[\"稍微修改了這個做法\",{\"1\":{\"63\":1}}],[\"稍微翻了一下\",{\"1\":{\"3\":1}}],[\"訓練的環境都不同\",{\"1\":{\"79\":1}}],[\"訓練的過程包含了一點專家系統的概念\",{\"1\":{\"67\":2}}],[\"訓練成效也就翻倍\",{\"1\":{\"79\":1}}],[\"訓練避免偏差\",{\"1\":{\"76\":1}}],[\"訓練\",{\"1\":{\"62\":1}}],[\"固定為\",{\"1\":{\"65\":1}}],[\"固定\",{\"1\":{\"61\":1}}],[\"舒緩\",{\"1\":{\"61\":1}}],[\"後對於\",{\"1\":{\"86\":1}}],[\"後續的研究中則發現到\",{\"1\":{\"61\":1}}],[\"後可以得到更好的成效\",{\"1\":{\"24\":1}}],[\"成功結合\",{\"1\":{\"68\":1}}],[\"成功在雙陸棋上面\",{\"1\":{\"60\":1}}],[\"成新的\",{\"1\":{\"40\":1}}],[\"算法當中\",{\"1\":{\"88\":1}}],[\"算法\",{\"1\":{\"60\":1}}],[\"他要去近似的是\",{\"1\":{\"59\":1}}],[\"他們會先透過\",{\"1\":{\"62\":1}}],[\"他們也試著用相同的手段訓練模型\",{\"1\":{\"50\":1}}],[\"他們選擇用\",{\"1\":{\"50\":1}}],[\"他們選擇其中\",{\"1\":{\"48\":1}}],[\"他們認為在其他的\",{\"1\":{\"50\":1}}],[\"他們並不是採用\",{\"1\":{\"21\":1}}],[\"他們將\",{\"1\":{\"6\":1}}],[\"既然\",{\"1\":{\"59\":1}}],[\"既然有\",{\"1\":{\"8\":1}}],[\"總和最大化\",{\"1\":{\"59\":1}}],[\"總之\",{\"1\":{\"19\":1}}],[\"整個問題\",{\"1\":{\"59\":1}}],[\"收斂性相關研究\",{\"0\":{\"61\":1},\"1\":{\"58\":1}}],[\"應該要是\",{\"1\":{\"87\":1}}],[\"應該要能夠好好收斂\",{\"1\":{\"87\":1}}],[\"應該要慢慢變大\",{\"1\":{\"66\":1}}],[\"應該要給\",{\"1\":{\"3\":1}}],[\"應用在\",{\"1\":{\"57\":1}}],[\"好\",{\"1\":{\"57\":1}}],[\"能不能把\",{\"1\":{\"57\":1}}],[\"能夠輕易地套用在所有的\",{\"1\":{\"88\":1}}],[\"能夠使用\",{\"1\":{\"79\":1}}],[\"能夠使模型更好去學習各自的\",{\"1\":{\"12\":1}}],[\"能夠在\",{\"1\":{\"88\":1}}],[\"能夠在所有的\",{\"1\":{\"25\":1}}],[\"能夠在上述\",{\"1\":{\"5\":1}}],[\"能夠上\",{\"1\":{\"22\":1}}],[\"能夠調整自己的\",{\"1\":{\"16\":1}}],[\"能夠盡可能去正確探索環境\",{\"1\":{\"3\":1}}],[\"已經能夠在這種資料上去擷取特徵\",{\"1\":{\"57\":1}}],[\"已知\",{\"1\":{\"14\":1}}],[\"語音資料等\",{\"1\":{\"57\":1}}],[\"視覺影像\",{\"1\":{\"57\":1}}],[\"領域當中把一些\",{\"1\":{\"57\":1}}],[\"物件偵測的領域自適應\",{\"1\":{\"52\":1}}],[\"挑選\",{\"1\":{\"50\":1}}],[\"針對\",{\"1\":{\"50\":1}}],[\"針對不同的\",{\"1\":{\"7\":1}}],[\"判斷要不要\",{\"1\":{\"50\":1}}],[\"頗偏\",{\"1\":{\"49\":1}}],[\"甚至對\",{\"1\":{\"49\":1}}],[\"考慮在時間\",{\"1\":{\"79\":1}}],[\"考慮到\",{\"1\":{\"49\":1}}],[\"考慮有限的\",{\"1\":{\"8\":1}}],[\"真的很糟\",{\"1\":{\"48\":1}}],[\"除了\",{\"1\":{\"48\":1,\"49\":1,\"65\":1}}],[\"除了展現驚人的成果以外\",{\"1\":{\"25\":1}}],[\"單純用\",{\"1\":{\"48\":1,\"49\":1}}],[\"單純的\",{\"1\":{\"15\":1,\"40\":1}}],[\"建構的\",{\"1\":{\"47\":1}}],[\"照片是在\",{\"1\":{\"46\":1,\"47\":1}}],[\"照片是在城市當中開車拍下的各種照片\",{\"1\":{\"45\":1}}],[\"照著這樣的想法\",{\"1\":{\"13\":1}}],[\"則是現實世界當中的影像\",{\"1\":{\"44\":1}}],[\">\",{\"0\":{\"48\":1,\"49\":1},\"1\":{\"44\":2}}],[\"作用只在於訓練的前中期提供\",{\"1\":{\"87\":1}}],[\"作法上\",{\"1\":{\"81\":1}}],[\"作為\",{\"1\":{\"43\":1,\"57\":1,\"86\":1}}],[\"作者發現在每一個遊戲當中最後一個\",{\"1\":{\"87\":1}}],[\"作者進一步去研究\",{\"1\":{\"87\":1}}],[\"作者推測是因為權重即便只有小的變化也會對\",{\"1\":{\"66\":1}}],[\"作者認為在這兩者都有一個共通點\",{\"1\":{\"23\":1}}],[\"作者將\",{\"1\":{\"23\":1,\"24\":1}}],[\"作者也觀察了在幾款遊戲訓練過程中當中\",{\"1\":{\"24\":1}}],[\"作者也發現到\",{\"1\":{\"22\":1}}],[\"作者也發現如果把\",{\"1\":{\"22\":1}}],[\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子\",{\"1\":{\"12\":1}}],[\"作者比較兩個模型在不同\",{\"1\":{\"22\":1}}],[\"作者建構一個簡單的\",{\"1\":{\"22\":1}}],[\"作者在實驗當中發現如果採用\",{\"1\":{\"17\":1}}],[\"部分的影響程度\",{\"1\":{\"41\":1}}],[\"調整\",{\"1\":{\"41\":1,\"79\":1}}],[\"λ\",{\"1\":{\"41\":1,\"79\":1}}],[\"得到的資料也就不同\",{\"1\":{\"79\":1}}],[\"得到的最大\",{\"1\":{\"59\":1}}],[\"得到不同的經驗\",{\"1\":{\"75\":1}}],[\"得到\",{\"1\":{\"41\":1}}],[\"得以有不同程度的影響\",{\"1\":{\"27\":1}}],[\"經過\",{\"1\":{\"41\":1}}],[\"經過相同的\",{\"1\":{\"36\":1}}],[\"取出圖片\",{\"1\":{\"41\":1}}],[\"取出圖片與\",{\"1\":{\"41\":1}}],[\"取得的\",{\"1\":{\"59\":1}}],[\"取得\",{\"1\":{\"19\":1,\"41\":1}}],[\"取得不同的\",{\"1\":{\"9\":1}}],[\"詳細的步驟具體來說\",{\"1\":{\"41\":1}}],[\"覆蓋\",{\"1\":{\"40\":1}}],[\"許多的\",{\"1\":{\"40\":1}}],[\"被預測成\",{\"1\":{\"40\":2}}],[\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清\",{\"1\":{\"38\":1}}],[\"先轉成\",{\"1\":{\"38\":1}}],[\"先在\",{\"1\":{\"37\":1}}],[\"像是\",{\"1\":{\"38\":1,\"40\":1,\"48\":1}}],[\"像是直接有一台車會去蒐集真實街景資料\",{\"1\":{\"33\":1}}],[\"混合成新的圖片\",{\"1\":{\"38\":1}}],[\"產生一個新的\",{\"1\":{\"38\":1}}],[\"產生出選擇\",{\"1\":{\"13\":1}}],[\"拿出來做一些\",{\"1\":{\"63\":1}}],[\"拿出兩張\",{\"1\":{\"38\":1}}],[\"拿去訓練\",{\"1\":{\"40\":1,\"41\":1}}],[\"拿來比較加上\",{\"1\":{\"24\":1}}],[\"看\",{\"1\":{\"37\":1}}],[\"缺乏\",{\"1\":{\"37\":1}}],[\"換句話說\",{\"1\":{\"37\":1}}],[\"畢竟每個\",{\"1\":{\"79\":1}}],[\"畢竟在\",{\"1\":{\"50\":1}}],[\"畢竟\",{\"1\":{\"37\":1}}],[\"了\",{\"1\":{\"37\":1,\"59\":1,\"83\":1,\"84\":1,\"87\":1}}],[\"舉例而言\",{\"1\":{\"75\":1}}],[\"舉例來說\",{\"1\":{\"37\":1}}],[\"舉一個在\",{\"1\":{\"33\":1}}],[\"舉一個例子來說\",{\"1\":{\"13\":1}}],[\"半監督式學習\",{\"1\":{\"37\":1,\"52\":2}}],[\"天空之類的就通常會像是在半空中\",{\"1\":{\"36\":1}}],[\"天生就有一些自己的偏好\",{\"1\":{\"13\":1}}],[\"號誌\",{\"1\":{\"36\":1}}],[\"行動之前需要先看運氣抽接下來使用的武器\",{\"1\":{\"75\":1}}],[\"行人都還是會跟地板黏在一起\",{\"1\":{\"36\":1}}],[\"行為模式改變\",{\"1\":{\"15\":1}}],[\"汽車\",{\"1\":{\"36\":1}}],[\"同樣地也可以對\",{\"1\":{\"83\":1}}],[\"同樣以自駕車的例子來說\",{\"1\":{\"36\":1}}],[\"同時也能在\",{\"1\":{\"25\":1}}],[\"同時也可以看到\",{\"1\":{\"25\":1}}],[\"通常並不會差太多\",{\"1\":{\"36\":1}}],[\"依照\",{\"1\":{\"36\":1}}],[\"依照過去\",{\"1\":{\"6\":1}}],[\"做修改\",{\"1\":{\"83\":1}}],[\"做調整\",{\"1\":{\"81\":1}}],[\"做得很棒不能直接表達在整體會表達很棒\",{\"1\":{\"50\":1}}],[\"做出一個\",{\"1\":{\"38\":1}}],[\"做\",{\"1\":{\"37\":1}}],[\"做對抗式學習\",{\"1\":{\"36\":1}}],[\"做了兩個小修正\",{\"1\":{\"16\":1}}],[\"還是存在差異的\",{\"1\":{\"37\":1}}],[\"還是\",{\"1\":{\"36\":1}}],[\"還要強\",{\"1\":{\"24\":1}}],[\"結合\",{\"1\":{\"68\":1}}],[\"結合起來\",{\"1\":{\"63\":1}}],[\"結果相當糟糕\",{\"1\":{\"34\":1}}],[\"結束\",{\"1\":{\"19\":1}}],[\"直接從\",{\"1\":{\"68\":1}}],[\"直接把訓練在虛擬環境的模型應用在真實環境\",{\"1\":{\"34\":1}}],[\"直到\",{\"1\":{\"19\":1}}],[\"擅自用自己的思維解讀\",{\"1\":{\"34\":1}}],[\"轉變到\",{\"1\":{\"34\":1}}],[\"影像分割\",{\"1\":{\"34\":1}}],[\"近年來透過\",{\"1\":{\"34\":1}}],[\"近似\",{\"1\":{\"12\":2}}],[\"讓選擇更多樣\",{\"1\":{\"75\":1}}],[\"讓\",{\"1\":{\"65\":1,\"76\":1,\"77\":1}}],[\"讓我們得以用較低的成本在虛擬環境中訓練模型\",{\"1\":{\"33\":1}}],[\"讓每個分身在各自的環境當中訓練\",{\"1\":{\"79\":1}}],[\"讓每個\",{\"1\":{\"13\":1}}],[\"降低訓練資料之間的關聯性\",{\"1\":{\"79\":1}}],[\"降低了資料之間的相關性\",{\"1\":{\"76\":1}}],[\"降低\",{\"1\":{\"33\":1,\"41\":1}}],[\"想解決的就是盡可能地將\",{\"1\":{\"33\":1}}],[\"到\",{\"1\":{\"33\":1}}],[\"環境與虛擬世界有差距\",{\"1\":{\"33\":1}}],[\"環境中取得\",{\"1\":{\"19\":1}}],[\"虛擬世界\",{\"1\":{\"33\":1}}],[\"很多時候我們並不會直接去蒐集真實的資料\",{\"1\":{\"33\":1}}],[\"投射到同一個平面上\",{\"1\":{\"33\":1}}],[\"所有的算法\",{\"1\":{\"67\":1}}],[\"所以對於一個\",{\"1\":{\"81\":1}}],[\"所以設定\",{\"1\":{\"65\":1}}],[\"所以\",{\"1\":{\"57\":1,\"78\":1}}],[\"所以他們認為這樣不太公平\",{\"1\":{\"50\":1}}],[\"所以在數據上\",{\"1\":{\"49\":1}}],[\"所謂的半監督式學習也就是說\",{\"1\":{\"37\":1}}],[\"所謂的\",{\"1\":{\"33\":1,\"37\":1}}],[\"所在的位置\",{\"1\":{\"22\":1}}],[\"共同發表\",{\"1\":{\"32\":1}}],[\"查爾摩斯理工大學\",{\"1\":{\"32\":1}}],[\"<>\",{\"1\":{\"28\":1}}],[\"值得一看的文章們\",{\"0\":{\"28\":1,\"52\":1,\"89\":1}}],[\"值得一提的是\",{\"1\":{\"22\":1}}],[\"第一個能夠在所有\",{\"1\":{\"27\":1}}],[\"使得所有\",{\"1\":{\"65\":1}}],[\"使得同類型的資料會相近\",{\"1\":{\"33\":1}}],[\"使模型具有更好的普遍性\",{\"1\":{\"27\":1}}],[\"使用線性的\",{\"1\":{\"61\":1}}],[\"使用非線性的\",{\"1\":{\"61\":1}}],[\"使用\",{\"1\":{\"9\":1,\"49\":1,\"76\":1,\"78\":1}}],[\"使用不同的\",{\"1\":{\"9\":1}}],[\"自適應不同環境\",{\"1\":{\"27\":1}}],[\"自己調整\",{\"1\":{\"24\":1}}],[\"解決過去\",{\"1\":{\"68\":1}}],[\"解決訓練不穩定\",{\"1\":{\"27\":1}}],[\"解決了前面提及的第三個問題\",{\"1\":{\"59\":1}}],[\"解決了\",{\"1\":{\"12\":1,\"16\":1,\"17\":1,\"25\":1}}],[\"解決了一些\",{\"1\":{\"10\":1}}],[\"顯示了\",{\"1\":{\"25\":1}}],[\"卻並不一定了\",{\"1\":{\"87\":1}}],[\"卻會受到\",{\"1\":{\"75\":1}}],[\"卻是最差的\",{\"1\":{\"25\":1}}],[\"卻在一些簡單的問題做得很差\",{\"1\":{\"10\":1}}],[\"雖然已經有\",{\"1\":{\"37\":1}}],[\"雖然在\",{\"1\":{\"25\":1}}],[\"雖然兩個模型都會把\",{\"1\":{\"12\":1}}],[\"適應不同的環境\",{\"1\":{\"24\":1}}],[\"提供一個簡單又有效的\",{\"1\":{\"88\":1}}],[\"提供了更大的普遍性\",{\"1\":{\"24\":1}}],[\"提出的方法略為不同的地方在於他並不是直接對\",{\"1\":{\"81\":1}}],[\"提出了解決這個高估問題的方法\",{\"1\":{\"77\":1}}],[\"提出如\",{\"1\":{\"37\":1}}],[\"提出\",{\"1\":{\"27\":1}}],[\"提出透過\",{\"1\":{\"27\":1}}],[\"提升\",{\"1\":{\"22\":1}}],[\"7\",{\"1\":{\"50\":2,\"65\":1}}],[\"79\",{\"1\":{\"24\":1}}],[\"76\",{\"1\":{\"24\":1,\"50\":1}}],[\"70\",{\"1\":{\"24\":2}}],[\"68\",{\"1\":{\"50\":1}}],[\"6\",{\"1\":{\"24\":1}}],[\"69571\",{\"1\":{\"24\":1}}],[\"6463\",{\"1\":{\"24\":1}}],[\"601\",{\"1\":{\"24\":1}}],[\"60\",{\"1\":{\"24\":2}}],[\"49\",{\"1\":{\"50\":1}}],[\"4\",{\"1\":{\"37\":1,\"63\":1}}],[\"4336\",{\"1\":{\"24\":1}}],[\"4596\",{\"1\":{\"24\":1}}],[\"4753\",{\"1\":{\"24\":1}}],[\"400\",{\"1\":{\"19\":1}}],[\"40\",{\"1\":{\"17\":1}}],[\"8\",{\"1\":{\"67\":1,\"89\":1}}],[\"83\",{\"1\":{\"24\":1,\"50\":1}}],[\"89\",{\"1\":{\"24\":2}}],[\"84\",{\"1\":{\"24\":1,\"50\":1}}],[\"80\",{\"1\":{\"17\":2}}],[\"35\",{\"1\":{\"50\":1}}],[\"31\",{\"1\":{\"24\":1}}],[\"30\",{\"1\":{\"24\":2}}],[\"32\",{\"1\":{\"24\":1}}],[\"38\",{\"1\":{\"24\":1}}],[\"349971\",{\"1\":{\"24\":1}}],[\"±\",{\"1\":{\"24\":10}}],[\"98\",{\"1\":{\"50\":1}}],[\"9400\",{\"1\":{\"47\":1}}],[\"90\",{\"1\":{\"24\":1}}],[\"96\",{\"1\":{\"24\":1}}],[\"9999\",{\"1\":{\"21\":1,\"24\":1}}],[\"99\",{\"1\":{\"21\":1,\"24\":1}}],[\"款比較困難的遊戲當中有些甚至是能夠比\",{\"1\":{\"24\":1}}],[\"異常地大\",{\"1\":{\"24\":1}}],[\"圖片中以\",{\"1\":{\"24\":1}}],[\"尤其是從\",{\"1\":{\"34\":1}}],[\"尤其在\",{\"1\":{\"23\":1,\"37\":1}}],[\"尤其當\",{\"1\":{\"10\":1}}],[\"反之在\",{\"1\":{\"75\":1}}],[\"反之會走出最短路\",{\"1\":{\"22\":1}}],[\"反之則相遠\",{\"1\":{\"33\":1}}],[\"反之則是\",{\"1\":{\"22\":1}}],[\"反之則會去走那些比較熟悉的\",{\"1\":{\"6\":1}}],[\"可對應到\",{\"1\":{\"46\":1}}],[\"可見\",{\"1\":{\"22\":1}}],[\"可以帶來很不錯的\",{\"1\":{\"86\":1}}],[\"可以看到在大多數的遊戲加上了\",{\"1\":{\"86\":1}}],[\"可以看到比較明顯的結果\",{\"1\":{\"23\":1}}],[\"可以從分數上明顯看出來加上了\",{\"1\":{\"86\":1}}],[\"可以用單一的\",{\"1\":{\"81\":1}}],[\"可以平行化加速訓練\",{\"1\":{\"79\":1}}],[\"可以單純透過觀察模型在\",{\"1\":{\"66\":1}}],[\"可以直接把資料之間的關聯建構起來\",{\"1\":{\"57\":1}}],[\"可以分成\",{\"1\":{\"36\":1}}],[\"可以發現\",{\"1\":{\"48\":1,\"49\":1}}],[\"可以發現單純用\",{\"1\":{\"34\":1}}],[\"可以發現到\",{\"1\":{\"25\":1}}],[\"可以發現到不同的遊戲會有不同的偏好\",{\"1\":{\"24\":1}}],[\"可以發現到加上\",{\"1\":{\"24\":1}}],[\"可以認為\",{\"1\":{\"24\":1}}],[\"可以移除\",{\"1\":{\"16\":1}}],[\"可以選擇\",{\"1\":{\"14\":1}}],[\"可以拿到多少\",{\"1\":{\"8\":1}}],[\"可以幫助我們去評估如果我們\",{\"1\":{\"8\":1}}],[\"移除\",{\"1\":{\"22\":1}}],[\"確實能夠帶來相當好的效益\",{\"1\":{\"22\":1}}],[\"確實它會傾向讓\",{\"1\":{\"14\":1}}],[\"增加通用性\",{\"1\":{\"79\":1}}],[\"增加模型的更新與\",{\"1\":{\"78\":1}}],[\"增加\",{\"1\":{\"22\":1,\"73\":1,\"81\":1}}],[\"較不具有彈性\",{\"1\":{\"22\":1}}],[\"進而去完成許多複雜的任務\",{\"1\":{\"57\":1}}],[\"進而解決這個問題\",{\"1\":{\"38\":1}}],[\"進而影響到結果\",{\"1\":{\"22\":1}}],[\"進而使得整體訓練採用的\",{\"1\":{\"13\":1}}],[\"導致接下來會經過的\",{\"1\":{\"66\":1}}],[\"導致卡車時常被預測成汽車\",{\"1\":{\"37\":1}}],[\"導致預測失準\",{\"1\":{\"34\":1}}],[\"導致\",{\"1\":{\"22\":1}}],[\"與\",{\"1\":{\"22\":1,\"25\":1,\"32\":1,\"57\":1,\"62\":1,\"77\":1,\"78\":1}}],[\"由此可見\",{\"1\":{\"22\":1}}],[\"由於\",{\"1\":{\"22\":2,\"83\":1,\"84\":1}}],[\"越大變得越小\",{\"1\":{\"22\":1}}],[\"綠色圓點表示\",{\"1\":{\"22\":1}}],[\"注意並不是\",{\"1\":{\"22\":1}}],[\"軸表示\",{\"1\":{\"22\":2}}],[\"各自的\",{\"1\":{\"34\":1}}],[\"各自最傾向\",{\"1\":{\"22\":1}}],[\"各有一個\",{\"1\":{\"13\":1}}],[\"接著作者比較\",{\"1\":{\"22\":1}}],[\"接下來評估加上\",{\"1\":{\"86\":1}}],[\"接下來這一整個\",{\"1\":{\"81\":1}}],[\"接下來依照你的需求不同\",{\"1\":{\"13\":1}}],[\"接下來用\",{\"1\":{\"7\":1}}],[\"走到\",{\"1\":{\"22\":1}}],[\"最外層的期望值是對\",{\"1\":{\"83\":1}}],[\"最相近的一個研究\",{\"1\":{\"62\":1}}],[\"最小化\",{\"1\":{\"59\":1}}],[\"最大化\",{\"1\":{\"59\":1}}],[\"最好的\",{\"1\":{\"48\":1}}],[\"最\",{\"1\":{\"40\":1}}],[\"最初被用於把\",{\"1\":{\"38\":1}}],[\"最初是為了解決\",{\"1\":{\"37\":1}}],[\"最終在所有的\",{\"1\":{\"25\":1}}],[\"最終\",{\"1\":{\"22\":1}}],[\"最傾向\",{\"1\":{\"22\":2}}],[\"最多\",{\"1\":{\"22\":1}}],[\"最後得出的結果為\",{\"1\":{\"86\":2}}],[\"最後得出來的結果\",{\"1\":{\"67\":1}}],[\"最後整體的\",{\"1\":{\"79\":1}}],[\"最後合併成\",{\"1\":{\"78\":1}}],[\"最後將\",{\"1\":{\"67\":1}}],[\"最後比較\",{\"1\":{\"25\":1}}],[\"最後是針對\",{\"1\":{\"24\":1}}],[\"最後\",{\"1\":{\"16\":1,\"24\":1,\"25\":1}}],[\"最後你一樣可以透過這些\",{\"1\":{\"13\":1}}],[\"最後只需要設定\",{\"1\":{\"9\":1}}],[\"最後分數越多越好\",{\"1\":{\"3\":1}}],[\"右移動\",{\"1\":{\"22\":1}}],[\"右邊是\",{\"1\":{\"7\":1}}],[\"左\",{\"1\":{\"22\":1}}],[\"左邊是\",{\"1\":{\"7\":1}}],[\"下拍攝\",{\"1\":{\"47\":1}}],[\"下拍攝的\",{\"1\":{\"46\":1}}],[\"下\",{\"1\":{\"22\":1,\"52\":1}}],[\"隨機地放在地圖上的任意格子\",{\"1\":{\"22\":1}}],[\"我們也就會期待\",{\"1\":{\"87\":1}}],[\"我們最後的\",{\"1\":{\"87\":1}}],[\"我們會寫成\",{\"1\":{\"81\":1}}],[\"我們是從\",{\"1\":{\"63\":1}}],[\"我們對於\",{\"1\":{\"37\":1}}],[\"我們可以想成現在\",{\"1\":{\"36\":1}}],[\"我們透過\",{\"1\":{\"22\":1}}],[\"我還沒有理解這一段做了什麼\",{\"1\":{\"8\":1}}],[\"範圍變成\",{\"1\":{\"21\":1}}],[\"學習\",{\"1\":{\"19\":1}}],[\"其實\",{\"1\":{\"87\":1}}],[\"其方法與這一篇可說是大同小異\",{\"1\":{\"75\":1}}],[\"其餘則為\",{\"1\":{\"65\":1}}],[\"其他遊戲則都是\",{\"1\":{\"65\":1}}],[\"其他的\",{\"1\":{\"48\":1}}],[\"其他絕大多都是\",{\"1\":{\"37\":1}}],[\"其他像是路燈\",{\"1\":{\"36\":1}}],[\"其他\",{\"1\":{\"21\":1}}],[\"其他部分基本上都跟\",{\"1\":{\"19\":1}}],[\"其中一半的\",{\"1\":{\"38\":1}}],[\"其中的\",{\"1\":{\"13\":1}}],[\"其中\",{\"1\":{\"8\":2,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"59\":1,\"82\":1}}],[\"交給\",{\"1\":{\"19\":1}}],[\"將沒有做任何修正的\",{\"1\":{\"86\":1}}],[\"將產出的\",{\"1\":{\"36\":1}}],[\"將\",{\"1\":{\"19\":1,\"40\":1,\"41\":1,\"63\":1}}],[\"重複\",{\"1\":{\"19\":1}}],[\"若已經又經過\",{\"1\":{\"19\":1}}],[\"⋅∣st+i​\",{\"1\":{\"79\":1,\"84\":1}}],[\"⋅∣s\",{\"1\":{\"76\":1}}],[\"⋅\",{\"1\":{\"19\":1,\"59\":1}}],[\"估計當前\",{\"1\":{\"19\":1}}],[\"開始前\",{\"1\":{\"19\":1}}],[\"開始之前先把參數加上\",{\"1\":{\"81\":1}}],[\"開始之前他們把一個\",{\"1\":{\"22\":1}}],[\"開始之前\",{\"1\":{\"13\":1}}],[\"搭配\",{\"1\":{\"17\":2,\"43\":1,\"61\":1,\"78\":1}}],[\"y=wx+b⇒y=\",{\"1\":{\"81\":1}}],[\"y∼p\",{\"1\":{\"76\":1}}],[\"you\",{\"1\":{\"52\":1}}],[\"ym​\",{\"1\":{\"41\":4}}],[\"ys​\",{\"1\":{\"41\":5}}],[\"ya​\",{\"1\":{\"38\":1}}],[\"yang\",{\"1\":{\"37\":1}}],[\"yi​−q\",{\"1\":{\"59\":1}}],[\"yi​​=es\",{\"1\":{\"59\":1}}],[\"yi\",{\"1\":{\"36\":2,\"43\":1}}],[\"yiheng\",{\"1\":{\"34\":1}}],[\"y\",{\"1\":{\"22\":1,\"76\":2,\"77\":3,\"78\":3,\"83\":10}}],[\"yk​\",{\"1\":{\"16\":1}}],[\"yt​^​=t^q\",{\"1\":{\"8\":1}}],[\"yt​^​\",{\"1\":{\"8\":1,\"41\":2}}],[\"之類的\",{\"1\":{\"40\":1}}],[\"之間都會具有相當高的相關性\",{\"1\":{\"57\":1}}],[\"之間有一些重疊的\",{\"1\":{\"24\":1}}],[\"之間均勻分布的隨機\",{\"1\":{\"16\":1}}],[\"之間均勻分布的隨機值\",{\"1\":{\"16\":1}}],[\"之後的結果都有些進步\",{\"1\":{\"86\":1}}],[\"之後的誤差\",{\"1\":{\"77\":1}}],[\"之後可以得到底下的\",{\"1\":{\"78\":1}}],[\"之後作為實際上儲存進\",{\"1\":{\"63\":1}}],[\"之後得到的成效在\",{\"1\":{\"24\":1}}],[\"之後\",{\"1\":{\"24\":1,\"63\":1,\"81\":1}}],[\"之後仍然不會停止的話就不會出現\",{\"1\":{\"22\":1}}],[\"之後交給\",{\"1\":{\"19\":1}}],[\"之所以說\",{\"1\":{\"3\":1}}],[\"多加上\",{\"1\":{\"16\":1}}],[\"多多探索\",{\"1\":{\"6\":1}}],[\"對應的\",{\"1\":{\"38\":2}}],[\"對陌生人的認識\",{\"1\":{\"37\":1}}],[\"對抗式學習\",{\"1\":{\"36\":1}}],[\"對於每個可訓練的參數拆解成\",{\"1\":{\"81\":1}}],[\"對於每個實驗的\",{\"1\":{\"21\":1}}],[\"對於\",{\"1\":{\"17\":1,\"37\":1,\"79\":1}}],[\"對於結果並不會有影響\",{\"1\":{\"16\":1}}],[\"對於學習是並沒有幫助的\",{\"1\":{\"10\":1}}],[\"對\",{\"1\":{\"16\":1,\"83\":1}}],[\"剩餘的都是相同的\",{\"1\":{\"15\":1}}],[\"剩下的四款遊戲則是因為環境太大\",{\"1\":{\"3\":1}}],[\"剩下這些遊戲有怎樣的共通點呢\",{\"1\":{\"3\":1}}],[\"僅僅是加上\",{\"1\":{\"15\":1}}],[\"∑k−1​rk​\",{\"1\":{\"15\":1}}],[\"∑k−1​1\",{\"1\":{\"15\":1}}],[\"來達成\",{\"1\":{\"75\":1}}],[\"來近似\",{\"1\":{\"59\":1}}],[\"來說可能導致收斂不穩定以及緩慢等問題\",{\"1\":{\"79\":1}}],[\"來說\",{\"1\":{\"59\":1,\"81\":1}}],[\"來說由於缺乏對於\",{\"1\":{\"37\":1}}],[\"來說是相當大的問題\",{\"1\":{\"37\":1}}],[\"來讓每個\",{\"1\":{\"27\":1}}],[\"來比較\",{\"1\":{\"23\":1}}],[\"來調整整體\",{\"1\":{\"22\":1}}],[\"來限制要考慮多久之前的經驗\",{\"1\":{\"15\":1}}],[\"來解決它\",{\"1\":{\"14\":1}}],[\"來解決\",{\"1\":{\"13\":1}}],[\"採取\",{\"1\":{\"59\":1}}],[\"採取的次數以及得到的\",{\"1\":{\"15\":1}}],[\"採用不同的\",{\"1\":{\"13\":1}}],[\"採用分散式學習\",{\"1\":{\"9\":1}}],[\"而接下來作者給出一個\",{\"1\":{\"82\":1}}],[\"而這裡則選擇在\",{\"1\":{\"75\":1}}],[\"而這種探索的困難度甚至是指數性地成長\",{\"1\":{\"73\":1}}],[\"而在\",{\"1\":{\"75\":2}}],[\"而且也往往會經過一段時間的延遲才取得\",{\"1\":{\"57\":1}}],[\"而\",{\"1\":{\"37\":1,\"43\":1,\"44\":1,\"59\":1,\"62\":1}}],[\"而半監督式學習困難的點在於雖然對於\",{\"1\":{\"37\":1}}],[\"而被提出的\",{\"1\":{\"37\":1}}],[\"而是給了參數\",{\"1\":{\"81\":1}}],[\"而是另外定義了一個\",{\"1\":{\"78\":1}}],[\"而是將\",{\"1\":{\"41\":1}}],[\"而是\",{\"1\":{\"21\":1}}],[\"而已\",{\"1\":{\"15\":1}}],[\"而隨著\",{\"1\":{\"15\":1}}],[\"而整體的\",{\"1\":{\"6\":1}}],[\"而整體\",{\"1\":{\"6\":1}}],[\"並無法繼續擴充到其他的領域\",{\"1\":{\"60\":1}}],[\"並無法好好只透過一個\",{\"1\":{\"22\":1}}],[\"並\",{\"1\":{\"59\":1}}],[\"並沒有\",{\"1\":{\"50\":1}}],[\"並不會都得出\",{\"1\":{\"87\":1}}],[\"並不會是一個好的選項\",{\"1\":{\"15\":1}}],[\"並不一定要是\",{\"1\":{\"48\":1}}],[\"並不是所有的\",{\"1\":{\"37\":1}}],[\"並且沒有使用\",{\"1\":{\"79\":1}}],[\"並且達到了很棒的效果\",{\"1\":{\"75\":1}}],[\"並且做出相對應的\",{\"1\":{\"65\":1}}],[\"並且會使用\",{\"1\":{\"65\":1}}],[\"並且\",{\"1\":{\"63\":1}}],[\"並且證明了底下兩個狀況是可以確保收斂\",{\"1\":{\"61\":1}}],[\"並且不同\",{\"1\":{\"27\":1}}],[\"並且每個\",{\"1\":{\"22\":1}}],[\"並且也是\",{\"1\":{\"12\":1}}],[\"然後拿到了\",{\"1\":{\"50\":1}}],[\"然後會有一個\",{\"1\":{\"36\":1}}],[\"然後應用在真實的環境當中\",{\"1\":{\"33\":1}}],[\"然後再透過\",{\"1\":{\"81\":1}}],[\"然後再把這些特徵丟去給\",{\"1\":{\"62\":1}}],[\"然後再應用在真實的世界當中\",{\"1\":{\"33\":1}}],[\"然後再讓\",{\"1\":{\"9\":1}}],[\"然後結束這個\",{\"1\":{\"22\":1}}],[\"然後繼續跟環境互動\",{\"1\":{\"19\":1}}],[\"然而若觀察倒數第二個\",{\"1\":{\"87\":1}}],[\"然而存在幾個缺點\",{\"1\":{\"79\":1}}],[\"然而使用了\",{\"1\":{\"77\":1}}],[\"然而在現實狀況下往往並不會如此簡單\",{\"1\":{\"73\":1}}],[\"然而這些研究都並未能夠給出用非線性去學\",{\"1\":{\"61\":1}}],[\"然而我們也看到近幾年\",{\"1\":{\"57\":1}}],[\"然而\",{\"1\":{\"15\":1,\"77\":1}}],[\"從訓練中的曲線也可以明顯看到\",{\"1\":{\"86\":1}}],[\"從\",{\"1\":{\"41\":2}}],[\"從這裡也可以了解到實際上讓每個\",{\"1\":{\"24\":1}}],[\"從這些觀察當中可以得到兩個待改善的地方\",{\"1\":{\"3\":1}}],[\"從上面的圖片中也可以觀察到這樣的\",{\"1\":{\"24\":1}}],[\"從結果可以發現到\",{\"1\":{\"22\":1}}],[\"從式子當中也可以觀察到\",{\"1\":{\"14\":1}}],[\"嘗試機率高\",{\"1\":{\"14\":1}}],[\"嘗試機率低\",{\"1\":{\"14\":1}}],[\"嘗試次數少\",{\"1\":{\"14\":1}}],[\"嘗試次數少的選擇\",{\"1\":{\"14\":1}}],[\"嘗試次數多的選擇\",{\"1\":{\"14\":1}}],[\"探索機率更高\",{\"1\":{\"14\":1}}],[\"探索機率高\",{\"1\":{\"14\":1}}],[\"更好的成效\",{\"1\":{\"27\":1}}],[\"更好學習\",{\"1\":{\"8\":1}}],[\"更新參數是從\",{\"1\":{\"76\":1}}],[\"更新模型參數\",{\"1\":{\"19\":1}}],[\"更新\",{\"1\":{\"15\":1}}],[\"更高\",{\"1\":{\"14\":1}}],[\"高估的問題消失\",{\"1\":{\"77\":1}}],[\"高估的狀況如底下的綠線\",{\"1\":{\"77\":1}}],[\"高\",{\"1\":{\"14\":5}}],[\"➡️\",{\"1\":{\"14\":8}}],[\"低\",{\"1\":{\"14\":3}}],[\"平均去評估\",{\"1\":{\"66\":1}}],[\"平均\",{\"1\":{\"14\":5,\"15\":1}}],[\"要傾向\",{\"1\":{\"14\":2}}],[\"未知\",{\"1\":{\"14\":1}}],[\"基本想法\",{\"0\":{\"81\":1}}],[\"基本概念\",{\"1\":{\"28\":1}}],[\"基本上就是將\",{\"1\":{\"82\":1}}],[\"基本上就是從\",{\"1\":{\"38\":1}}],[\"基本上都相當接近\",{\"1\":{\"22\":1}}],[\"基本上使用了\",{\"1\":{\"9\":1}}],[\"基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界\",{\"1\":{\"14\":1}}],[\"目標\",{\"1\":{\"77\":2}}],[\"目標是在整個\",{\"1\":{\"14\":1}}],[\"目的也是希望能夠讓\",{\"1\":{\"5\":1}}],[\"你選擇\",{\"1\":{\"14\":1}}],[\"你分別把這幾個\",{\"1\":{\"13\":1}}],[\"k=4\",{\"1\":{\"65\":1}}],[\"k=3\",{\"1\":{\"65\":1}}],[\"k=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"koray\",{\"1\":{\"56\":1,\"63\":1,\"66\":1,\"67\":1}}],[\"know\",{\"1\":{\"52\":1}}],[\"k−τ\",{\"1\":{\"15\":2}}],[\"k−1\",{\"1\":{\"14\":1,\"15\":1}}],[\"kalman\",{\"1\":{\"89\":1}}],[\"kavukcuoglu\",{\"1\":{\"56\":1,\"63\":1,\"66\":1,\"67\":1}}],[\"kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"kapturowski\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"k\",{\"1\":{\"14\":2,\"15\":1,\"65\":1,\"79\":1}}],[\"017\",{\"1\":{\"84\":1}}],[\"02\",{\"1\":{\"50\":1}}],[\"04\",{\"1\":{\"24\":1}}],[\"00\",{\"1\":{\"24\":6}}],[\"06\",{\"1\":{\"24\":1}}],[\"0\",{\"1\":{\"14\":1,\"15\":2,\"16\":2,\"21\":3,\"22\":3,\"24\":5,\"50\":1,\"65\":1,\"75\":1,\"78\":2,\"86\":1,\"87\":2}}],[\"簡單設計成一個\",{\"1\":{\"14\":1}}],[\"把兩個圖片\",{\"1\":{\"38\":1}}],[\"把這個上界當成是它預期的\",{\"1\":{\"14\":1}}],[\"把\",{\"1\":{\"14\":1,\"38\":3,\"40\":1,\"41\":1,\"78\":1}}],[\"ϵj​∈rp\",{\"1\":{\"82\":1}}],[\"ϵj​\",{\"1\":{\"82\":2}}],[\"ϵi​∈rq\",{\"1\":{\"82\":1}}],[\"ϵi​\",{\"1\":{\"82\":1}}],[\"ϵi\",{\"1\":{\"82\":1}}],[\"ϵb∈rq\",{\"1\":{\"82\":1}}],[\"ϵb\",{\"1\":{\"81\":1}}],[\"ϵw∈rq×p\",{\"1\":{\"82\":1}}],[\"ϵw\",{\"1\":{\"81\":1}}],[\"ϵucb​\",{\"1\":{\"16\":1}}],[\"ϵ\",{\"1\":{\"13\":1,\"16\":1,\"63\":1,\"73\":1,\"74\":1,\"75\":3,\"76\":1,\"78\":1,\"81\":1,\"83\":8,\"84\":3,\"87\":2}}],[\"ϵl​=ϵ1+αl−11​\",{\"1\":{\"19\":1}}],[\"ϵl​\",{\"1\":{\"13\":1,\"19\":2}}],[\"亦即\",{\"1\":{\"13\":1,\"25\":1}}],[\"w∈rq×p\",{\"1\":{\"82\":1}}],[\"weight\",{\"1\":{\"82\":1}}],[\"with\",{\"0\":{\"55\":1},\"1\":{\"51\":1,\"58\":1,\"75\":2,\"89\":2}}],[\"wilhelm\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"winter\",{\"1\":{\"32\":1}}],[\"window\",{\"0\":{\"15\":1,\"16\":1,\"17\":1,\"23\":1},\"1\":{\"15\":2,\"16\":1}}],[\"work\",{\"1\":{\"34\":1}}],[\"works\",{\"0\":{\"4\":1,\"35\":1,\"58\":1,\"74\":1}}],[\"what\",{\"0\":{\"33\":1}}],[\"wang\",{\"1\":{\"76\":1,\"78\":1}}],[\"wall\",{\"1\":{\"47\":1}}],[\"wacv\",{\"1\":{\"32\":1},\"2\":{\"54\":1}}],[\"warning\",{\"1\":{\"13\":1,\"75\":1}}],[\"中隨意挑一筆\",{\"1\":{\"76\":1}}],[\"中的\",{\"1\":{\"63\":1}}],[\"中各取圖片\",{\"1\":{\"36\":1}}],[\"中\",{\"1\":{\"13\":1}}],[\"此外\",{\"1\":{\"13\":1,\"21\":1,\"22\":1,\"24\":1,\"87\":1}}],[\"前面都加上一組\",{\"1\":{\"13\":1}}],[\"具有更高的靈活性\",{\"1\":{\"13\":1}}],[\"具體來說就是他們試圖在\",{\"1\":{\"75\":1}}],[\"具體來說如下圖\",{\"1\":{\"21\":1}}],[\"具體來說\",{\"1\":{\"5\":1,\"38\":1}}],[\"蒐集的\",{\"1\":{\"13\":1}}],[\"蒐集一些\",{\"1\":{\"13\":1}}],[\"另一方面\",{\"1\":{\"13\":1,\"22\":1,\"24\":1}}],[\"給你學習\",{\"1\":{\"13\":1}}],[\"什麼時候該\",{\"1\":{\"13\":1}}],[\"透過儲存\",{\"1\":{\"76\":1}}],[\"透過一個\",{\"1\":{\"60\":1}}],[\"透過一些方式混在一起\",{\"1\":{\"38\":1}}],[\"透過固定訓練的目標\",{\"1\":{\"59\":1}}],[\"透過這個模型我們就有辦法給\",{\"1\":{\"37\":1}}],[\"透過上一個\",{\"1\":{\"19\":1}}],[\"透過各自的\",{\"1\":{\"19\":1}}],[\"透過\",{\"1\":{\"16\":1,\"17\":1,\"19\":1,\"25\":1,\"36\":1,\"41\":1,\"63\":1,\"68\":1,\"76\":1,\"78\":1,\"87\":1}}],[\"透過它決定接下來要使用的\",{\"1\":{\"13\":1}}],[\"透過加上\",{\"1\":{\"13\":1}}],[\"透過拆開訓練\",{\"1\":{\"12\":1}}],[\"無法收斂的問題可以透過\",{\"1\":{\"61\":1}}],[\"無法收斂\",{\"1\":{\"61\":1}}],[\"無法好好處理\",{\"1\":{\"10\":1,\"17\":1}}],[\"無論是否有使用\",{\"1\":{\"12\":1}}],[\"輸入進去\",{\"1\":{\"12\":1}}],[\"於是他們定義了底下的\",{\"1\":{\"76\":1}}],[\"於是\",{\"1\":{\"12\":1}}],[\"為了進一步去釐清這樣的做法為什麼是可行\",{\"1\":{\"87\":1}}],[\"為當前\",{\"1\":{\"12\":1}}],[\"為目標\",{\"1\":{\"12\":1}}],[\"為\",{\"1\":{\"12\":3,\"59\":1,\"65\":1}}],[\"表示著在當前這個\",{\"1\":{\"78\":1}}],[\"表示不影響\",{\"1\":{\"65\":1}}],[\"表示不同的\",{\"1\":{\"13\":1}}],[\"表示在時間\",{\"1\":{\"59\":1}}],[\"表示\",{\"1\":{\"12\":1,\"24\":1}}],[\"表示從\",{\"1\":{\"12\":1}}],[\"表示使用的是哪一個\",{\"1\":{\"12\":1}}],[\"因為遊戲當中的雷射會跑很快\",{\"1\":{\"65\":1}}],[\"因為缺乏對他人的理解\",{\"1\":{\"34\":1}}],[\"因為過去的經驗即便在現實狀況改變仍然有大影響力\",{\"1\":{\"15\":1}}],[\"因為是一次更新\",{\"1\":{\"12\":1}}],[\"因此不太需要考慮上述的\",{\"1\":{\"84\":1}}],[\"因此上述的\",{\"1\":{\"83\":1}}],[\"因此在參數上也就包含了兩項\",{\"1\":{\"79\":1}}],[\"因此細節上是還會對\",{\"1\":{\"78\":1}}],[\"因此這一篇論文提出一個方法試圖去消除\",{\"1\":{\"73\":1}}],[\"因此這時候\",{\"1\":{\"38\":1}}],[\"因此仍然沒有解決問題\",{\"1\":{\"73\":1}}],[\"因此作者改成\",{\"1\":{\"66\":1}}],[\"因此作者認為\",{\"1\":{\"24\":1}}],[\"因此評估一個\",{\"1\":{\"66\":1}}],[\"因此會比較能夠好好評估\",{\"1\":{\"21\":1}}],[\"因此\",{\"1\":{\"12\":1,\"15\":1,\"33\":1,\"63\":1,\"77\":1,\"81\":1,\"87\":1}}],[\"個數值\",{\"1\":{\"82\":1}}],[\"個的平均\",{\"1\":{\"49\":1}}],[\"個平均跟\",{\"1\":{\"49\":1}}],[\"個比較困難的遊戲當中\",{\"1\":{\"24\":1}}],[\"個比較難的遊戲當中測試的結果\",{\"1\":{\"23\":1}}],[\"個\",{\"1\":{\"12\":1,\"14\":1,\"19\":1,\"22\":1,\"47\":1,\"49\":2,\"63\":1,\"65\":2,\"82\":1,\"86\":1}}],[\"個遊戲場景\",{\"1\":{\"3\":1}}],[\"個遊戲是\",{\"1\":{\"3\":1}}],[\"個遊戲當中有\",{\"1\":{\"3\":1}}],[\"細節上\",{\"1\":{\"12\":1,\"13\":1,\"22\":1}}],[\"時於\",{\"1\":{\"79\":1}}],[\"時訓練不佳的問題\",{\"1\":{\"68\":1}}],[\"時常我們會訓練在合成資料上\",{\"1\":{\"33\":1}}],[\"時間\",{\"0\":{\"82\":1},\"1\":{\"12\":2}}],[\"時\",{\"1\":{\"12\":1}}],[\"跟幾個\",{\"1\":{\"67\":1}}],[\"跟環境的互動過程當中的\",{\"1\":{\"63\":1}}],[\"跟這一篇\",{\"1\":{\"62\":1}}],[\"跟\",{\"1\":{\"12\":1,\"24\":1,\"38\":2,\"41\":1,\"43\":1,\"67\":1,\"81\":1,\"86\":2}}],[\"相較之下\",{\"1\":{\"57\":1,\"75\":1}}],[\"相鄰而導致的誤判被稱為\",{\"1\":{\"40\":1}}],[\"相差過大\",{\"1\":{\"33\":1}}],[\"相對的\",{\"1\":{\"22\":1}}],[\"相同\",{\"1\":{\"19\":1}}],[\"相同的\",{\"1\":{\"12\":1}}],[\"相當重要的問題\",{\"1\":{\"3\":1}}],[\"兩部分影響程度的參數\",{\"1\":{\"78\":1}}],[\"兩個參數的\",{\"1\":{\"79\":1}}],[\"兩個模型都是使用\",{\"1\":{\"12\":1}}],[\"兩個\",{\"1\":{\"12\":1,\"36\":1}}],[\"兩篇\",{\"1\":{\"3\":1}}],[\"j​=p​0\",{\"1\":{\"84\":1}}],[\"j​=0\",{\"1\":{\"84\":1}}],[\"j​∼u\",{\"1\":{\"84\":2}}],[\"jw​ϵjb​​=f\",{\"1\":{\"82\":1}}],[\"juliani\",{\"1\":{\"79\":1}}],[\"john\",{\"1\":{\"19\":1}}],[\"j\",{\"1\":{\"12\":5,\"13\":1,\"19\":1}}],[\"處理\",{\"1\":{\"12\":1,\"34\":1}}],[\"用來調整兩個\",{\"1\":{\"79\":1}}],[\"用來調整兩種\",{\"1\":{\"6\":1}}],[\"用來加上\",{\"1\":{\"74\":1}}],[\"用來表示一個\",{\"1\":{\"14\":2}}],[\"用兩個\",{\"1\":{\"12\":1}}],[\"首先把\",{\"1\":{\"86\":1}}],[\"首先針對\",{\"1\":{\"12\":1}}],[\"首先定義從\",{\"1\":{\"8\":1}}],[\"問題在於不同的\",{\"1\":{\"34\":1}}],[\"問題\",{\"1\":{\"10\":1,\"14\":1}}],[\"問題描述\",{\"0\":{\"3\":1,\"34\":1,\"57\":1,\"73\":1}}],[\"但有包含了部分的學習過程\",{\"1\":{\"67\":1}}],[\"但在\",{\"1\":{\"57\":1}}],[\"但在邊界上往往還是難以有好的結果\",{\"1\":{\"37\":1}}],[\"但數值範圍往往很\",{\"1\":{\"57\":1}}],[\"但數學有點太難\",{\"1\":{\"8\":1}}],[\"但透過剪貼則可以造成不同環境的突兀感\",{\"1\":{\"38\":1}}],[\"但主要的問題來自於\",{\"1\":{\"37\":1}}],[\"但並不\",{\"1\":{\"25\":1}}],[\"但最後能取得更好的\",{\"1\":{\"23\":1}}],[\"但整體來說兩者都能在最後趨近於\",{\"1\":{\"22\":1}}],[\"但是這種做法實際上效果很糟糕\",{\"1\":{\"40\":1}}],[\"但是並不全面\",{\"1\":{\"37\":1}}],[\"但是對於真實世界\",{\"1\":{\"33\":1}}],[\"但是在\",{\"1\":{\"25\":1,\"66\":1}}],[\"但是\",{\"1\":{\"13\":1}}],[\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的\",{\"1\":{\"13\":1}}],[\"但是卻跟其他人有同樣的影響力\",{\"1\":{\"10\":1}}],[\"但\",{\"1\":{\"12\":1,\"48\":1}}],[\"數量相同\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"每\",{\"1\":{\"86\":2}}],[\"每一個\",{\"1\":{\"13\":1}}],[\"每個遊戲的\",{\"1\":{\"65\":1}}],[\"每個\",{\"1\":{\"12\":1,\"13\":2,\"16\":1,\"19\":1}}],[\"每種\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"每忽略一個\",{\"1\":{\"3\":1}}],[\"難以收斂的問題\",{\"1\":{\"12\":1,\"27\":1}}],[\"難以收斂\",{\"1\":{\"10\":1}}],[\"實驗是做在\",{\"1\":{\"86\":1}}],[\"實驗上調整了\",{\"1\":{\"65\":1}}],[\"實驗做在\",{\"1\":{\"65\":1}}],[\"實驗設定\",{\"0\":{\"43\":1,\"65\":1}}],[\"實作上\",{\"1\":{\"10\":1,\"63\":1}}],[\"實際上跟\",{\"1\":{\"78\":1}}],[\"實際上是有幫助的\",{\"1\":{\"24\":1}}],[\"實際上還會為了讓\",{\"1\":{\"8\":1}}],[\"實際上訓練的\",{\"1\":{\"8\":1}}],[\"丟在\",{\"1\":{\"9\":1}}],[\"有多好多壞\",{\"1\":{\"78\":1}}],[\"有多少影響呢\",{\"1\":{\"23\":1}}],[\"有更好的效果\",{\"1\":{\"77\":1}}],[\"有更多的\",{\"1\":{\"75\":1}}],[\"有提及一個使用\",{\"1\":{\"73\":1}}],[\"有大的影響\",{\"1\":{\"66\":1}}],[\"有不少人最後給的結果之所以那麼好看是因為\",{\"1\":{\"50\":1}}],[\"有兩列分別表示\",{\"1\":{\"49\":1}}],[\"有點偏以及\",{\"1\":{\"48\":1}}],[\"有還不錯的成效\",{\"1\":{\"63\":1}}],[\"有還不錯的\",{\"1\":{\"48\":1}}],[\"有一些常見的\",{\"1\":{\"44\":1}}],[\"有可能就被誤判成人行道\",{\"1\":{\"37\":1}}],[\"有部分的認知\",{\"1\":{\"37\":1}}],[\"有相當大的差異\",{\"1\":{\"36\":1}}],[\"有最大的\",{\"1\":{\"25\":1}}],[\"有最好的結果\",{\"1\":{\"25\":1}}],[\"有了\",{\"1\":{\"24\":1}}],[\"有了目標\",{\"1\":{\"8\":1}}],[\"有較大的影響\",{\"1\":{\"22\":1}}],[\"有人天生愛保險\",{\"1\":{\"13\":1}}],[\"有人天生愛探險\",{\"1\":{\"13\":1}}],[\"有些甚至是遞增的\",{\"1\":{\"87\":1}}],[\"有些是\",{\"1\":{\"49\":1}}],[\"有些傾向\",{\"1\":{\"13\":2}}],[\"有些則不需要\",{\"1\":{\"10\":1}}],[\"有些環境需要更多的\",{\"1\":{\"10\":1}}],[\"有些\",{\"1\":{\"10\":1,\"49\":1}}],[\"有時會很不穩定\",{\"1\":{\"10\":1}}],[\"有許多的\",{\"1\":{\"9\":1}}],[\"只需要產出\",{\"1\":{\"82\":1}}],[\"只是調整\",{\"1\":{\"78\":1}}],[\"只是用來限制\",{\"1\":{\"6\":1}}],[\"只會每經過\",{\"1\":{\"65\":1}}],[\"只會取出最後\",{\"1\":{\"63\":1}}],[\"只會儲存最後\",{\"1\":{\"63\":1}}],[\"只對簡單的\",{\"1\":{\"48\":1}}],[\"只有\",{\"1\":{\"37\":1}}],[\"只訓練在\",{\"1\":{\"34\":1}}],[\"只用了一個\",{\"1\":{\"10\":1}}],[\"只不過輸入上會丟\",{\"1\":{\"9\":1}}],[\"∣st+i​\",{\"1\":{\"84\":1}}],[\"∣st​\",{\"1\":{\"7\":1}}],[\"∣x∣​\",{\"1\":{\"82\":1}}],[\"∣z∣+1+ϵ\",{\"1\":{\"8\":1}}],[\"∣z∣+1​−1\",{\"1\":{\"8\":1}}],[\"zero\",{\"1\":{\"81\":2}}],[\"ziyu\",{\"1\":{\"76\":1,\"78\":1}}],[\"zou\",{\"1\":{\"37\":1}}],[\"zhang\",{\"1\":{\"34\":1}}],[\"z\",{\"1\":{\"8\":4}}],[\"∀z∈r\",{\"1\":{\"8\":2}}],[\"+p1​​\",{\"1\":{\"84\":1}}],[\"+p3​​\",{\"1\":{\"84\":1}}],[\"+λlv\",{\"1\":{\"79\":1}}],[\"+λh\",{\"1\":{\"41\":1}}],[\"+​a\",{\"1\":{\"78\":1}}],[\"+a\",{\"1\":{\"78\":1}}],[\"+0\",{\"1\":{\"50\":1}}],[\"+1\",{\"1\":{\"50\":2}}],[\"+2\",{\"1\":{\"50\":1}}],[\"+βi=0∑k​∇θπ​​h\",{\"1\":{\"79\":1,\"84\":1}}],[\"+βnk−1​\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"+βj​q\",{\"1\":{\"12\":1}}],[\"+ϵz=sgn\",{\"1\":{\"8\":1}}],[\"+t≥0∑​γt\",{\"1\":{\"8\":1}}],[\"+s=t∑t+k−1​γs−t\",{\"1\":{\"8\":1}}],[\"x+\",{\"1\":{\"81\":1}}],[\"xm​\",{\"1\":{\"41\":4}}],[\"xs​\",{\"1\":{\"41\":5}}],[\"xsb​\",{\"1\":{\"12\":3}}],[\"xa​\",{\"1\":{\"38\":2}}],[\"x\",{\"1\":{\"8\":2,\"12\":6,\"22\":1,\"82\":2}}],[\"xt+1​\",{\"1\":{\"8\":3,\"19\":1}}],[\"xt​\",{\"1\":{\"8\":8,\"19\":2,\"41\":3}}],[\"版本\",{\"1\":{\"8\":1}}],[\"改成\",{\"1\":{\"8\":1}}],[\"上適用\",{\"1\":{\"88\":1}}],[\"上較為顯著\",{\"1\":{\"86\":1}}],[\"上圖就是在最後分開成兩個輸出結果\",{\"1\":{\"78\":1}}],[\"上加上\",{\"1\":{\"75\":1}}],[\"上加\",{\"1\":{\"75\":1}}],[\"上增加\",{\"1\":{\"75\":2}}],[\"上增加了\",{\"1\":{\"75\":1}}],[\"上鼓勵\",{\"1\":{\"75\":1}}],[\"上會發生\",{\"1\":{\"40\":1}}],[\"上訓練\",{\"1\":{\"83\":1,\"84\":1}}],[\"上訓練一個模型\",{\"1\":{\"37\":1}}],[\"上訓練的模型難以直接\",{\"1\":{\"33\":1}}],[\"上\",{\"1\":{\"33\":1,\"38\":2,\"52\":1,\"57\":1,\"82\":1,\"86\":1}}],[\"上也許我們能夠對各種物件去做標記\",{\"1\":{\"33\":1}}],[\"上面基本的做法作者稱他為\",{\"1\":{\"82\":1}}],[\"上面的差異就是這裡傳入的分別是\",{\"1\":{\"12\":1}}],[\"上面提及的是單純的\",{\"1\":{\"8\":1}}],[\"上的認知\",{\"1\":{\"37\":1}}],[\"上的時候\",{\"1\":{\"34\":1}}],[\"上的例子\",{\"1\":{\"33\":1}}],[\"上的\",{\"1\":{\"8\":1,\"36\":1,\"66\":1}}],[\"−scorerandom​scoreagent​−scorebaseline​​\",{\"1\":{\"86\":1}}],[\"−p1​​\",{\"1\":{\"84\":1}}],[\"−p3​​\",{\"1\":{\"84\":1}}],[\"−∣a∣1​a\",{\"1\":{\"78\":1}}],[\"−v\",{\"1\":{\"78\":1}}],[\"−trt\",{\"1\":{\"59\":1}}],[\"−t^r\",{\"1\":{\"12\":1}}],[\"−1\",{\"1\":{\"8\":1,\"65\":1}}],[\"−h−1q\",{\"1\":{\"8\":1}}],[\"−yt​^​\",{\"1\":{\"8\":1}}],[\"−q\",{\"1\":{\"8\":1,\"76\":1,\"78\":1,\"83\":4}}],[\"θ=defμ+σ⊙ϵ\",{\"1\":{\"81\":1,\"87\":1}}],[\"θv\",{\"1\":{\"79\":3}}],[\"θv​\",{\"1\":{\"78\":1,\"79\":5,\"84\":3}}],[\"θπ​\",{\"1\":{\"79\":5,\"84\":3}}],[\"θa​\",{\"1\":{\"78\":1}}],[\"θ~=θ+n\",{\"1\":{\"75\":1}}],[\"θl​\",{\"1\":{\"19\":1}}],[\"θi−1​\",{\"1\":{\"59\":2}}],[\"θi​\",{\"1\":{\"59\":3}}],[\"θi\",{\"1\":{\"12\":2}}],[\"θe∪θi\",{\"1\":{\"12\":1}}],[\"θe\",{\"1\":{\"12\":2}}],[\"θ\",{\"1\":{\"8\":2,\"12\":6,\"41\":1,\"76\":2,\"77\":2,\"78\":8,\"79\":3,\"81\":3,\"83\":5}}],[\"θ−\",{\"1\":{\"8\":2,\"12\":3,\"76\":2,\"77\":3,\"78\":1,\"83\":2}}],[\"一次\",{\"1\":{\"86\":1}}],[\"一開始都不太相同\",{\"1\":{\"65\":1}}],[\"一開始都是一樣的\",{\"1\":{\"13\":1}}],[\"一起\",{\"1\":{\"41\":1}}],[\"一個\",{\"1\":{\"60\":1}}],[\"一個常見的問題是產出的結果通常會傾向去預測結果為常見的\",{\"1\":{\"37\":1}}],[\"一個簡單的方法是想辦法給這些\",{\"1\":{\"37\":1}}],[\"一個是\",{\"1\":{\"8\":2}}],[\"一些\",{\"1\":{\"19\":1,\"37\":1}}],[\"一樣好\",{\"1\":{\"86\":1}}],[\"一樣糟\",{\"1\":{\"86\":1}}],[\"一樣\",{\"1\":{\"8\":1,\"43\":1}}],[\"​logπ\",{\"1\":{\"79\":1}}],[\"​​\",{\"1\":{\"14\":1,\"15\":1,\"79\":2}}],[\"​​​∀0≤k≤n−1∀n≤k≤k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​=−e\",{\"1\":{\"84\":1}}],[\"​=−eπ\",{\"1\":{\"79\":1,\"84\":1}}],[\"​=m=max\",{\"1\":{\"15\":1}}],[\"​=m=0∑k−1​1\",{\"1\":{\"14\":1}}],[\"​=nk​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​=sgn\",{\"1\":{\"8\":1}}],[\"​−1​\",{\"1\":{\"8\":1}}],[\"​\",{\"1\":{\"8\":3,\"59\":3,\"78\":1,\"79\":3,\"82\":1,\"84\":1}}],[\"δth​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δth​\",{\"1\":{\"8\":1}}],[\"δt​cs​​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δs​\",{\"1\":{\"8\":1}}],[\"=sgn\",{\"1\":{\"82\":1}}],[\"=f\",{\"1\":{\"82\":1}}],[\"=∇e\",{\"1\":{\"81\":1}}],[\"=lπ\",{\"1\":{\"79\":1}}],[\"=i=0∑k​eπ\",{\"1\":{\"79\":1,\"84\":1}}],[\"=v\",{\"1\":{\"78\":2}}],[\"=t∑t​γt\",{\"1\":{\"59\":1}}],[\"=πmax​e\",{\"1\":{\"59\":1}}],[\"=b=0∑b−1​s=t∑t+h−1​\",{\"1\":{\"12\":1}}],[\"=arga∈amax​q\",{\"1\":{\"12\":1}}],[\"=\",{\"1\":{\"8\":1}}],[\"=λmin\",{\"1\":{\"8\":1}}],[\"=q\",{\"1\":{\"8\":1,\"12\":1,\"78\":1}}],[\"=es\",{\"1\":{\"59\":1}}],[\"=eμ​\",{\"1\":{\"8\":1}}],[\"=e\",{\"1\":{\"7\":1,\"41\":1,\"76\":1,\"78\":1,\"81\":2,\"83\":4,\"84\":1}}],[\"定義底下的平均\",{\"1\":{\"87\":1}}],[\"定義\",{\"1\":{\"8\":1,\"59\":1}}],[\"τ∈n∗\",{\"1\":{\"15\":1}}],[\"τ=\",{\"1\":{\"8\":1}}],[\"τ\",{\"1\":{\"8\":1,\"15\":8,\"16\":2}}],[\"π​q\",{\"1\":{\"12\":1}}],[\"π\",{\"1\":{\"8\":2,\"12\":4,\"59\":2,\"79\":3,\"84\":3}}],[\"μi\",{\"1\":{\"84\":2}}],[\"μ+σ⊙ξ\",{\"1\":{\"81\":1}}],[\"μ+σ⊙ϵ\",{\"1\":{\"81\":1}}],[\"μb+σb⊙ϵb\",{\"1\":{\"81\":1}}],[\"μw+σw⊙ϵw\",{\"1\":{\"81\":1}}],[\"μ^​k​\",{\"1\":{\"14\":2,\"15\":2}}],[\"μ\",{\"1\":{\"8\":3,\"12\":3,\"81\":1,\"87\":1}}],[\"演算法\",{\"1\":{\"8\":1}}],[\"計算分別如下\",{\"1\":{\"79\":1}}],[\"計算\",{\"1\":{\"8\":1,\"19\":1}}],[\"那我們就可以用\",{\"1\":{\"37\":1}}],[\"那也就會有\",{\"1\":{\"8\":1}}],[\"那麼這裡加上的\",{\"1\":{\"87\":1}}],[\"那麼\",{\"1\":{\"3\":1,\"82\":1}}],[\"選擇採用\",{\"1\":{\"43\":1}}],[\"選擇中最大的\",{\"1\":{\"24\":1}}],[\"選擇出一組\",{\"1\":{\"19\":1}}],[\"選擇出現傾向\",{\"1\":{\"13\":1}}],[\"選擇其中最大的當成這次的選擇\",{\"1\":{\"14\":1}}],[\"選擇\",{\"1\":{\"13\":1,\"19\":1}}],[\"選擇的分布\",{\"1\":{\"7\":1}}],[\"選大一些\",{\"1\":{\"7\":1}}],[\"選小一些\",{\"1\":{\"7\":1}}],[\"需要\",{\"1\":{\"79\":1}}],[\"需要額外的\",{\"1\":{\"79\":1}}],[\"需要特別注意到對於\",{\"1\":{\"59\":1}}],[\"需要透過與環境互動取得\",{\"1\":{\"57\":1}}],[\"需要看遠一些\",{\"1\":{\"7\":1}}],[\"需要相當大量的探索之後才能得到\",{\"1\":{\"3\":1}}],[\"小\",{\"1\":{\"7\":1,\"15\":1}}],[\"傾向\",{\"1\":{\"7\":2}}],[\"大小\",{\"1\":{\"13\":1}}],[\"大小為\",{\"1\":{\"12\":1}}],[\"大\",{\"1\":{\"7\":1}}],[\"γ=0\",{\"1\":{\"24\":1}}],[\"γj​\",{\"1\":{\"13\":1,\"19\":1}}],[\"γ\",{\"1\":{\"7\":4,\"16\":1,\"21\":1,\"24\":1,\"59\":1}}],[\"qi​\",{\"1\":{\"79\":1}}],[\"qi​−v\",{\"1\":{\"79\":1,\"84\":2}}],[\"q∗\",{\"1\":{\"59\":4,\"76\":2,\"78\":2}}],[\"quan\",{\"1\":{\"19\":1}}],[\"qπ\",{\"1\":{\"8\":1}}],[\"q\",{\"0\":{\"59\":1},\"1\":{\"7\":2,\"8\":6,\"12\":3,\"19\":1,\"56\":1,\"58\":3,\"59\":2,\"61\":2,\"63\":2,\"65\":1,\"66\":1,\"76\":1,\"78\":4}}],[\"去決定要加怎樣的\",{\"1\":{\"81\":1}}],[\"去加上\",{\"1\":{\"81\":1}}],[\"去儲存\",{\"1\":{\"79\":1}}],[\"去避免訓練資料上的強關聯性\",{\"1\":{\"79\":1}}],[\"去試圖得到\",{\"1\":{\"76\":1}}],[\"去增加\",{\"1\":{\"73\":1}}],[\"去比較\",{\"1\":{\"67\":1}}],[\"去選擇\",{\"1\":{\"63\":1}}],[\"去訓練\",{\"1\":{\"62\":1,\"81\":1}}],[\"去訓練顯然很糟糕\",{\"1\":{\"48\":1,\"49\":1}}],[\"去學\",{\"1\":{\"61\":1}}],[\"去學習\",{\"1\":{\"13\":1,\"22\":1,\"63\":1,\"76\":1,\"78\":1}}],[\"去學習導致\",{\"1\":{\"10\":1}}],[\"去預測\",{\"1\":{\"60\":1}}],[\"去\",{\"1\":{\"41\":1,\"48\":1,\"59\":1}}],[\"去轉移出來\",{\"1\":{\"37\":1}}],[\"去判別現在給我的究竟是\",{\"1\":{\"36\":1}}],[\"去拉近\",{\"1\":{\"36\":1}}],[\"去紀錄訓練過程當中的\",{\"1\":{\"21\":1}}],[\"去跟環境互動\",{\"1\":{\"19\":1}}],[\"去調整選擇不同\",{\"1\":{\"13\":1}}],[\"去更新參數學習\",{\"1\":{\"9\":1}}],[\"去得到目標\",{\"1\":{\"8\":1}}],[\"去逼近\",{\"1\":{\"8\":1}}],[\"去近似\",{\"1\":{\"7\":1,\"81\":1}}],[\"去找到藏在地圖當中的寶藏\",{\"1\":{\"3\":1}}],[\"f\",{\"1\":{\"82\":2}}],[\"fitted\",{\"1\":{\"58\":1}}],[\"fine\",{\"1\":{\"37\":1}}],[\"fence\",{\"1\":{\"47\":1}}],[\"features\",{\"1\":{\"67\":1}}],[\"feature\",{\"1\":{\"36\":1}}],[\"fθ​\",{\"1\":{\"41\":3}}],[\"fortunato\",{\"1\":{\"72\":1,\"86\":3,\"87\":1}}],[\"for\",{\"0\":{\"71\":1,\"75\":1},\"1\":{\"28\":1,\"37\":1,\"52\":4,\"74\":1,\"81\":1,\"89\":2}}],[\"follow\",{\"1\":{\"8\":1,\"12\":1,\"59\":1}}],[\"factorised\",{\"1\":{\"82\":1,\"83\":1,\"84\":1}}],[\"factor\",{\"1\":{\"24\":1,\"59\":1}}],[\"family\",{\"0\":{\"13\":1}}],[\"free\",{\"1\":{\"60\":1,\"61\":1}}],[\"freeway\",{\"1\":{\"24\":1}}],[\"frame\",{\"1\":{\"65\":1}}],[\"framework\",{\"1\":{\"51\":1}}],[\"frames\",{\"1\":{\"19\":1,\"63\":1,\"65\":2,\"86\":2}}],[\"from\",{\"1\":{\"3\":2,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"33\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"63\":1,\"66\":1,\"67\":1,\"75\":2,\"76\":1,\"77\":1,\"78\":1,\"79\":1,\"86\":3,\"87\":1}}],[\"functions\",{\"1\":{\"89\":1}}],[\"functiona\",{\"1\":{\"78\":1}}],[\"function\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"58\":1,\"59\":1,\"60\":1,\"61\":1,\"63\":1,\"76\":3,\"78\":2,\"79\":4,\"84\":1}}],[\"比較\",{\"1\":{\"86\":2}}],[\"比較好\",{\"1\":{\"75\":1}}],[\"比較基準\",{\"0\":{\"67\":1}}],[\"比較差的結果\",{\"1\":{\"21\":1}}],[\"比較傾向去試試看那些不熟的\",{\"1\":{\"6\":1}}],[\"比較大的時候\",{\"1\":{\"6\":1}}],[\"會\",{\"1\":{\"86\":1}}],[\"會比較大\",{\"1\":{\"83\":1}}],[\"會直接去學\",{\"1\":{\"79\":1}}],[\"會透過\",{\"1\":{\"63\":1}}],[\"會將\",{\"1\":{\"63\":1}}],[\"會有幾個明顯的問題\",{\"1\":{\"57\":1}}],[\"會有兩組總和\",{\"1\":{\"12\":1}}],[\"會有兩個\",{\"1\":{\"8\":1}}],[\"會造成的問題是吻合的\",{\"1\":{\"40\":1}}],[\"會不同\",{\"1\":{\"34\":1}}],[\"會導致訓練前期較為緩慢\",{\"1\":{\"23\":1}}],[\"會掉\",{\"1\":{\"22\":1}}],[\"會偏向\",{\"1\":{\"22\":1}}],[\"會得到\",{\"1\":{\"22\":1}}],[\"會從\",{\"1\":{\"19\":1}}],[\"會把每個\",{\"1\":{\"13\":1}}],[\"會選擇不同的\",{\"1\":{\"7\":1}}],[\"會使得\",{\"1\":{\"6\":1}}],[\"會一直到遊戲的最後依照最後通過的時間決定\",{\"1\":{\"3\":1}}],[\"βj​=0\",{\"1\":{\"22\":2}}],[\"βj​=maxj​βj​\",{\"1\":{\"22\":2}}],[\"βj​\",{\"1\":{\"13\":1,\"19\":1,\"22\":4}}],[\"β=0\",{\"1\":{\"9\":1}}],[\"β\",{\"1\":{\"6\":1,\"7\":1,\"16\":1,\"22\":6,\"78\":4,\"79\":1}}],[\"βi​\",{\"1\":{\"6\":1,\"7\":5,\"9\":2,\"10\":1}}],[\"當作輸入\",{\"1\":{\"68\":1}}],[\"當要去更新模型的時候\",{\"1\":{\"63\":1}}],[\"當兩個\",{\"1\":{\"33\":1}}],[\"當\",{\"1\":{\"6\":1,\"22\":2}}],[\"當中使用了\",{\"1\":{\"79\":1}}],[\"當中使用的是\",{\"1\":{\"38\":1}}],[\"當中加上\",{\"1\":{\"75\":1}}],[\"當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",{\"1\":{\"73\":1}}],[\"當中就是環境給予的\",{\"1\":{\"6\":1}}],[\"當中並沒有\",{\"1\":{\"66\":1}}],[\"當中如果要評估一個\",{\"1\":{\"66\":1}}],[\"當中的\",{\"1\":{\"63\":1,\"79\":1}}],[\"當中取得隨機幾筆去更新\",{\"1\":{\"63\":1}}],[\"當中取得的\",{\"1\":{\"8\":1}}],[\"當中我們需要同時訓練兩個\",{\"1\":{\"77\":1}}],[\"當中我們往往仰賴對\",{\"1\":{\"73\":1}}],[\"當中我們看到了使用\",{\"1\":{\"63\":1}}],[\"當中我們會透過\",{\"1\":{\"59\":1}}],[\"當中我們會預設資料之間是沒有什麼相依性的\",{\"1\":{\"57\":1}}],[\"當中同一個\",{\"1\":{\"57\":1}}],[\"當中呢\",{\"1\":{\"57\":1}}],[\"當中通常\",{\"1\":{\"37\":1}}],[\"當中都獲得了超過人類的成效\",{\"1\":{\"25\":1}}],[\"當中獲得比人類平均還要好的成果\",{\"1\":{\"25\":1}}],[\"當中獲得相當不錯的\",{\"1\":{\"3\":1}}],[\"當中是小許多的\",{\"1\":{\"24\":1}}],[\"當中你可以得到最好的\",{\"1\":{\"14\":1}}],[\"當中\",{\"1\":{\"3\":1,\"6\":1,\"19\":1,\"40\":1,\"63\":1}}],[\"設定如下\",{\"1\":{\"6\":1}}],[\"deterministic\",{\"1\":{\"87\":2}}],[\"details\",{\"1\":{\"84\":1}}],[\"deep\",{\"0\":{\"55\":1},\"1\":{\"56\":1,\"57\":4,\"66\":1,\"68\":2,\"89\":2}}],[\"deeplab\",{\"1\":{\"43\":1,\"48\":1}}],[\"deepmind\",{\"1\":{\"2\":1,\"72\":1}}],[\"dθv​←dθv​+∂\",{\"1\":{\"79\":1}}],[\"dθ←dθ+∇θ\",{\"1\":{\"79\":1}}],[\"dueling\",{\"0\":{\"78\":1,\"83\":1},\"1\":{\"74\":1,\"78\":3,\"83\":2,\"86\":2}}],[\"d=e1​\",{\"1\":{\"63\":1}}],[\"dl\",{\"1\":{\"57\":3}}],[\"dt​\",{\"1\":{\"41\":1}}],[\"ds​\",{\"1\":{\"41\":1}}],[\"domain\",{\"0\":{\"31\":2,\"33\":1,\"36\":1,\"41\":2},\"1\":{\"33\":14,\"34\":5,\"36\":7,\"37\":10,\"40\":2,\"41\":6,\"48\":2,\"49\":1,\"52\":5},\"2\":{\"54\":1}}],[\"double\",{\"0\":{\"77\":1},\"1\":{\"8\":2,\"74\":1,\"77\":4,\"78\":1}}],[\"differences\",{\"1\":{\"89\":1}}],[\"dimensional\",{\"1\":{\"57\":1}}],[\"distribution\",{\"1\":{\"34\":1,\"66\":1,\"76\":1}}],[\"distributed\",{\"0\":{\"84\":1},\"1\":{\"28\":2}}],[\"discriminator\",{\"1\":{\"36\":1}}],[\"discussion\",{\"0\":{\"26\":1}}],[\"discount\",{\"1\":{\"24\":1,\"59\":2}}],[\"directed\",{\"1\":{\"28\":1}}],[\"day\",{\"1\":{\"52\":2}}],[\"dataset\",{\"0\":{\"44\":1},\"1\":{\"40\":6}}],[\"data\",{\"1\":{\"33\":1,\"34\":2,\"36\":2,\"37\":7,\"38\":2,\"52\":1}}],[\"dacs\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"36\":1,\"38\":1,\"41\":1,\"48\":1,\"49\":1,\"50\":3,\"52\":1}}],[\"david\",{\"1\":{\"19\":1,\"56\":1,\"63\":1,\"66\":1,\"67\":1,\"77\":1}}],[\"dan\",{\"1\":{\"19\":1}}],[\"d\",{\"1\":{\"12\":2,\"63\":1,\"76\":1}}],[\"dqn\",{\"0\":{\"76\":1,\"77\":1,\"78\":1,\"83\":2},\"1\":{\"8\":1,\"56\":1,\"62\":2,\"63\":2,\"67\":3,\"74\":3,\"76\":1,\"77\":8,\"78\":6,\"79\":1,\"83\":4,\"86\":3}}],[\"driven\",{\"1\":{\"6\":1}}],[\"cityscape\",{\"1\":{\"51\":1}}],[\"cityscapes\",{\"0\":{\"45\":1,\"48\":1,\"49\":1},\"1\":{\"44\":3,\"46\":1,\"47\":1,\"50\":1}}],[\"city\",{\"1\":{\"47\":1}}],[\"cbst\",{\"1\":{\"37\":1}}],[\"classes\",{\"1\":{\"38\":1,\"45\":1,\"46\":2,\"47\":3,\"49\":1,\"50\":2}}],[\"classmix\",{\"1\":{\"38\":2,\"40\":1,\"41\":1,\"43\":1,\"51\":1,\"52\":1}}],[\"class\",{\"1\":{\"37\":2,\"40\":5,\"48\":2,\"49\":1,\"52\":1}}],[\"cnn\",{\"1\":{\"34\":2,\"57\":1}}],[\"cv\",{\"1\":{\"33\":1}}],[\"carlo\",{\"1\":{\"81\":1}}],[\"car\",{\"1\":{\"48\":1}}],[\"cars\",{\"1\":{\"32\":1}}],[\"capped\",{\"1\":{\"21\":1,\"25\":2}}],[\"channel\",{\"1\":{\"67\":1}}],[\"chalmers\",{\"1\":{\"32\":1}}],[\"chiehchen\",{\"1\":{\"34\":1}}],[\"chns=max\",{\"1\":{\"21\":1}}],[\"chns\",{\"1\":{\"21\":1}}],[\"critic\",{\"1\":{\"79\":1,\"89\":1}}],[\"cross\",{\"0\":{\"31\":1},\"1\":{\"41\":1,\"52\":1}}],[\"credit\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"count\",{\"1\":{\"89\":1}}],[\"color\",{\"1\":{\"67\":1}}],[\"column\",{\"1\":{\"37\":1}}],[\"cordts\",{\"1\":{\"45\":1}}],[\"corss\",{\"0\":{\"41\":1}}],[\"computer\",{\"1\":{\"32\":1,\"52\":1,\"62\":1},\"2\":{\"54\":1}}],[\"coin\",{\"1\":{\"22\":6}}],[\"contingency\",{\"1\":{\"67\":1}}],[\"context\",{\"1\":{\"36\":1}}],[\"contribution\",{\"0\":{\"27\":1,\"51\":1,\"68\":1,\"88\":1}}],[\"controller\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":7,\"25\":2,\"27\":1}}],[\"control\",{\"1\":{\"8\":1,\"61\":2,\"67\":1}}],[\"conflation\",{\"1\":{\"40\":1}}],[\"conference\",{\"1\":{\"32\":1}}],[\"confidence\",{\"0\":{\"14\":1}}],[\"conroller\",{\"1\":{\"24\":1}}],[\"curiosity\",{\"1\":{\"6\":1}}],[\"1m\",{\"1\":{\"86\":1}}],[\"17\",{\"1\":{\"50\":1}}],[\"1774\",{\"1\":{\"24\":1}}],[\"13\",{\"1\":{\"47\":2,\"49\":2,\"50\":1}}],[\"19\",{\"1\":{\"45\":1,\"46\":1}}],[\"12326\",{\"1\":{\"24\":1}}],[\"1187\",{\"1\":{\"24\":1}}],[\"11361\",{\"1\":{\"24\":1}}],[\"1177\",{\"1\":{\"24\":1}}],[\"14814\",{\"1\":{\"24\":1}}],[\"14\",{\"1\":{\"24\":1}}],[\"16\",{\"1\":{\"24\":1,\"47\":1,\"49\":2,\"50\":1}}],[\"1664\",{\"1\":{\"24\":1}}],[\"16926\",{\"1\":{\"24\":1}}],[\"160\",{\"1\":{\"17\":1}}],[\"108k\",{\"1\":{\"86\":1}}],[\"101\",{\"1\":{\"48\":1}}],[\"100×max\",{\"1\":{\"86\":1}}],[\"100×scorehuman​−scorerandom​scoreagent​−scorerandom​​\",{\"1\":{\"86\":1}}],[\"100\",{\"1\":{\"25\":1,\"86\":1}}],[\"10362\",{\"1\":{\"24\":1}}],[\"10\",{\"1\":{\"23\":1,\"24\":3,\"50\":1}}],[\"15×15\",{\"1\":{\"22\":1}}],[\"1​​yk​​∀0≤k≤n−1∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"1​m=max\",{\"1\":{\"15\":1}}],[\"1​m=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"1\",{\"1\":{\"6\":1,\"8\":1,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"22\":2,\"50\":1,\"65\":1}}],[\"如\",{\"1\":{\"24\":1,\"57\":2}}],[\"如同前面提及\",{\"1\":{\"22\":1}}],[\"如果出現道路或甚至機車\",{\"1\":{\"37\":1}}],[\"如果我們想要訓練一個模型去做自駕車的街景物件偵測\",{\"1\":{\"33\":1}}],[\"如果選擇較大\",{\"1\":{\"22\":1}}],[\"如果\",{\"1\":{\"15\":1}}],[\"如果每個\",{\"1\":{\"13\":1}}],[\"如此一來\",{\"1\":{\"13\":1,\"41\":1,\"82\":1}}],[\"如下\",{\"1\":{\"6\":1,\"19\":1,\"59\":1}}],[\"如何讓\",{\"1\":{\"3\":1}}],[\"如何決定哪些\",{\"1\":{\"3\":1}}],[\"鼓勵去探索那些在\",{\"1\":{\"6\":1}}],[\"lˉ\",{\"1\":{\"81\":1,\"83\":2}}],[\"lv\",{\"1\":{\"79\":1,\"84\":2}}],[\"layer\",{\"1\":{\"60\":2,\"81\":1,\"87\":2}}],[\"layout\",{\"1\":{\"36\":2}}],[\"labelled\",{\"1\":{\"40\":1}}],[\"labelling\",{\"0\":{\"37\":1},\"1\":{\"38\":1,\"40\":1}}],[\"labeling\",{\"1\":{\"37\":1}}],[\"labeled\",{\"1\":{\"37\":2}}],[\"label\",{\"1\":{\"33\":2,\"37\":4,\"41\":1,\"57\":1}}],[\"linear\",{\"1\":{\"61\":1,\"67\":1,\"81\":1}}],[\"li​\",{\"1\":{\"59\":1}}],[\"liang\",{\"1\":{\"34\":1}}],[\"life\",{\"1\":{\"6\":2}}],[\"lebel\",{\"1\":{\"41\":1}}],[\"level\",{\"0\":{\"18\":1},\"1\":{\"36\":3}}],[\"length\",{\"1\":{\"15\":1,\"17\":2,\"23\":1}}],[\"learner\",{\"1\":{\"9\":1,\"19\":1}}],[\"learning\",{\"0\":{\"55\":1},\"1\":{\"8\":2,\"28\":5,\"36\":1,\"37\":3,\"52\":5,\"57\":4,\"58\":3,\"61\":1,\"66\":1,\"68\":3,\"76\":1,\"89\":3},\"2\":{\"30\":1,\"70\":1,\"91\":1}}],[\"l=5\",{\"1\":{\"6\":1}}],[\"l\",{\"1\":{\"6\":1,\"8\":1,\"12\":1,\"13\":1,\"41\":1,\"76\":1,\"78\":1,\"79\":1,\"81\":2,\"83\":2}}],[\"local\",{\"1\":{\"36\":1}}],[\"log\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"loss\",{\"0\":{\"8\":1},\"1\":{\"5\":1,\"8\":5,\"12\":4,\"41\":1,\"59\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":3,\"81\":1,\"83\":1,\"87\":1}}],[\"long\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"17\":4,\"23\":2,\"25\":1}}],[\"hasselt\",{\"1\":{\"77\":1}}],[\"hado\",{\"1\":{\"77\":1}}],[\"hard\",{\"1\":{\"5\":1,\"10\":1}}],[\"hneat\",{\"1\":{\"67\":2}}],[\"hns=humanscore​−randomscore​agentscore​−randomscore​​\",{\"1\":{\"21\":1}}],[\"hns\",{\"1\":{\"21\":3}}],[\"history\",{\"1\":{\"63\":1}}],[\"hidden\",{\"1\":{\"60\":1}}],[\"high\",{\"0\":{\"18\":1},\"1\":{\"24\":2,\"51\":1,\"57\":1}}],[\"hsuan\",{\"1\":{\"36\":2,\"43\":1}}],[\"ht−1​\",{\"1\":{\"19\":1}}],[\"hyperparameters\",{\"1\":{\"50\":1}}],[\"hyperparameter\",{\"1\":{\"16\":1,\"21\":1,\"43\":1}}],[\"horgan\",{\"1\":{\"19\":1}}],[\"horizon\",{\"1\":{\"14\":1}}],[\"hot\",{\"1\":{\"12\":1}}],[\"hμ\",{\"1\":{\"12\":1}}],[\"h−1\",{\"1\":{\"8\":3}}],[\"h\",{\"1\":{\"8\":2,\"12\":4,\"41\":2,\"79\":2}}],[\"human\",{\"0\":{\"1\":1},\"1\":{\"21\":2,\"24\":2,\"27\":1,\"28\":1,\"67\":1,\"86\":2}}],[\"uda\",{\"0\":{\"40\":1},\"1\":{\"37\":4,\"40\":2,\"51\":1}}],[\"unity\",{\"1\":{\"47\":1}}],[\"university\",{\"1\":{\"32\":1}}],[\"universal\",{\"1\":{\"7\":1}}],[\"unlabelled\",{\"1\":{\"40\":1}}],[\"unlabeled\",{\"1\":{\"37\":3,\"38\":1}}],[\"unlebelled\",{\"1\":{\"40\":1}}],[\"unsupervised\",{\"1\":{\"37\":1}}],[\"uncapped\",{\"1\":{\"25\":1}}],[\"undiscounted\",{\"1\":{\"21\":1}}],[\"uk​\",{\"1\":{\"16\":1}}],[\"uk​<ϵucb​​\",{\"1\":{\"16\":1}}],[\"uk​≥ϵucb​∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"ucb\",{\"0\":{\"14\":1,\"15\":1,\"16\":1},\"1\":{\"14\":5,\"15\":2,\"16\":1,\"28\":1}}],[\"uvfa\",{\"0\":{\"7\":1},\"1\":{\"5\":1,\"7\":1}}],[\"upper\",{\"0\":{\"14\":1}}],[\"up\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"us\",{\"0\":{\"0\":1}}],[\"模型能夠得到的\",{\"1\":{\"66\":1}}],[\"模型還是能夠順利學習\",{\"1\":{\"24\":1}}],[\"模型\",{\"1\":{\"3\":1}}],[\"模型已經能夠在大多的\",{\"1\":{\"3\":1}}],[\"都有正面的影響\",{\"1\":{\"86\":1}}],[\"都需要\",{\"1\":{\"82\":1}}],[\"都不太好\",{\"1\":{\"49\":1}}],[\"都是會逐漸趨近於\",{\"1\":{\"87\":1}}],[\"都是要對\",{\"1\":{\"81\":1}}],[\"都是\",{\"1\":{\"48\":1,\"78\":1}}],[\"都是虛擬世界當中的影像\",{\"1\":{\"44\":1}}],[\"都是採用\",{\"1\":{\"13\":1}}],[\"都可以使用\",{\"1\":{\"43\":1}}],[\"都被其他\",{\"1\":{\"40\":1}}],[\"都會帶來\",{\"1\":{\"86\":1}}],[\"都會對到\",{\"1\":{\"47\":1}}],[\"都會特別大\",{\"1\":{\"37\":1}}],[\"都會接收同樣的\",{\"1\":{\"12\":1}}],[\"都能夠透過\",{\"1\":{\"37\":1}}],[\"都能夠學習什麼時候該\",{\"1\":{\"13\":1}}],[\"都獲得比\",{\"1\":{\"27\":1}}],[\"都另外加上一個\",{\"1\":{\"21\":1}}],[\"都當成是工廠生產出來的機器人\",{\"1\":{\"13\":1}}],[\"都\",{\"1\":{\"3\":1}}],[\"希望改善這兩個對\",{\"1\":{\"3\":1}}],[\"也並不是每次加上\",{\"1\":{\"86\":1}}],[\"也可以達到類似的效果\",{\"1\":{\"82\":1}}],[\"也確實發現會平滑許多\",{\"1\":{\"66\":1}}],[\"也避免了上述提及的幾個問題\",{\"1\":{\"57\":1}}],[\"也跟最後評估的\",{\"1\":{\"50\":1}}],[\"也有部分是源自於這樣的相似性帶來的好處\",{\"1\":{\"36\":1}}],[\"也獲得不錯的成果\",{\"1\":{\"34\":1}}],[\"也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的\",{\"1\":{\"87\":1}}],[\"也說明了\",{\"1\":{\"25\":1}}],[\"也限制了數值範圍\",{\"1\":{\"21\":1}}],[\"也因為如此\",{\"1\":{\"13\":1}}],[\"也會依據得到的\",{\"1\":{\"13\":1}}],[\"也就不需要再使用\",{\"1\":{\"83\":1,\"84\":1}}],[\"也就意味著需要\",{\"1\":{\"82\":1}}],[\"也就如下\",{\"1\":{\"79\":1}}],[\"也就是要讓底下的\",{\"1\":{\"59\":1}}],[\"也就是要找到\",{\"1\":{\"59\":1}}],[\"也就是在\",{\"1\":{\"59\":1}}],[\"也就是\",{\"1\":{\"17\":1,\"77\":1,\"79\":1}}],[\"也就是讓底下的期望值最大化\",{\"1\":{\"14\":1}}],[\"也就是說最後的\",{\"1\":{\"87\":1}}],[\"也就是說對於一個參數\",{\"1\":{\"81\":1}}],[\"也就是說理想上每經過一輪更新\",{\"1\":{\"66\":1}}],[\"也就是說可以直接從\",{\"1\":{\"62\":1}}],[\"也就是說我們對於\",{\"1\":{\"37\":1}}],[\"也就是說我現在面前有\",{\"1\":{\"14\":1}}],[\"也就是說能夠順利到達\",{\"1\":{\"22\":1}}],[\"也就是說\",{\"1\":{\"14\":1,\"59\":1,\"87\":1}}],[\"也就是說這種做法的正確性是被確保的\",{\"1\":{\"12\":1}}],[\"也就是前面定義的\",{\"1\":{\"6\":1}}],[\"也就能夠得到\",{\"1\":{\"8\":1}}],[\"也提出了一個可以在所有\",{\"1\":{\"3\":1}}],[\"也許才有機會遇到\",{\"1\":{\"3\":1}}],[\"也需要嘗試越過那些障礙\",{\"1\":{\"3\":1}}],[\"的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同\",{\"1\":{\"87\":1}}],[\"的數量\",{\"1\":{\"84\":2}}],[\"的形式\",{\"1\":{\"83\":2}}],[\"的計算\",{\"1\":{\"81\":1}}],[\"的想法基本上是相同方向\",{\"1\":{\"81\":1}}],[\"的想法跟\",{\"1\":{\"81\":1}}],[\"的算式\",{\"1\":{\"79\":1}}],[\"的影響力\",{\"1\":{\"79\":1}}],[\"的影響程度\",{\"1\":{\"6\":1,\"79\":1}}],[\"的論文\",{\"1\":{\"79\":2}}],[\"的概念就如同火影忍者的影分身之術\",{\"1\":{\"79\":1}}],[\"的概念仍然是透過\",{\"1\":{\"78\":1}}],[\"的總和就能夠得到\",{\"1\":{\"78\":1}}],[\"的決定上採用了\",{\"1\":{\"76\":1,\"78\":1}}],[\"的限制\",{\"1\":{\"75\":1,\"78\":1}}],[\"的效果\",{\"1\":{\"75\":1}}],[\"的亂度越高越好\",{\"1\":{\"75\":1}}],[\"的時候都是透過增加\",{\"1\":{\"75\":1}}],[\"的各種\",{\"1\":{\"74\":1}}],[\"的成功\",{\"1\":{\"67\":1}}],[\"的成功也放進\",{\"1\":{\"57\":1}}],[\"的位置以及類型\",{\"1\":{\"67\":1}}],[\"的平均去評估\",{\"1\":{\"66\":1}}],[\"的好壞就相對困難\",{\"1\":{\"66\":1}}],[\"的好壞\",{\"1\":{\"66\":1}}],[\"的目的是要讓整體的\",{\"1\":{\"59\":1}}],[\"的目標就是要讓整體的\",{\"1\":{\"59\":1}}],[\"的目標是把兩個不同分佈的\",{\"1\":{\"33\":1}}],[\"的定義如下\",{\"1\":{\"59\":1,\"83\":1}}],[\"的改變而有巨大幅度的變化\",{\"1\":{\"57\":1,\"59\":1,\"63\":1}}],[\"的訓練資料具有高度相關性\",{\"1\":{\"57\":1,\"63\":1}}],[\"的訓練資料\",{\"1\":{\"57\":1}}],[\"的角度來看\",{\"1\":{\"57\":1}}],[\"的輸入去學習一直是一個很大的挑戰\",{\"1\":{\"57\":1}}],[\"的感官資料\",{\"1\":{\"57\":1}}],[\"的關聯性就能被連結起來\",{\"1\":{\"41\":1}}],[\"的核心做法是不單只是跟\",{\"1\":{\"41\":1}}],[\"的步驟\",{\"1\":{\"38\":1}}],[\"的一種\",{\"1\":{\"38\":1}}],[\"的技巧\",{\"1\":{\"38\":1,\"65\":1}}],[\"的例子\",{\"1\":{\"37\":1}}],[\"的認識\",{\"1\":{\"37\":1}}],[\"的資料分佈會隨著\",{\"1\":{\"57\":1,\"59\":1,\"63\":1}}],[\"的資料通常都會先\",{\"1\":{\"57\":1}}],[\"的資料不存在任何\",{\"1\":{\"37\":1}}],[\"的資料上只有一些\",{\"1\":{\"37\":1}}],[\"的方式\",{\"1\":{\"79\":1}}],[\"的方式是採用\",{\"1\":{\"8\":1}}],[\"的方法上雖然任何\",{\"1\":{\"43\":1}}],[\"的方法\",{\"1\":{\"40\":1,\"61\":1,\"73\":1,\"87\":1}}],[\"的方法來降低這種問題\",{\"1\":{\"37\":1}}],[\"的方法解決了\",{\"1\":{\"37\":1}}],[\"的不同\",{\"1\":{\"36\":1}}],[\"的預測結果要接近\",{\"1\":{\"41\":1}}],[\"的預測結果\",{\"1\":{\"36\":1}}],[\"的四個缺陷\",{\"1\":{\"25\":1}}],[\"的比較當中明顯看到在所有的成績都有所提升\",{\"1\":{\"25\":1}}],[\"的比例改變\",{\"1\":{\"13\":1}}],[\"的普遍性\",{\"1\":{\"25\":1}}],[\"的優劣\",{\"1\":{\"25\":1}}],[\"的實驗\",{\"1\":{\"24\":1}}],[\"的重要性\",{\"1\":{\"22\":1}}],[\"的趨勢仍然是隨著\",{\"1\":{\"22\":1}}],[\"的狀況缺乏認知\",{\"1\":{\"34\":1}}],[\"的狀況下\",{\"1\":{\"22\":1}}],[\"的狀況\",{\"1\":{\"22\":2}}],[\"的做法就是照著\",{\"1\":{\"40\":1}}],[\"的做法之所以能夠成功\",{\"1\":{\"36\":1}}],[\"的做法\",{\"1\":{\"22\":1,\"82\":1}}],[\"的缺陷\",{\"1\":{\"22\":1}}],[\"的設計上也相當直覺\",{\"1\":{\"41\":1}}],[\"的設計是採用\",{\"1\":{\"17\":1}}],[\"的設定上對於目標被發現存在高估的問題\",{\"1\":{\"77\":1}}],[\"的設定上參考了許多過去的研究\",{\"1\":{\"43\":1}}],[\"的設定基本上跟\",{\"1\":{\"43\":1}}],[\"的設定下會大程度影響到最終\",{\"1\":{\"22\":1}}],[\"的設定取得的\",{\"1\":{\"22\":1}}],[\"的設定會透過\",{\"1\":{\"22\":1}}],[\"的設定詳閱論文的\",{\"1\":{\"21\":1}}],[\"的話會導致\",{\"1\":{\"61\":1}}],[\"的話\",{\"1\":{\"17\":1,\"57\":1}}],[\"的選項有更高機率被選擇到\",{\"1\":{\"14\":1}}],[\"的選擇根據\",{\"1\":{\"19\":1}}],[\"的選擇應遠比\",{\"1\":{\"15\":1}}],[\"的選擇\",{\"1\":{\"10\":1}}],[\"的傾向\",{\"1\":{\"13\":1}}],[\"的存在\",{\"1\":{\"13\":1}}],[\"的機率\",{\"1\":{\"13\":1}}],[\"的版本是少了\",{\"1\":{\"47\":1}}],[\"的版本\",{\"1\":{\"12\":1,\"49\":1,\"59\":1}}],[\"的模型都跟\",{\"1\":{\"86\":1}}],[\"的模型萃取出圖片的特徵\",{\"1\":{\"62\":1}}],[\"的模型得出來的結果\",{\"1\":{\"59\":1}}],[\"的模型\",{\"1\":{\"48\":1}}],[\"的模型對於\",{\"1\":{\"34\":1}}],[\"的模型雖然有許多\",{\"1\":{\"34\":1}}],[\"的模型會盡可能避開\",{\"1\":{\"22\":1}}],[\"的模型變成底下的樣子\",{\"1\":{\"12\":1}}],[\"的模型當成最後的結果\",{\"1\":{\"9\":1}}],[\"的參數加上\",{\"1\":{\"81\":1}}],[\"的參數\",{\"1\":{\"12\":2,\"79\":1}}],[\"的部分在退步也是有幾項退步蠻多\",{\"1\":{\"86\":1}}],[\"的部分改用\",{\"1\":{\"83\":1,\"84\":1}}],[\"的部分也可以發現到兩者的發展方向會稍有不同\",{\"1\":{\"22\":1}}],[\"的部分是分別給\",{\"1\":{\"12\":1}}],[\"的部分目的也是希望能夠促使\",{\"1\":{\"6\":1}}],[\"的作者認為是因為\",{\"1\":{\"10\":1}}],[\"的大小相差越來越懸殊\",{\"1\":{\"22\":1}}],[\"的大小下\",{\"1\":{\"22\":1}}],[\"的大小\",{\"1\":{\"10\":1,\"22\":1,\"65\":1}}],[\"的問題似乎能夠得到改善\",{\"1\":{\"17\":1}}],[\"的問題\",{\"0\":{\"10\":1},\"1\":{\"10\":1,\"17\":1}}],[\"的\",{\"1\":{\"8\":2,\"10\":1,\"12\":7,\"13\":2,\"16\":1,\"22\":2,\"24\":1,\"36\":2,\"41\":1,\"43\":1,\"46\":1,\"47\":1,\"49\":1,\"57\":1,\"63\":2,\"73\":1,\"75\":1,\"76\":2,\"79\":1,\"81\":2,\"87\":2}}],[\"的分布上有做了一點調整\",{\"1\":{\"21\":1}}],[\"的分布也會變動\",{\"1\":{\"15\":1}}],[\"的分布會變動的話\",{\"1\":{\"15\":1}}],[\"的分布是固定的狀況下會使用\",{\"1\":{\"14\":1}}],[\"的分布\",{\"1\":{\"7\":1}}],[\"的研究\",{\"1\":{\"6\":1}}],[\"的範圍\",{\"1\":{\"6\":1}}],[\"的環境當中有更好的成效\",{\"1\":{\"5\":1}}],[\"的地方\",{\"1\":{\"3\":1}}],[\"的結果要接近\",{\"1\":{\"41\":1}}],[\"的結果就當作是他的\",{\"1\":{\"37\":1}}],[\"的結果\",{\"1\":{\"3\":1,\"22\":1}}],[\"是把一個可訓練參數拆成\",{\"1\":{\"87\":1}}],[\"是為了採用底下的特性方便後續\",{\"1\":{\"81\":1}}],[\"是使用\",{\"1\":{\"79\":1}}],[\"是被固定的參數\",{\"1\":{\"76\":1}}],[\"是上一個\",{\"1\":{\"76\":1}}],[\"是只有使用\",{\"1\":{\"48\":1}}],[\"是源自於即便\",{\"1\":{\"36\":1}}],[\"是每個\",{\"1\":{\"13\":1}}],[\"是在\",{\"1\":{\"12\":2,\"75\":1,\"83\":1,\"84\":1}}],[\"是一種\",{\"1\":{\"38\":1}}],[\"是一樣的\",{\"1\":{\"12\":1,\"50\":1}}],[\"是一個\",{\"1\":{\"16\":3}}],[\"是一個可以用來評估或是用在\",{\"1\":{\"8\":1}}],[\"是一個相當重要的\",{\"1\":{\"3\":1}}],[\"是不同的\",{\"1\":{\"6\":1}}],[\"是\",{\"1\":{\"6\":2,\"8\":1,\"43\":1,\"62\":1,\"84\":2}}],[\"是因為即便是在很多\",{\"1\":{\"3\":1}}],[\"正確\",{\"1\":{\"3\":1}}],[\"或是汽車比卡車更常見\",{\"1\":{\"37\":1}}],[\"或是\",{\"1\":{\"3\":1,\"14\":1,\"61\":1,\"79\":1}}],[\"truncate\",{\"1\":{\"86\":1}}],[\"train\",{\"1\":{\"48\":2}}],[\"training\",{\"0\":{\"37\":1},\"1\":{\"38\":2,\"45\":1,\"46\":1,\"47\":1,\"52\":1}}],[\"transition\",{\"1\":{\"76\":1}}],[\"transformed\",{\"1\":{\"8\":2,\"12\":3}}],[\"tranheden\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"trajectory\",{\"1\":{\"19\":1}}],[\"trajectories\",{\"1\":{\"8\":1,\"12\":1,\"19\":1}}],[\"trace\",{\"1\":{\"17\":4,\"23\":4,\"25\":1}}],[\"table\",{\"1\":{\"76\":1}}],[\"target\",{\"1\":{\"8\":3,\"12\":1,\"33\":3,\"34\":1,\"36\":2,\"37\":7,\"40\":2,\"41\":4,\"59\":1,\"63\":1,\"68\":1,\"76\":1}}],[\"t\",{\"1\":{\"59\":1,\"63\":1,\"79\":1}}],[\"td\",{\"0\":{\"60\":1},\"1\":{\"58\":1,\"61\":1,\"63\":1}}],[\"to\",{\"0\":{\"40\":1},\"1\":{\"40\":1,\"44\":1,\"50\":2,\"51\":2,\"52\":2,\"62\":1,\"89\":1}}],[\"tune\",{\"1\":{\"37\":1}}],[\"temporal\",{\"1\":{\"89\":1}}],[\"tensorflow\",{\"1\":{\"89\":1}}],[\"testset\",{\"1\":{\"50\":1}}],[\"technology\",{\"1\":{\"32\":1}}],[\"term\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"tsai\",{\"1\":{\"36\":2,\"43\":1}}],[\"ts\",{\"1\":{\"28\":1}}],[\"time\",{\"0\":{\"17\":1,\"23\":1}}],[\"tips\",{\"1\":{\"8\":1,\"12\":1,\"13\":1,\"16\":1,\"17\":1,\"22\":1,\"36\":1,\"40\":1,\"75\":1,\"76\":1,\"77\":1,\"78\":1,\"79\":1,\"81\":1}}],[\"thread\",{\"1\":{\"83\":1,\"84\":1}}],[\"through\",{\"0\":{\"17\":1,\"23\":1}}],[\"thq\",{\"1\":{\"8\":1}}],[\"thelimeydragon\",{\"1\":{\"3\":1}}],[\"the\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1}}],[\"t^q\",{\"1\":{\"8\":1}}],[\"t∈n​\",{\"1\":{\"8\":1}}],[\"找到寶藏可以加分\",{\"1\":{\"3\":1}}],[\"過高\",{\"1\":{\"33\":1}}],[\"過去增加探索的方法大多都是在\",{\"1\":{\"75\":1}}],[\"過去會透過多次遊戲中\",{\"1\":{\"66\":1}}],[\"過去在\",{\"1\":{\"57\":1}}],[\"過去對於\",{\"1\":{\"14\":1}}],[\"過去的研究當中發現到如果是\",{\"1\":{\"61\":1}}],[\"過去的\",{\"1\":{\"3\":1}}],[\"過程中有許多陷阱\",{\"1\":{\"3\":1}}],[\"分布相當不同時\",{\"1\":{\"10\":1}}],[\"分成了兩個部分\",{\"1\":{\"6\":1}}],[\"分鐘的時間探索\",{\"1\":{\"3\":1}}],[\"分別表示\",{\"1\":{\"79\":1}}],[\"分別落在哪個\",{\"1\":{\"24\":1}}],[\"分別用\",{\"1\":{\"23\":1}}],[\"分別去針對\",{\"1\":{\"12\":1}}],[\"分別是\",{\"1\":{\"6\":1}}],[\"分別是這些遊戲\",{\"1\":{\"3\":1}}],[\"分別在\",{\"1\":{\"3\":1}}],[\"玩家要操作主角在\",{\"1\":{\"3\":1}}],[\"玩家要操作角色滑雪\",{\"1\":{\"3\":1}}],[\"number\",{\"0\":{\"82\":1},\"1\":{\"82\":2}}],[\"nfq\",{\"0\":{\"62\":1},\"1\":{\"58\":1,\"62\":2}}],[\"naive\",{\"0\":{\"40\":1},\"1\":{\"40\":2}}],[\"nk​\",{\"1\":{\"14\":2,\"15\":2}}],[\"n−1\",{\"1\":{\"14\":1,\"16\":1}}],[\"n\",{\"1\":{\"14\":1,\"63\":1}}],[\"nn\",{\"1\":{\"8\":3,\"10\":1,\"12\":3,\"22\":1}}],[\"ngu+sep\",{\"1\":{\"24\":1}}],[\"ngu\",{\"0\":{\"9\":1,\"10\":1},\"1\":{\"5\":3,\"7\":2,\"8\":1,\"9\":2,\"10\":2,\"12\":2,\"13\":1,\"16\":1,\"17\":1,\"19\":1,\"22\":5,\"24\":1,\"25\":2}}],[\"net\",{\"1\":{\"74\":1,\"81\":1,\"83\":2}}],[\"networks\",{\"0\":{\"59\":1,\"71\":1},\"1\":{\"24\":1,\"25\":1,\"27\":1,\"28\":1,\"56\":1,\"58\":1}}],[\"network\",{\"1\":{\"8\":4,\"12\":3,\"22\":6,\"24\":2,\"36\":2,\"41\":1,\"43\":1,\"59\":4,\"61\":2,\"63\":4,\"66\":1,\"68\":1,\"73\":1,\"76\":4,\"78\":1,\"81\":2,\"83\":2,\"84\":2}}],[\"next\",{\"1\":{\"63\":1}}],[\"neurips\",{\"1\":{\"56\":1},\"2\":{\"70\":1}}],[\"neural\",{\"1\":{\"28\":1,\"58\":1,\"59\":1,\"63\":1,\"73\":1,\"76\":3,\"78\":1}}],[\"needs\",{\"1\":{\"52\":1}}],[\"need\",{\"1\":{\"52\":1}}],[\"never\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"negative\",{\"1\":{\"3\":3,\"65\":1}}],[\"noise\",{\"0\":{\"75\":1},\"1\":{\"74\":1,\"75\":12,\"81\":8,\"82\":3,\"83\":1,\"84\":4,\"87\":2,\"89\":2}}],[\"noisynet\",{\"1\":{\"84\":1,\"86\":6,\"87\":1}}],[\"noisy\",{\"0\":{\"71\":1},\"1\":{\"74\":1,\"81\":1,\"83\":3,\"84\":1}}],[\"non\",{\"1\":{\"61\":1}}],[\"normalized\",{\"1\":{\"21\":2}}],[\"notes\",{\"1\":{\"52\":1}}],[\"note\",{\"1\":{\"12\":1,\"79\":2,\"86\":1},\"2\":{\"29\":1,\"53\":1,\"69\":1,\"90\":1}}],[\"novelty\",{\"1\":{\"6\":1}}],[\"noveltyαt​\",{\"1\":{\"6\":1}}],[\"noveltyrtepisodic​\",{\"1\":{\"6\":1}}],[\"no\",{\"1\":{\"3\":1}}],[\"又有不少的\",{\"1\":{\"3\":1}}],[\"秒的\",{\"1\":{\"3\":1}}],[\"就應該隨著訓練慢慢被忽視\",{\"1\":{\"87\":1}}],[\"就都是用這個\",{\"1\":{\"81\":1}}],[\"就只是這樣而已\",{\"1\":{\"81\":1}}],[\"就小到幾乎不存在了\",{\"1\":{\"77\":1}}],[\"就比較有系統性一些\",{\"1\":{\"75\":1}}],[\"就比較像是在亂試\",{\"1\":{\"75\":1}}],[\"就像是\",{\"1\":{\"78\":1}}],[\"就像是可以換個角度去想其他人會怎麼做\",{\"1\":{\"75\":1}}],[\"就像是獵人裡面的凱特\",{\"1\":{\"75\":1}}],[\"就很不相同\",{\"1\":{\"66\":1}}],[\"就持續上一個做出的\",{\"1\":{\"65\":1}}],[\"就定義成\",{\"1\":{\"63\":1}}],[\"就能夠比較好發揮作用\",{\"1\":{\"38\":1}}],[\"就相當地雷同\",{\"1\":{\"36\":1}}],[\"就是在\",{\"1\":{\"75\":1}}],[\"就是用\",{\"1\":{\"59\":1}}],[\"就是用來描述一群資料他們的分布狀況\",{\"1\":{\"33\":1}}],[\"就是希望\",{\"1\":{\"41\":1}}],[\"就是\",{\"1\":{\"36\":1,\"40\":1}}],[\"就會導致互相的不理解\",{\"1\":{\"34\":1}}],[\"就會導致單純在\",{\"1\":{\"33\":1}}],[\"就會因為\",{\"1\":{\"13\":1}}],[\"就會去環境當中互動\",{\"1\":{\"13\":1}}],[\"就會多\",{\"1\":{\"3\":1}}],[\"就有不同重要程度了\",{\"1\":{\"13\":1}}],[\"就做得頗差\",{\"1\":{\"10\":1}}],[\"就可以再拿去\",{\"1\":{\"37\":1}}],[\"就可以得到相當好的影像分割結果\",{\"1\":{\"34\":1}}],[\"就可以得到單純\",{\"1\":{\"9\":1}}],[\"就可以透過\",{\"1\":{\"8\":1}}],[\"就跟\",{\"1\":{\"8\":1}}],[\"就通常完全沒辦法學習\",{\"1\":{\"3\":1}}],[\"途中要盡可能快速通過指定數量的\",{\"1\":{\"3\":1}}],[\"這跟前面提到只使用\",{\"1\":{\"40\":1}}],[\"這種相似的\",{\"1\":{\"40\":1}}],[\"這種\",{\"1\":{\"38\":2}}],[\"這種狀況下訓練模型就被稱為半監督式學習\",{\"1\":{\"37\":1}}],[\"這種差距被描述為\",{\"1\":{\"33\":1}}],[\"這類的\",{\"1\":{\"36\":1}}],[\"這樣的想法自然而然就出現了\",{\"1\":{\"57\":1}}],[\"這樣的問題只在\",{\"1\":{\"40\":1}}],[\"這樣的做法下每一個\",{\"1\":{\"82\":1}}],[\"這樣的做法有趣的是能夠將\",{\"1\":{\"38\":1}}],[\"這樣的做法之所以可行\",{\"1\":{\"36\":1}}],[\"這樣所需要的成本會過大\",{\"1\":{\"33\":1}}],[\"這就像是同理心\",{\"1\":{\"34\":1}}],[\"這一款遊戲\",{\"1\":{\"23\":1}}],[\"這一篇論文成功將\",{\"1\":{\"57\":1}}],[\"這一篇\",{\"1\":{\"3\":1}}],[\"這個參數會漸漸趨近於\",{\"1\":{\"87\":1}}],[\"這個論文提出的做法稱為\",{\"1\":{\"56\":1}}],[\"這個\",{\"1\":{\"50\":1}}],[\"這個結果如果在取得\",{\"1\":{\"22\":1}}],[\"這個測量標準比較強調那些\",{\"1\":{\"21\":1}}],[\"這個問題\",{\"1\":{\"13\":1}}],[\"這裡考慮有\",{\"1\":{\"59\":1}}],[\"這裡要來實驗這一個做法實際上帶來多少影響\",{\"1\":{\"22\":1}}],[\"這裡就不贅述\",{\"1\":{\"21\":1}}],[\"這裡的經驗指的是一個\",{\"1\":{\"15\":1}}],[\"這些普遍做得不錯的\",{\"1\":{\"48\":1}}],[\"這些\",{\"1\":{\"13\":1}}],[\"這兩個問題\",{\"1\":{\"16\":1}}],[\"這兩者分別會讓\",{\"1\":{\"6\":1}}],[\"這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到\",{\"1\":{\"3\":1}}],[\"這款遊戲來說\",{\"1\":{\"3\":2}}],[\"以達成\",{\"1\":{\"75\":1}}],[\"以達到更好的訓練成效\",{\"1\":{\"12\":1}}],[\"以上\",{\"1\":{\"22\":1}}],[\"以\",{\"1\":{\"3\":2}}],[\"以及加上\",{\"1\":{\"86\":1}}],[\"以及沒有的狀況\",{\"1\":{\"24\":1}}],[\"以及對應的\",{\"1\":{\"23\":1}}],[\"以及最傾向\",{\"1\":{\"22\":1}}],[\"以及一個\",{\"1\":{\"22\":1}}],[\"以及\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"12\":2,\"22\":2,\"23\":2,\"24\":1,\"33\":1,\"36\":2,\"40\":1,\"41\":1,\"44\":1,\"63\":2,\"65\":1,\"68\":2,\"79\":3,\"86\":2,\"88\":1}}],[\"v\",{\"1\":{\"78\":5}}],[\"volodymyr\",{\"1\":{\"56\":1,\"63\":1,\"66\":1,\"67\":1}}],[\"volvo\",{\"1\":{\"32\":1}}],[\"variational\",{\"1\":{\"89\":1}}],[\"varied\",{\"1\":{\"52\":1}}],[\"van\",{\"1\":{\"77\":1}}],[\"validation\",{\"1\":{\"50\":4,\"66\":2}}],[\"value\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"19\":1,\"59\":1,\"60\":1,\"63\":1,\"76\":2,\"78\":1,\"79\":2,\"89\":1}}],[\"v2\",{\"1\":{\"43\":1,\"48\":1}}],[\"vime\",{\"1\":{\"89\":1}}],[\"visual\",{\"1\":{\"62\":1}}],[\"vision\",{\"1\":{\"32\":1,\"52\":1,\"62\":1},\"2\":{\"54\":1}}],[\"virtual\",{\"1\":{\"47\":1}}],[\"viktor\",{\"1\":{\"38\":2}}],[\"via\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"52\":2,\"89\":1}}],[\"videos\",{\"1\":{\"75\":1}}],[\"video\",{\"1\":{\"3\":2}}],[\"veg\",{\"1\":{\"48\":1}}],[\"vector\",{\"1\":{\"12\":1}}],[\"venture\",{\"1\":{\"3\":1,\"24\":1}}],[\"2​\",{\"1\":{\"84\":1}}],[\"2∣st+i​\",{\"1\":{\"79\":1,\"84\":1}}],[\"25\",{\"1\":{\"52\":1}}],[\"255\",{\"1\":{\"3\":1}}],[\"24\",{\"1\":{\"52\":1}}],[\"24966\",{\"1\":{\"46\":1}}],[\"22480\",{\"1\":{\"24\":1}}],[\"21\",{\"1\":{\"24\":1}}],[\"26\",{\"1\":{\"24\":2}}],[\"2600\",{\"1\":{\"3\":2}}],[\"2975\",{\"1\":{\"45\":1}}],[\"29\",{\"1\":{\"24\":1}}],[\"2ϵ1+4ϵ\",{\"1\":{\"8\":1}}],[\"2\",{\"1\":{\"8\":1,\"12\":1,\"19\":1,\"59\":1,\"76\":1,\"78\":1,\"79\":1,\"83\":4}}],[\"2017\",{\"1\":{\"75\":1}}],[\"2014\",{\"1\":{\"73\":1}}],[\"2013\",{\"1\":{\"56\":1}}],[\"2016\",{\"1\":{\"45\":1,\"47\":1}}],[\"2015\",{\"1\":{\"34\":1,\"63\":1,\"66\":1,\"67\":1,\"76\":1,\"77\":1,\"78\":1}}],[\"2018\",{\"1\":{\"19\":1,\"34\":1,\"36\":2,\"37\":1,\"43\":1,\"72\":1,\"86\":3,\"87\":1}}],[\"2021\",{\"1\":{\"32\":1,\"37\":1}}],[\"2020\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"32\":1,\"38\":2}}],[\"200\",{\"1\":{\"22\":1}}],[\"20\",{\"1\":{\"3\":1,\"22\":1}}],[\"olivier\",{\"1\":{\"73\":1}}],[\"olsson\",{\"1\":{\"38\":2}}],[\"object\",{\"1\":{\"67\":1}}],[\"observation\",{\"1\":{\"19\":1}}],[\"openai\",{\"1\":{\"75\":3,\"81\":1}}],[\"operation\",{\"1\":{\"8\":1}}],[\"operator\",{\"1\":{\"8\":1,\"12\":1}}],[\"optimal\",{\"1\":{\"59\":1,\"76\":1,\"78\":1}}],[\"output\",{\"1\":{\"52\":1}}],[\"outperform\",{\"1\":{\"3\":2,\"60\":1,\"67\":1}}],[\"outperforming\",{\"0\":{\"1\":1},\"1\":{\"28\":1}}],[\"or\",{\"0\":{\"37\":1},\"1\":{\"47\":1}}],[\"off\",{\"1\":{\"28\":1,\"61\":1,\"79\":2,\"88\":1}}],[\"of\",{\"0\":{\"13\":1},\"1\":{\"32\":2}}],[\"overhead\",{\"1\":{\"83\":1,\"84\":1}}],[\"over\",{\"0\":{\"13\":1}}],[\"one\",{\"1\":{\"12\":1}}],[\"online\",{\"1\":{\"8\":1,\"12\":1,\"79\":1}}],[\"on\",{\"1\":{\"3\":2,\"32\":1,\"43\":2,\"51\":1,\"52\":1,\"67\":1,\"79\":1,\"88\":1}}],[\"eπ\",{\"1\":{\"84\":1}}],[\"eπ​\",{\"1\":{\"14\":1}}],[\"e\",{\"1\":{\"83\":2}}],[\"e2​\",{\"1\":{\"63\":1}}],[\"en​\",{\"1\":{\"63\":1}}],[\"enduro\",{\"1\":{\"65\":1}}],[\"end\",{\"1\":{\"62\":2}}],[\"entropy\",{\"1\":{\"41\":1,\"73\":1,\"74\":1,\"75\":1,\"79\":3,\"84\":1}}],[\"evolution\",{\"1\":{\"89\":1}}],[\"everything\",{\"1\":{\"52\":1}}],[\"evaluation\",{\"0\":{\"50\":1}}],[\"evaluator\",{\"1\":{\"21\":1}}],[\"early\",{\"1\":{\"50\":1}}],[\"efficient\",{\"1\":{\"28\":1}}],[\"experiments\",{\"0\":{\"86\":1}}],[\"experience\",{\"1\":{\"9\":2,\"10\":1,\"13\":4,\"16\":1,\"19\":2,\"28\":2,\"63\":9,\"68\":1,\"76\":2,\"79\":1}}],[\"explore\",{\"1\":{\"13\":1}}],[\"exploration\",{\"0\":{\"13\":1,\"24\":1,\"71\":1,\"75\":1},\"1\":{\"3\":1,\"5\":1,\"6\":2,\"7\":1,\"10\":2,\"13\":1,\"14\":1,\"22\":5,\"28\":1,\"73\":2,\"74\":1,\"75\":5,\"78\":1,\"79\":1,\"81\":1,\"87\":1,\"88\":1,\"89\":4}}],[\"exploit\",{\"1\":{\"13\":1}}],[\"exploitation\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"13\":1,\"14\":1,\"22\":5}}],[\"extrinsic\",{\"1\":{\"6\":1,\"9\":1,\"12\":3,\"19\":1,\"22\":4}}],[\"episode\",{\"1\":{\"6\":2,\"13\":1,\"19\":2,\"21\":1,\"22\":3,\"57\":1,\"81\":2,\"82\":1,\"86\":1}}],[\"eye\",{\"1\":{\"3\":2,\"24\":1}}],[\"et​=\",{\"1\":{\"63\":1}}],[\"et\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"43\":1,\"45\":1,\"46\":1,\"47\":1,\"56\":1,\"63\":1,\"66\":1,\"67\":1,\"72\":1,\"76\":1,\"78\":1,\"86\":3,\"87\":1}}],[\"mdp\",{\"1\":{\"59\":1}}],[\"mnih\",{\"1\":{\"56\":1,\"63\":1,\"66\":1,\"67\":1}}],[\"mscoco\",{\"1\":{\"43\":1}}],[\"m\",{\"1\":{\"38\":3}}],[\"monte\",{\"1\":{\"81\":1}}],[\"montezuma\",{\"1\":{\"3\":2,\"24\":1}}],[\"mohammad\",{\"1\":{\"72\":1,\"86\":3,\"87\":1}}],[\"model\",{\"1\":{\"37\":1,\"48\":1,\"59\":1,\"60\":1,\"61\":1,\"77\":1}}],[\"miou\",{\"1\":{\"49\":1}}],[\"mix\",{\"1\":{\"41\":1}}],[\"mixup\",{\"1\":{\"38\":1}}],[\"mixing\",{\"0\":{\"38\":1,\"40\":1},\"1\":{\"38\":3,\"40\":2,\"41\":2,\"43\":2}}],[\"mixed\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"40\":1,\"52\":1}}],[\"min\",{\"1\":{\"6\":1,\"15\":1,\"21\":1}}],[\"median\",{\"1\":{\"86\":1}}],[\"medium\",{\"1\":{\"33\":2,\"79\":1}}],[\"meire\",{\"1\":{\"72\":1,\"86\":3,\"87\":1}}],[\"memory\",{\"1\":{\"63\":2,\"79\":1}}],[\"mean\",{\"1\":{\"25\":3,\"81\":2,\"86\":1}}],[\"methods\",{\"1\":{\"89\":1}}],[\"method\",{\"1\":{\"51\":1}}],[\"methodology\",{\"0\":{\"11\":1,\"39\":1,\"63\":1,\"80\":1}}],[\"meta\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":8,\"25\":2,\"27\":1}}],[\"matthieu\",{\"1\":{\"73\":1}}],[\"marius\",{\"1\":{\"45\":1}}],[\"mask\",{\"1\":{\"38\":3,\"43\":1}}],[\"majchrowska\",{\"1\":{\"37\":1}}],[\"map\",{\"1\":{\"36\":1,\"38\":3}}],[\"maps\",{\"1\":{\"36\":2}}],[\"mab\",{\"1\":{\"14\":2,\"28\":1}}],[\"maximizing\",{\"1\":{\"89\":1}}],[\"maxb∈a​q\",{\"1\":{\"77\":1}}],[\"max​q\",{\"1\":{\"59\":1}}],[\"max\",{\"1\":{\"6\":2}}],[\"multi\",{\"1\":{\"14\":1,\"60\":1,\"84\":1}}],[\"muzero\",{\"1\":{\"3\":3,\"25\":3}}],[\"不要想太多\",{\"1\":{\"81\":1}}],[\"不難發現到確實都存在高估的狀況\",{\"1\":{\"77\":1}}],[\"不需要事先經過其他的分解\",{\"1\":{\"68\":1}}],[\"不需要看太遠\",{\"1\":{\"7\":1}}],[\"不是\",{\"1\":{\"62\":1}}],[\"不是那麼地\",{\"1\":{\"10\":1,\"16\":1}}],[\"不確定性低\",{\"1\":{\"14\":1}}],[\"不確定性高\",{\"1\":{\"14\":1}}],[\"不會被固定下來\",{\"1\":{\"13\":1}}],[\"不穩定\",{\"1\":{\"12\":1}}],[\"不過進步主要在\",{\"1\":{\"86\":1}}],[\"不過並沒有保證收斂\",{\"1\":{\"73\":1}}],[\"不過作者發現在他們的模型得出來的結果往往會是很不穩定的\",{\"1\":{\"66\":1}}],[\"不過\",{\"1\":{\"62\":1}}],[\"不過從\",{\"1\":{\"57\":1}}],[\"不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索\",{\"1\":{\"73\":1}}],[\"不過這裡的成功只停止在雙陸棋上\",{\"1\":{\"60\":1}}],[\"不過這裡最主要都是使用\",{\"1\":{\"43\":1}}],[\"不過這種情況下一個直覺的問題是\",{\"1\":{\"33\":1}}],[\"不過像是馬路\",{\"1\":{\"36\":1}}],[\"不過如果遇到新的\",{\"1\":{\"34\":1}}],[\"不過實際上訓練時因為拆開來訓練\",{\"1\":{\"12\":1}}],[\"不過在計算\",{\"1\":{\"12\":1}}],[\"不過可惜的是\",{\"1\":{\"3\":1}}],[\"不同的地方在於他並不是直接去學習\",{\"1\":{\"78\":1}}],[\"不同的地方在於\",{\"1\":{\"62\":1}}],[\"不同的環境下需要的\",{\"1\":{\"6\":1}}],[\"不同\",{\"1\":{\"10\":1,\"13\":2,\"36\":1}}],[\"不太好\",{\"1\":{\"3\":1,\"34\":1}}],[\"scalable\",{\"1\":{\"89\":1}}],[\"scorebaseline​\",{\"1\":{\"86\":1}}],[\"scorehuman​\",{\"1\":{\"86\":1}}],[\"scores\",{\"1\":{\"21\":2}}],[\"sw\",{\"1\":{\"48\":1,\"49\":1}}],[\"swear\",{\"1\":{\"3\":1}}],[\"skipping\",{\"1\":{\"65\":1}}],[\"skiiing\",{\"1\":{\"10\":1}}],[\"skiing\",{\"1\":{\"3\":5,\"24\":1}}],[\"sky\",{\"1\":{\"48\":1}}],[\"sb​\",{\"1\":{\"38\":2}}],[\"synthia\",{\"0\":{\"47\":1,\"49\":1},\"1\":{\"44\":2,\"49\":1,\"50\":1}}],[\"synthetic\",{\"1\":{\"44\":1,\"46\":1,\"47\":1}}],[\"synethic\",{\"1\":{\"33\":1,\"34\":1,\"36\":1}}],[\"sylwia\",{\"1\":{\"37\":1}}],[\"ssl\",{\"1\":{\"37\":1,\"51\":1}}],[\"sparse\",{\"1\":{\"57\":1}}],[\"space\",{\"0\":{\"75\":1},\"1\":{\"52\":1,\"65\":2,\"74\":1,\"75\":8,\"81\":2,\"89\":1}}],[\"spatial\",{\"1\":{\"36\":2}}],[\"sprechmann\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"shift\",{\"1\":{\"33\":3,\"37\":2,\"41\":1}}],[\"sarsa\",{\"1\":{\"67\":2}}],[\"sa​\",{\"1\":{\"38\":3}}],[\"sampling\",{\"0\":{\"31\":1,\"41\":1},\"1\":{\"52\":1}}],[\"sample\",{\"1\":{\"10\":1,\"12\":2,\"13\":1,\"16\":1,\"19\":1,\"81\":1}}],[\"sampled\",{\"1\":{\"8\":1}}],[\"safe\",{\"1\":{\"28\":1}}],[\"supervise\",{\"1\":{\"37\":2}}],[\"supervised\",{\"1\":{\"37\":1,\"52\":5}}],[\"summary\",{\"0\":{\"25\":1}}],[\"surround\",{\"1\":{\"24\":1}}],[\"small\",{\"1\":{\"23\":1}}],[\"seaquest\",{\"1\":{\"65\":1}}],[\"set\",{\"1\":{\"50\":5,\"66\":2}}],[\"settings\",{\"0\":{\"21\":1}}],[\"semi\",{\"1\":{\"37\":2,\"52\":5}}],[\"semantic\",{\"1\":{\"34\":1,\"36\":3,\"37\":1,\"38\":4,\"52\":4}}],[\"self\",{\"0\":{\"37\":1},\"1\":{\"52\":1}}],[\"segmentation\",{\"1\":{\"34\":1,\"36\":2,\"37\":1,\"38\":1,\"41\":1,\"43\":1,\"52\":5}}],[\"separate\",{\"1\":{\"22\":6,\"24\":1,\"25\":1,\"27\":1}}],[\"sequence\",{\"1\":{\"12\":1}}],[\"sequences\",{\"1\":{\"8\":1}}],[\"single\",{\"1\":{\"83\":1}}],[\"si​\",{\"1\":{\"79\":2}}],[\"silver\",{\"1\":{\"56\":1,\"63\":1,\"66\":1,\"67\":1,\"77\":1}}],[\"simple\",{\"1\":{\"51\":1,\"89\":1}}],[\"simplified\",{\"0\":{\"16\":1}}],[\"sidewalk\",{\"1\":{\"40\":1}}],[\"size\",{\"0\":{\"17\":1,\"23\":1}}],[\"sliding\",{\"0\":{\"15\":1,\"16\":1},\"1\":{\"15\":1,\"16\":1}}],[\"s\",{\"1\":{\"12\":2,\"59\":4,\"63\":1,\"76\":4,\"78\":16,\"83\":8}}],[\"s=1∏t​cs​\",{\"1\":{\"8\":1}}],[\"st+i​\",{\"1\":{\"79\":3,\"84\":4}}],[\"st+1​\",{\"1\":{\"63\":1}}],[\"structured\",{\"1\":{\"52\":1}}],[\"strong\",{\"1\":{\"52\":1}}],[\"strategies\",{\"1\":{\"28\":1,\"89\":1}}],[\"stop\",{\"1\":{\"50\":1}}],[\"stephan\",{\"1\":{\"46\":1}}],[\"steps\",{\"1\":{\"22\":1}}],[\"steven\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"st​\",{\"1\":{\"7\":1,\"63\":1}}],[\"state\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"6\":1,\"12\":4,\"19\":1,\"57\":1,\"59\":1,\"63\":3,\"66\":1,\"76\":1,\"78\":1,\"79\":1}}],[\"solution\",{\"1\":{\"87\":2}}],[\"solaris\",{\"1\":{\"3\":3,\"10\":1,\"23\":1,\"24\":1}}],[\"some\",{\"0\":{\"50\":1}}],[\"source\",{\"1\":{\"33\":3,\"34\":1,\"36\":2,\"37\":2,\"41\":3,\"48\":3,\"49\":1}}],[\"sota\",{\"1\":{\"3\":1,\"51\":1}}],[\"和一層\",{\"1\":{\"60\":1}}],[\"和\",{\"1\":{\"3\":2,\"6\":1,\"10\":1,\"12\":3,\"15\":1,\"36\":1,\"37\":1,\"63\":1,\"73\":1,\"78\":3,\"79\":1,\"82\":1,\"83\":2}}],[\"5​\",{\"1\":{\"84\":1}}],[\"55\",{\"1\":{\"50\":1}}],[\"5595\",{\"1\":{\"24\":1}}],[\"53\",{\"1\":{\"50\":1}}],[\"50\",{\"1\":{\"24\":3}}],[\"5\",{\"1\":{\"3\":1}}],[\"52\",{\"1\":{\"3\":1}}],[\"51\",{\"1\":{\"3\":1}}],[\"57\",{\"1\":{\"3\":2,\"86\":1}}],[\"r−v\",{\"1\":{\"79\":3}}],[\"r=r\",{\"1\":{\"76\":1}}],[\"r+γq\",{\"1\":{\"77\":1,\"78\":1,\"83\":2}}],[\"r+γmaxb∈a​q\",{\"1\":{\"77\":1}}],[\"r+γb∈amax​q\",{\"1\":{\"76\":1,\"83\":2}}],[\"r+γa\",{\"1\":{\"59\":1}}],[\"rgb\",{\"1\":{\"68\":1}}],[\"raw\",{\"1\":{\"68\":1}}],[\"randomized\",{\"1\":{\"89\":1}}],[\"randomness\",{\"1\":{\"73\":1}}],[\"random\",{\"0\":{\"82\":1},\"1\":{\"22\":1,\"82\":4,\"83\":1,\"84\":1,\"86\":1}}],[\"road\",{\"1\":{\"40\":1,\"48\":1,\"49\":1}}],[\"r\",{\"1\":{\"12\":2,\"46\":1,\"76\":1,\"78\":1,\"83\":4}}],[\"richter\",{\"1\":{\"46\":1}}],[\"rider\",{\"1\":{\"24\":1,\"40\":1,\"65\":1}}],[\"ri\",{\"1\":{\"12\":3}}],[\"rt−1i​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt−1e​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt​=t\",{\"1\":{\"59\":1}}],[\"rt​∣st​=s\",{\"1\":{\"59\":1}}],[\"rt​\",{\"1\":{\"8\":1,\"59\":1,\"63\":1}}],[\"rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+\",{\"1\":{\"7\":1}}],[\"rti​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rti​=rtepisodic​⋅min\",{\"1\":{\"6\":1}}],[\"rte​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rtβi​​=rte​+βi​rti​\",{\"1\":{\"6\":1}}],[\"review\",{\"1\":{\"75\":1}}],[\"revenge\",{\"1\":{\"3\":2,\"24\":1}}],[\"regularization\",{\"1\":{\"73\":1,\"74\":1,\"75\":1,\"79\":1}}],[\"reply\",{\"1\":{\"79\":1}}],[\"replay\",{\"1\":{\"9\":1,\"17\":3,\"19\":3,\"23\":1,\"28\":2,\"63\":6,\"68\":1,\"76\":3,\"79\":1}}],[\"representation\",{\"1\":{\"67\":1}}],[\"residual\",{\"1\":{\"58\":1}}],[\"resnet\",{\"1\":{\"48\":1}}],[\"resnet101\",{\"1\":{\"43\":2}}],[\"results\",{\"0\":{\"20\":1,\"42\":1,\"64\":1,\"85\":1}}],[\"real\",{\"1\":{\"34\":1,\"36\":1,\"44\":1}}],[\"read\",{\"2\":{\"30\":1,\"54\":1,\"70\":1,\"91\":1}}],[\"release\",{\"1\":{\"32\":1}}],[\"related\",{\"0\":{\"4\":1,\"35\":1,\"58\":1,\"74\":1}}],[\"reinforcement\",{\"0\":{\"55\":1},\"1\":{\"28\":3,\"58\":1,\"68\":1,\"89\":3},\"2\":{\"30\":1,\"70\":1,\"91\":1}}],[\"recurrent\",{\"1\":{\"28\":2}}],[\"returns\",{\"1\":{\"21\":1}}],[\"return\",{\"1\":{\"14\":8,\"15\":1,\"22\":2,\"24\":1,\"59\":3,\"78\":1,\"79\":1}}],[\"retrace\",{\"1\":{\"8\":6,\"12\":2,\"24\":1}}],[\"re\",{\"1\":{\"12\":3}}],[\"reward\",{\"0\":{\"6\":1},\"1\":{\"3\":7,\"5\":1,\"6\":10,\"8\":1,\"9\":2,\"12\":7,\"13\":2,\"14\":1,\"15\":2,\"19\":2,\"22\":6,\"57\":2,\"59\":2,\"63\":1,\"65\":4,\"66\":2,\"76\":1}}],[\"r2d2+sep\",{\"1\":{\"24\":1}}],[\"r2d2\",{\"1\":{\"3\":3,\"9\":1,\"17\":1,\"23\":1,\"24\":2,\"25\":3}}],[\"rl\",{\"0\":{\"8\":1},\"1\":{\"3\":4,\"5\":2,\"6\":1,\"8\":1,\"57\":8,\"59\":4,\"60\":1,\"62\":1,\"63\":2,\"66\":1,\"68\":1,\"73\":1,\"74\":1,\"79\":1,\"88\":1}}],[\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的\",{\"1\":{\"37\":1}}],[\"例如在\",{\"1\":{\"10\":1,\"73\":1}}],[\"例如\",{\"1\":{\"3\":1,\"73\":1}}],[\"guez\",{\"1\":{\"77\":1}}],[\"go\",{\"1\":{\"73\":1}}],[\"google\",{\"1\":{\"2\":1,\"72\":1}}],[\"gheshlaghi\",{\"1\":{\"72\":1,\"86\":3,\"87\":1}}],[\"geist\",{\"1\":{\"73\":1}}],[\"germanros\",{\"1\":{\"47\":1}}],[\"generator\",{\"1\":{\"36\":1}}],[\"general\",{\"1\":{\"10\":1,\"16\":1,\"21\":1,\"25\":1}}],[\"gta5\",{\"0\":{\"46\":1,\"48\":1},\"1\":{\"44\":2,\"46\":1,\"50\":1,\"51\":1}}],[\"gradient\",{\"1\":{\"61\":1}}],[\"gridworld\",{\"1\":{\"22\":1}}],[\"greedy\",{\"1\":{\"13\":1,\"16\":1,\"19\":1,\"63\":1,\"73\":1,\"74\":1,\"75\":2,\"76\":1,\"78\":1,\"83\":1}}],[\"g\",{\"1\":{\"21\":1}}],[\"give\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"gaussian\",{\"1\":{\"75\":1,\"81\":1,\"82\":2,\"83\":1,\"84\":3}}],[\"gan\",{\"1\":{\"36\":1}}],[\"gammon\",{\"0\":{\"60\":1},\"1\":{\"58\":1,\"63\":1}}],[\"gamma\",{\"1\":{\"24\":2}}],[\"gamer\",{\"1\":{\"3\":1}}],[\"game\",{\"1\":{\"3\":2}}],[\"games\",{\"1\":{\"3\":3,\"24\":1,\"25\":3,\"27\":1,\"65\":1,\"86\":1}}],[\"gate\",{\"1\":{\"3\":1}}],[\"gates\",{\"1\":{\"3\":1}}],[\"在這裡採用\",{\"1\":{\"83\":1,\"84\":1}}],[\"在這邊我們在意的是評估的部分\",{\"1\":{\"8\":1}}],[\"在實務上為了避免像是\",{\"1\":{\"78\":1}}],[\"在過往的研究可以發現到說往往我們在設計讓\",{\"1\":{\"75\":1}}],[\"在過去的\",{\"1\":{\"73\":1}}],[\"在幾乎所有的遊戲當中都\",{\"1\":{\"67\":1}}],[\"在經過\",{\"1\":{\"63\":1}}],[\"在絕大多數並非是最佳的結果上都不會離最佳太遠\",{\"1\":{\"48\":1,\"49\":1}}],[\"在邊界上往往會出現誤差的問題解決\",{\"1\":{\"38\":1}}],[\"在限定幾款遊戲有特別出色的成效\",{\"1\":{\"25\":1}}],[\"在所有\",{\"1\":{\"25\":1}}],[\"在搭配了\",{\"1\":{\"24\":1}}],[\"在下表當中可以看到\",{\"1\":{\"24\":1}}],[\"在最傾向\",{\"1\":{\"22\":1}}],[\"在不同\",{\"1\":{\"22\":1}}],[\"在每個\",{\"1\":{\"22\":1,\"24\":1}}],[\"在每一個\",{\"1\":{\"13\":1,\"81\":1}}],[\"在時間\",{\"1\":{\"14\":1,\"63\":1,\"79\":1}}],[\"在計算上分別都只會拿自己的\",{\"1\":{\"12\":1}}],[\"在目標的\",{\"1\":{\"8\":1}}],[\"在整個訓練過程當中沒有踏足過的狀態\",{\"1\":{\"6\":1}}],[\"在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響\",{\"1\":{\"3\":1}}],[\"在剩下的遊戲當中這些\",{\"1\":{\"3\":1}}],[\"在\",{\"1\":{\"3\":1,\"6\":2,\"14\":1,\"17\":1,\"21\":1,\"22\":1,\"23\":2,\"24\":2,\"33\":1,\"36\":1,\"37\":1,\"38\":2,\"40\":1,\"41\":1,\"43\":2,\"44\":1,\"57\":1,\"59\":1,\"63\":1,\"66\":1,\"77\":1,\"79\":1,\"82\":1}}],[\"i\",{\"1\":{\"79\":1}}],[\"i=0∑k​eπ\",{\"1\":{\"84\":1}}],[\"i=0∑k​∇ζπ​​log\",{\"1\":{\"84\":1}}],[\"i=0∑k​∇θπ​​log\",{\"1\":{\"79\":1,\"84\":1}}],[\"i=t+1∏s​ci​\",{\"1\":{\"8\":1}}],[\"iclr\",{\"1\":{\"72\":1,\"75\":1},\"2\":{\"91\":1}}],[\"icml\",{\"1\":{\"2\":1},\"2\":{\"30\":1}}],[\"issues\",{\"0\":{\"50\":1}}],[\"is\",{\"0\":{\"33\":1}}],[\"improvement\",{\"1\":{\"24\":1,\"86\":2}}],[\"images\",{\"1\":{\"45\":1,\"46\":1,\"47\":1}}],[\"imagenet\",{\"1\":{\"43\":1}}],[\"image\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":2,\"22\":3,\"23\":2,\"24\":2,\"25\":2,\"33\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":5,\"40\":2,\"41\":2,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"63\":2,\"66\":2,\"67\":2,\"75\":1,\"76\":1,\"77\":1,\"78\":1,\"79\":1,\"86\":3,\"87\":1}}],[\"initialize\",{\"1\":{\"84\":1}}],[\"independent\",{\"1\":{\"82\":1,\"84\":2}}],[\"invaders\",{\"1\":{\"65\":2}}],[\"input\",{\"1\":{\"62\":1,\"84\":2}}],[\"introduce\",{\"1\":{\"51\":1}}],[\"intrinsic\",{\"0\":{\"6\":1},\"1\":{\"5\":1,\"6\":4,\"9\":1,\"12\":3,\"19\":1,\"22\":3}}],[\"in\",{\"1\":{\"28\":2,\"51\":1,\"52\":1}}],[\"info\",{\"1\":{\"3\":1,\"34\":1,\"36\":1,\"37\":4,\"40\":1,\"48\":1,\"49\":1,\"63\":1,\"73\":1,\"75\":1,\"78\":1,\"79\":1}}],[\"information\",{\"0\":{\"2\":1,\"32\":1,\"56\":1,\"72\":1},\"1\":{\"89\":1}}],[\"p\",{\"1\":{\"84\":2}}],[\"p+q\",{\"1\":{\"82\":1}}],[\"pq+q\",{\"1\":{\"82\":1}}],[\"playing\",{\"0\":{\"55\":1}}],[\"pseudo\",{\"0\":{\"37\":1},\"1\":{\"37\":3,\"38\":1,\"40\":1,\"41\":1}}],[\"probability\",{\"1\":{\"76\":1}}],[\"progress\",{\"1\":{\"28\":1}}],[\"preprocess\",{\"1\":{\"63\":1}}],[\"pretrained\",{\"1\":{\"43\":1}}],[\"prediction\",{\"1\":{\"37\":2}}],[\"prioritized\",{\"1\":{\"28\":1}}],[\"private\",{\"1\":{\"3\":2,\"24\":1}}],[\"posts\",{\"0\":{\"92\":1}}],[\"positive\",{\"1\":{\"3\":3,\"65\":1}}],[\"pole\",{\"1\":{\"47\":1}}],[\"policies\",{\"0\":{\"13\":1}}],[\"policy有什么区别\",{\"1\":{\"89\":1}}],[\"policy\",{\"1\":{\"8\":3,\"10\":2,\"12\":4,\"13\":5,\"16\":1,\"24\":1,\"27\":1,\"28\":1,\"57\":1,\"59\":2,\"61\":2,\"63\":1,\"66\":1,\"67\":1,\"73\":1,\"75\":1,\"79\":6,\"88\":2,\"89\":1}}],[\"pong\",{\"1\":{\"24\":1,\"65\":1}}],[\"part\",{\"1\":{\"89\":1}}],[\"parameterization\",{\"0\":{\"12\":1,\"22\":1}}],[\"parameter\",{\"0\":{\"75\":1},\"1\":{\"8\":1,\"74\":1,\"75\":7,\"81\":2,\"89\":2}}],[\"pablo\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"paper\",{\"1\":{\"3\":2,\"37\":1,\"49\":1,\"50\":1,\"62\":1,\"75\":1},\"2\":{\"30\":1,\"54\":1,\"70\":1,\"91\":1}}],[\"ptifall\",{\"1\":{\"3\":1}}],[\"perceptron\",{\"1\":{\"60\":1}}],[\"perturbations\",{\"1\":{\"52\":1}}],[\"person\",{\"1\":{\"40\":1,\"48\":1}}],[\"period\",{\"1\":{\"17\":2,\"23\":1}}],[\"per\",{\"1\":{\"6\":1}}],[\"performance\",{\"1\":{\"3\":2,\"21\":1,\"22\":1,\"23\":1,\"48\":2,\"49\":1,\"51\":1,\"66\":1}}],[\"penalty\",{\"1\":{\"3\":1}}],[\"pietquin\",{\"1\":{\"73\":1}}],[\"pixel\",{\"1\":{\"36\":1,\"67\":1}}],[\"pitfall\",{\"1\":{\"3\":3,\"24\":1}}],[\"piot\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"72\":1,\"86\":3,\"87\":1}}],[\"puigdomènech\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"b∈rq\",{\"1\":{\"82\":1}}],[\"breakout\",{\"1\":{\"65\":1}}],[\"bias\",{\"1\":{\"82\":1}}],[\"binary\",{\"1\":{\"38\":1,\"43\":1}}],[\"bilal\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"72\":1,\"86\":3,\"87\":1}}],[\"better\",{\"1\":{\"75\":2,\"89\":1}}],[\"bert\",{\"1\":{\"65\":1}}],[\"beat\",{\"1\":{\"51\":1}}],[\"beam\",{\"1\":{\"24\":1,\"65\":1}}],[\"best\",{\"1\":{\"50\":3,\"67\":1}}],[\"behaviour\",{\"1\":{\"28\":1}}],[\"benifit\",{\"1\":{\"24\":1}}],[\"benchmarks\",{\"1\":{\"44\":1}}],[\"benchmark\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1}}],[\"build\",{\"1\":{\"48\":1}}],[\"budden\",{\"1\":{\"19\":1}}],[\"buffer\",{\"1\":{\"9\":1,\"17\":1,\"19\":3,\"76\":2,\"79\":1}}],[\"bound\",{\"0\":{\"14\":1}}],[\"b\",{\"1\":{\"12\":3,\"38\":2,\"76\":1,\"77\":2,\"78\":1,\"83\":4}}],[\"balanced\",{\"1\":{\"52\":1}}],[\"baseline\",{\"1\":{\"50\":1,\"67\":1,\"78\":1,\"86\":3}}],[\"based\",{\"1\":{\"43\":1,\"52\":1,\"89\":1}}],[\"basic\",{\"0\":{\"2\":1,\"32\":1,\"56\":1,\"72\":1}}],[\"backbone\",{\"1\":{\"43\":1,\"48\":1}}],[\"backprop\",{\"0\":{\"17\":1,\"23\":1}}],[\"bandit\",{\"0\":{\"13\":1},\"1\":{\"14\":1,\"16\":1,\"24\":3,\"25\":1}}],[\"batch\",{\"1\":{\"12\":4}}],[\"badia\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"analysis\",{\"0\":{\"87\":1}}],[\"and\",{\"1\":{\"28\":1}}],[\"ai​∣si​\",{\"1\":{\"79\":1}}],[\"a3c\",{\"0\":{\"79\":1,\"84\":1},\"1\":{\"74\":1,\"75\":1,\"79\":7,\"84\":3,\"86\":2,\"89\":1}}],[\"azar\",{\"1\":{\"72\":1,\"86\":3,\"87\":1}}],[\"a∼ρ\",{\"1\":{\"59\":1}}],[\"augmentation\",{\"1\":{\"52\":1}}],[\"augumentation\",{\"1\":{\"38\":2}}],[\"average\",{\"1\":{\"24\":2,\"27\":1}}],[\"approximation\",{\"1\":{\"58\":1,\"81\":1}}],[\"approximators\",{\"1\":{\"61\":1}}],[\"approximator\",{\"1\":{\"7\":1,\"61\":2,\"67\":1}}],[\"apply\",{\"1\":{\"33\":1,\"38\":2,\"51\":1}}],[\"applications\",{\"1\":{\"32\":1}}],[\"appendix\",{\"1\":{\"21\":1}}],[\"argb∈amax​q\",{\"1\":{\"78\":1,\"83\":2}}],[\"arthur\",{\"1\":{\"77\":1,\"79\":1}}],[\"architecture\",{\"0\":{\"18\":1}}],[\"arm\",{\"1\":{\"14\":1}}],[\"am​=a\",{\"1\":{\"14\":2,\"15\":2}}],[\"ak​=⎩⎨⎧​kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"16\":1}}],[\"ak​=\",{\"1\":{\"14\":1,\"15\":1}}],[\"ak​\",{\"1\":{\"14\":2}}],[\"advantage\",{\"1\":{\"78\":2,\"79\":2}}],[\"adversarial\",{\"1\":{\"36\":1,\"37\":1,\"52\":1}}],[\"adapt\",{\"1\":{\"52\":1}}],[\"adaptation\",{\"0\":{\"31\":1},\"1\":{\"52\":4}}],[\"adaption\",{\"0\":{\"33\":1,\"41\":1},\"1\":{\"33\":2,\"37\":1},\"2\":{\"54\":1}}],[\"adapting\",{\"1\":{\"28\":1}}],[\"adaptive\",{\"0\":{\"13\":1,\"24\":1}}],[\"adrià\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"actors\",{\"0\":{\"19\":1},\"1\":{\"19\":1}}],[\"actor\",{\"1\":{\"9\":1,\"13\":13,\"16\":1,\"24\":1,\"27\":1,\"79\":1,\"89\":1}}],[\"action\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"3\":1,\"7\":1,\"8\":1,\"9\":1,\"12\":4,\"13\":1,\"14\":3,\"15\":1,\"16\":1,\"19\":2,\"57\":1,\"59\":2,\"63\":2,\"65\":2,\"75\":5,\"76\":4,\"78\":3}}],[\"as\",{\"1\":{\"89\":1}}],[\"asynchronous\",{\"1\":{\"89\":2}}],[\"asb​\",{\"1\":{\"12\":3}}],[\"as​∣xs​\",{\"1\":{\"8\":2}}],[\"assignment\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"a\",{\"0\":{\"13\":1},\"1\":{\"8\":4,\"12\":5,\"14\":10,\"15\":8,\"16\":2,\"38\":2,\"51\":1,\"59\":4,\"76\":5,\"78\":18,\"79\":3,\"83\":8,\"84\":2,\"89\":1}}],[\"a∣xt+1​\",{\"1\":{\"8\":2}}],[\"at+i​\",{\"1\":{\"79\":1,\"84\":2}}],[\"at+i​∣st+i​\",{\"1\":{\"79\":1,\"84\":2}}],[\"at\",{\"1\":{\"40\":2,\"41\":2,\"48\":2,\"49\":1}}],[\"at−1​\",{\"1\":{\"9\":1,\"19\":1}}],[\"at​=a\",{\"1\":{\"59\":1}}],[\"at​\",{\"1\":{\"7\":2,\"8\":8,\"63\":1}}],[\"atari\",{\"0\":{\"1\":1,\"55\":1},\"1\":{\"3\":5,\"25\":3,\"27\":1,\"28\":1,\"65\":1,\"86\":1}}],[\"agents\",{\"1\":{\"89\":1}}],[\"agent\",{\"0\":{\"9\":1},\"1\":{\"3\":1,\"5\":2,\"6\":3,\"15\":1,\"21\":1,\"22\":4,\"57\":1,\"63\":2,\"65\":1,\"66\":2,\"73\":1,\"75\":1,\"79\":1}}],[\"agent57\",{\"0\":{\"1\":1},\"1\":{\"10\":1,\"12\":2,\"13\":3,\"14\":1,\"16\":1,\"21\":1,\"22\":1,\"23\":1,\"25\":5,\"28\":2}}],[\"alternative\",{\"1\":{\"89\":1}}],[\"alogorithm\",{\"1\":{\"79\":1}}],[\"alpha\",{\"1\":{\"73\":1}}],[\"alignment\",{\"0\":{\"36\":1},\"1\":{\"36\":1}}],[\"algorithms\",{\"1\":{\"58\":1}}],[\"algorithm\",{\"0\":{\"14\":1}}],[\"al\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"34\":2,\"36\":2,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"43\":1,\"45\":1,\"46\":1,\"47\":1,\"48\":2,\"49\":1,\"56\":1,\"63\":1,\"66\":1,\"67\":1,\"72\":1,\"76\":1,\"78\":1,\"86\":3,\"87\":1}}],[\"about\",{\"0\":{\"0\":1,\"50\":1}}]],\"serializationVersion\":2}";