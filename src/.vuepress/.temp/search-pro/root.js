export default "{\"documentCount\":208,\"nextId\":208,\"documentIds\":{\"0\":\"v-184f4da6\",\"1\":\"v-3caeec67\",\"2\":\"v-3caeec67#basic-information\",\"3\":\"v-3caeec67#問題描述\",\"4\":\"v-3caeec67#related-works\",\"5\":\"v-3caeec67#never-give-up\",\"6\":\"v-3caeec67#intrinsic-reward\",\"7\":\"v-3caeec67#uvfa\",\"8\":\"v-3caeec67#rl-loss\",\"9\":\"v-3caeec67#ngu-agent\",\"10\":\"v-3caeec67#ngu-的問題\",\"11\":\"v-3caeec67#methodology\",\"12\":\"v-3caeec67#state-action-value-function-parameterization\",\"13\":\"v-3caeec67#adaptive-exploration-over-a-family-of-policies-bandit\",\"14\":\"v-3caeec67#upper-confidence-bound-algorithm-ucb\",\"15\":\"v-3caeec67#sliding-window-ucb\",\"16\":\"v-3caeec67#simplified-sliding-window-ucb\",\"17\":\"v-3caeec67#backprop-through-time-window-size\",\"18\":\"v-3caeec67#high-level-architecture\",\"19\":\"v-3caeec67#actors\",\"20\":\"v-3caeec67#results\",\"21\":\"v-3caeec67#settings\",\"22\":\"v-3caeec67#state-action-value-function-parameterization-1\",\"23\":\"v-3caeec67#backprop-through-time-window-size-1\",\"24\":\"v-3caeec67#adaptive-exploration\",\"25\":\"v-3caeec67#summary\",\"26\":\"v-3caeec67#discussion\",\"27\":\"v-3caeec67#contribution\",\"28\":\"v-3caeec67#值得一看的文章們\",\"29\":\"v-3caeec67@0\",\"30\":\"v-3caeec67@1\",\"31\":\"v-50ae5da2\",\"32\":\"v-50ae5da2#basic-information\",\"33\":\"v-50ae5da2#問題描述\",\"34\":\"v-50ae5da2#related-works\",\"35\":\"v-50ae5da2#softmax-function\",\"36\":\"v-50ae5da2#non-saturating-neurons\",\"37\":\"v-50ae5da2#dropout\",\"38\":\"v-50ae5da2#methodology\",\"39\":\"v-50ae5da2#dataset\",\"40\":\"v-50ae5da2#architecture\",\"41\":\"v-50ae5da2#relu-nonlinearity\",\"42\":\"v-50ae5da2#training-on-mulitiple-gpus\",\"43\":\"v-50ae5da2#local-response-normalization\",\"44\":\"v-50ae5da2#overlapping-pooling\",\"45\":\"v-50ae5da2#reduce-overfitting\",\"46\":\"v-50ae5da2#data-augmentation\",\"47\":\"v-50ae5da2#image-extraction-and-reflection\",\"48\":\"v-50ae5da2#altering-rgb-intensities\",\"49\":\"v-50ae5da2#dropout-1\",\"50\":\"v-50ae5da2#details-of-learning\",\"51\":\"v-50ae5da2#results\",\"52\":\"v-50ae5da2#quantitative-evaluation\",\"53\":\"v-50ae5da2#qualitative-evaluation\",\"54\":\"v-50ae5da2#discussion\",\"55\":\"v-50ae5da2#作者觀點\",\"56\":\"v-50ae5da2#我的觀點\",\"57\":\"v-50ae5da2#心得\",\"58\":\"v-50ae5da2#references\",\"59\":\"v-50ae5da2@0\",\"60\":\"v-50ae5da2@1\",\"61\":\"v-c0336012\",\"62\":\"v-c0336012#basic-information\",\"63\":\"v-c0336012#what-is-domain-adaption\",\"64\":\"v-c0336012#問題描述\",\"65\":\"v-c0336012#related-works\",\"66\":\"v-c0336012#domain-alignment\",\"67\":\"v-c0336012#pseudo-labelling-or-self-training\",\"68\":\"v-c0336012#mixing\",\"69\":\"v-c0336012#methodology\",\"70\":\"v-c0336012#naive-mixing-to-uda\",\"71\":\"v-c0336012#domain-adaption-via-corss-domain-mixed-sampling-dacs\",\"72\":\"v-c0336012#results\",\"73\":\"v-c0336012#實驗設定\",\"74\":\"v-c0336012#dataset\",\"75\":\"v-c0336012#cityscapes\",\"76\":\"v-c0336012#gta5\",\"77\":\"v-c0336012#synthia\",\"78\":\"v-c0336012#gta5-cityscapes\",\"79\":\"v-c0336012#synthia-cityscapes\",\"80\":\"v-c0336012#some-issues-about-evaluation\",\"81\":\"v-c0336012#contribution\",\"82\":\"v-c0336012#值得一看的文章們\",\"83\":\"v-c0336012@0\",\"84\":\"v-c0336012@1\",\"85\":\"v-6fdb6976\",\"86\":\"v-6fdb6976#basic-information\",\"87\":\"v-6fdb6976#問題描述\",\"88\":\"v-6fdb6976#related-works\",\"89\":\"v-6fdb6976#methodology\",\"90\":\"v-6fdb6976#self-training-for-uda\",\"91\":\"v-6fdb6976#daformer-network-architecture\",\"92\":\"v-6fdb6976#rare-class-sampling-rcs\",\"93\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd\",\"94\":\"v-6fdb6976#learning-rate-warmup-for-uda\",\"95\":\"v-6fdb6976#results\",\"96\":\"v-6fdb6976#實驗設定\",\"97\":\"v-6fdb6976#summary\",\"98\":\"v-6fdb6976#learning-rate-warmup\",\"99\":\"v-6fdb6976#rare-class-sampling-rcs-1\",\"100\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd-1\",\"101\":\"v-6fdb6976#daformer-decoder\",\"102\":\"v-6fdb6976#contribution\",\"103\":\"v-6fdb6976#值得一看的文章們\",\"104\":\"v-6fdb6976@0\",\"105\":\"v-6fdb6976@1\",\"106\":\"v-32d63a0d\",\"107\":\"v-32d63a0d#basic-information\",\"108\":\"v-32d63a0d#問題描述\",\"109\":\"v-32d63a0d#related-works\",\"110\":\"v-32d63a0d#q-networks\",\"111\":\"v-32d63a0d#td-gammon\",\"112\":\"v-32d63a0d#收斂性相關研究\",\"113\":\"v-32d63a0d#nfq\",\"114\":\"v-32d63a0d#methodology\",\"115\":\"v-32d63a0d#results\",\"116\":\"v-32d63a0d#實驗設定\",\"117\":\"v-32d63a0d#評估方式\",\"118\":\"v-32d63a0d#比較基準\",\"119\":\"v-32d63a0d#contribution\",\"120\":\"v-32d63a0d@0\",\"121\":\"v-32d63a0d@1\",\"122\":\"v-073f61cf\",\"123\":\"v-073f61cf#basic-information\",\"124\":\"v-073f61cf#問題描述\",\"125\":\"v-073f61cf#related-works\",\"126\":\"v-073f61cf#denoising-autoencoders-daes\",\"127\":\"v-073f61cf#methodology\",\"128\":\"v-073f61cf#overview\",\"129\":\"v-073f61cf#model-description\",\"130\":\"v-073f61cf#learning-dropout-nets\",\"131\":\"v-073f61cf#results\",\"132\":\"v-073f61cf#datasets\",\"133\":\"v-073f61cf#result-on-image-datasets\",\"134\":\"v-073f61cf#result-on-speech-recognition\",\"135\":\"v-073f61cf#result-on-text-dataset\",\"136\":\"v-073f61cf#how-dropout-effect-network\",\"137\":\"v-073f61cf#hyperparameter\",\"138\":\"v-073f61cf#effect-of-data-size\",\"139\":\"v-073f61cf#contribution\",\"140\":\"v-073f61cf#值得一看的文章們\",\"141\":\"v-073f61cf@0\",\"142\":\"v-073f61cf@1\",\"143\":\"v-25c9f246\",\"144\":\"v-25c9f246#basic-information\",\"145\":\"v-25c9f246#問題描述\",\"146\":\"v-25c9f246#related-works\",\"147\":\"v-25c9f246#methodology\",\"148\":\"v-25c9f246#preliminary\",\"149\":\"v-25c9f246#overview\",\"150\":\"v-25c9f246#context-and-detail-crop\",\"151\":\"v-25c9f246#multi-resolution-fusion\",\"152\":\"v-25c9f246#pseudo-label-generation-with-overlapping-sliding-window\",\"153\":\"v-25c9f246#results\",\"154\":\"v-25c9f246#實驗設定\",\"155\":\"v-25c9f246#overview-1\",\"156\":\"v-25c9f246#influence-of-resolution-and-crop-size-on-uda\",\"157\":\"v-25c9f246#crop-size-selection\",\"158\":\"v-25c9f246#memory-usage-comparison\",\"159\":\"v-25c9f246#ablation-study\",\"160\":\"v-25c9f246#contribution\",\"161\":\"v-25c9f246#值得一看的文章們\",\"162\":\"v-25c9f246@0\",\"163\":\"v-25c9f246@1\",\"164\":\"v-5b18c8c4\",\"165\":\"v-5b18c8c4#basic-information\",\"166\":\"v-5b18c8c4#問題描述\",\"167\":\"v-5b18c8c4#related-works\",\"168\":\"v-5b18c8c4#parameter-space-noise-for-exploration\",\"169\":\"v-5b18c8c4#dqn\",\"170\":\"v-5b18c8c4#double-dqn\",\"171\":\"v-5b18c8c4#dueling-dqn\",\"172\":\"v-5b18c8c4#a3c\",\"173\":\"v-5b18c8c4#methodology\",\"174\":\"v-5b18c8c4#基本想法\",\"175\":\"v-5b18c8c4#減少產-random-number-時間\",\"176\":\"v-5b18c8c4#dqn-dueling-dqn\",\"177\":\"v-5b18c8c4#distributed-a3c\",\"178\":\"v-5b18c8c4#results\",\"179\":\"v-5b18c8c4#experiments\",\"180\":\"v-5b18c8c4#analysis\",\"181\":\"v-5b18c8c4#contribution\",\"182\":\"v-5b18c8c4#值得一看的文章們\",\"183\":\"v-5b18c8c4@0\",\"184\":\"v-5b18c8c4@1\",\"185\":\"v-0fd9e004\",\"186\":\"v-0fd9e004#basic-information\",\"187\":\"v-0fd9e004#問題描述\",\"188\":\"v-0fd9e004#related-works\",\"189\":\"v-0fd9e004#methodology\",\"190\":\"v-0fd9e004#preliminary\",\"191\":\"v-0fd9e004#target\",\"192\":\"v-0fd9e004#prototypical-pseudo-label-denoising\",\"193\":\"v-0fd9e004#權重計算\",\"194\":\"v-0fd9e004#prototype-計算\",\"195\":\"v-0fd9e004#loss-計算\",\"196\":\"v-0fd9e004#structure-learning-by-enforcing-consistency\",\"197\":\"v-0fd9e004#distillation-to-self-supervised-model\",\"198\":\"v-0fd9e004#整體流程\",\"199\":\"v-0fd9e004#results\",\"200\":\"v-0fd9e004#實驗設定\",\"201\":\"v-0fd9e004#gta5-cityscapes\",\"202\":\"v-0fd9e004#synthia-cityscapes\",\"203\":\"v-0fd9e004#contribution\",\"204\":\"v-0fd9e004#值得一看的文章們\",\"205\":\"v-0fd9e004@0\",\"206\":\"v-0fd9e004@1\",\"207\":\"v-e1e3da16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2],\"1\":[6],\"2\":[2,13],\"3\":[1,115],\"4\":[2],\"5\":[3,18],\"6\":[2,75],\"7\":[1,46],\"8\":[2,114],\"9\":[2,41],\"10\":[2,58],\"11\":[1],\"12\":[5,132],\"13\":[9,91],\"14\":[6,83],\"15\":[3,56],\"16\":[4,52],\"17\":[5,30],\"18\":[3,13],\"19\":[1,80],\"20\":[1],\"21\":[1,56],\"22\":[5,137],\"23\":[5,39],\"24\":[2,144],\"25\":[1,63],\"26\":[1],\"27\":[1,22],\"28\":[1,41],\"29\":[null,null,1],\"30\":[null,null,5],\"31\":[8],\"32\":[2,12],\"33\":[1,19],\"34\":[2,8],\"35\":[2,9],\"36\":[3,13],\"37\":[1,20],\"38\":[1,15],\"39\":[1,31],\"40\":[1,8],\"41\":[2,31],\"42\":[4,29],\"43\":[3,50],\"44\":[2,32],\"45\":[2,8],\"46\":[2,10],\"47\":[4,12],\"48\":[3,25],\"49\":[1,16],\"50\":[3,43],\"51\":[1],\"52\":[2,22],\"53\":[2,9],\"54\":[1],\"55\":[1,11],\"56\":[1,21],\"57\":[1,22],\"58\":[1,53],\"59\":[null,null,1],\"60\":[null,null,9],\"61\":[8],\"62\":[2,19],\"63\":[4,49],\"64\":[1,49],\"65\":[2],\"66\":[2,77],\"67\":[6,108],\"68\":[1,66],\"69\":[1],\"70\":[4,54],\"71\":[9,67],\"72\":[1],\"73\":[1,37],\"74\":[1,15],\"75\":[1,13],\"76\":[1,19],\"77\":[1,28],\"78\":[3,48],\"79\":[3,39],\"80\":[4,52],\"81\":[1,19],\"82\":[1,52],\"83\":[null,null,1],\"84\":[null,null,7],\"85\":[12],\"86\":[2,24],\"87\":[1,49],\"88\":[2,10],\"89\":[1],\"90\":[4,149],\"91\":[3,167],\"92\":[5,60],\"93\":[7,112],\"94\":[5,13],\"95\":[1],\"96\":[1,42],\"97\":[1,45],\"98\":[3,22],\"99\":[5,48],\"100\":[7,63],\"101\":[2,43],\"102\":[1,22],\"103\":[1,20],\"104\":[null,null,1],\"105\":[null,null,7],\"106\":[6],\"107\":[2,16],\"108\":[1,60],\"109\":[2,16],\"110\":[2,85],\"111\":[2,20],\"112\":[1,32],\"113\":[1,25],\"114\":[1,94],\"115\":[1],\"116\":[1,52],\"117\":[1,50],\"118\":[1,49],\"119\":[1,19],\"120\":[null,null,1],\"121\":[null,null,5],\"122\":[10],\"123\":[2,14],\"124\":[1,43],\"125\":[2],\"126\":[4,35],\"127\":[1],\"128\":[1,33],\"129\":[2,82],\"130\":[3,28],\"131\":[1],\"132\":[1,22],\"133\":[4,115],\"134\":[4,26],\"135\":[4,29],\"136\":[4,81],\"137\":[1,43],\"138\":[4,28],\"139\":[1,6],\"140\":[1,3],\"141\":[null,null,1],\"142\":[null,null,4],\"143\":[9],\"144\":[2,14],\"145\":[1,75],\"146\":[2,10],\"147\":[1],\"148\":[1,164],\"149\":[1,44],\"150\":[4,123],\"151\":[3,154],\"152\":[7,52],\"153\":[1],\"154\":[1,34],\"155\":[1,53],\"156\":[8,65],\"157\":[3,43],\"158\":[3,50],\"159\":[2,54],\"160\":[1,28],\"161\":[1,25],\"162\":[null,null,1],\"163\":[null,null,7],\"164\":[4],\"165\":[2,13],\"166\":[1,40],\"167\":[2,19],\"168\":[5,78],\"169\":[1,70],\"170\":[2,43],\"171\":[2,92],\"172\":[1,129],\"173\":[1],\"174\":[1,86],\"175\":[4,48],\"176\":[2,55],\"177\":[2,71],\"178\":[1],\"179\":[1,73],\"180\":[1,61],\"181\":[1,12],\"182\":[1,49],\"183\":[null,null,1],\"184\":[null,null,5],\"185\":[13],\"186\":[2,22],\"187\":[1,83],\"188\":[2,10],\"189\":[1],\"190\":[1,33],\"191\":[1,75],\"192\":[4,86],\"193\":[1,43],\"194\":[2,46],\"195\":[2,60],\"196\":[5,117],\"197\":[5,45],\"198\":[1,52],\"199\":[1],\"200\":[1,35],\"201\":[4,37],\"202\":[4,11],\"203\":[1,19],\"204\":[1,37],\"205\":[null,null,1],\"206\":[null,null,7],\"207\":[1]},\"averageFieldLength\":[2.504700029509124,47.15597231573776,0.6725282217919828],\"storedFields\":{\"0\":{\"h\":\"About us\"},\"1\":{\"h\":\"Agent57: Outperforming the Atari Human Benchmark\"},\"2\":{\"h\":\"Basic Information\",\"t\":[\"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, et al. @ Google DeepMind\",\"2020 ICML\"]},\"3\":{\"h\":\"問題描述\",\"t\":[\"在 RL 當中，Atari games 是一個相當重要的 benchmark。過去的 RL 模型已經能夠在大多的 atari games 當中獲得相當不錯的 performance，例如 MuZero、R2D2，分別在 57 個遊戲當中有 51 和 52 個遊戲是 outperform 人類的。不過可惜的是，在剩下的遊戲當中這些 SoTA 就通常完全沒辦法學習。\",\"Info\",\"稍微翻了一下 MuZero 以及 R2D2 兩篇 paper 的結果，分別是這些遊戲 performance 不太好。\",\"MuZero \",\"montezuma revenge, pitfall, private eye, skiing, solaris, venture\",\"R2D2 \",\"montezuma revenge, pitfall, private eye, skiing, solaris\",\"那麼，剩下這些遊戲有怎樣的共通點呢？\",\"skiing 和 solaris 這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到 reward，在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響。\",\"Skiing game on Atari 2600. Video from TheLimeyDragon\",\"以 Skiing 這款遊戲來說，玩家要操作角色滑雪，途中要盡可能快速通過指定數量的 gates。每忽略一個 gate 就會多 5 秒的 penalty。Reward 會一直到遊戲的最後依照最後通過的時間決定。\",\"剩下的四款遊戲則是因為環境太大，又有不少的 negative reward，需要相當大量的探索之後才能得到 positive reward。\",\"Pitfall game on Atari 2600. Video from The No Swear Gamer\",\"以 Ptifall 這款遊戲來說，玩家要操作主角在 20 分鐘的時間探索 255 個遊戲場景，去找到藏在地圖當中的寶藏。過程中有許多陷阱，找到寶藏可以加分，最後分數越多越好。\",\"從這些觀察當中可以得到兩個待改善的地方\",\"long-term credit assignment 如何決定哪些 action 應該要給 positive 或是 negative reward\",\"exploration 如何讓 agent 能夠盡可能去正確探索環境 \",\"之所以說\\\"正確\\\"，是因為即便是在很多 negative reward 的地方，也需要嘗試越過那些障礙，也許才有機會遇到 positive reward。\",\"這一篇 paper 希望改善這兩個對 RL 相當重要的問題，也提出了一個可以在所有 57 Atari games 都 outperform 人類的 RL 模型。\"]},\"4\":{\"h\":\"Related Works\"},\"5\":{\"h\":\"Never Give Up\",\"t\":[\"Never Give Up(NGU) 目的也是希望能夠讓 RL agent 能夠在上述 hard-exploration 的環境當中有更好的成效。具體來說 NGU 包含了幾個重要的部分。\",\"Intrinsic Reward\",\"UVFA\",\"RL Loss\",\"NGU Agent\"]},\"6\":{\"h\":\"Intrinsic Reward\",\"t\":[\"在 Intrinsic Reward 的部分目的也是希望能夠促使 agent 多多探索，他們將 reward 分成了兩個部分，分別是 per-episode noveltyrtepisodic​ 以及 life-long noveltyαt​。這兩者分別會讓 agent 鼓勵去探索那些在 episode 當中、在整個訓練過程當中沒有踏足過的狀態。而整體 intrinsic Reward 如下。\",\"rti​=rtepisodic​⋅min{max{αt​,1},L}(L=5)\",\"min 和 max 只是用來限制 life-long novelty 的範圍，避免太大或是太小。\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"而整體的 reward 依照過去 curiosity-driven exploration 的研究，設定如下。\",\"rtβi​​=rte​+βi​rti​\",\"rte​ 是 Extrinsic Reward，在 RL 當中就是環境給予的 reward\",\"rti​ 是 Intrinsic Reward，也就是前面定義的 reward\",\"βi​ 用來調整兩種 reward 的影響程度\",\"不同的環境下需要的 exploration 以及 exploitation 是不同的。當 β 比較大的時候，intrinsic reward 會使得 agent 比較傾向去試試看那些不熟的 state，反之則會去走那些比較熟悉的。\"]},\"7\":{\"h\":\"UVFA\",\"t\":[\"NGU 接下來用 Universal Value Function Approximator, UVFA 去近似 action value function Q。\",\"Q(st​,at​,βi​)=E[rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+…∣st​,at​,βi​]\",\"針對不同的 βi​，NGU 會選擇不同的 γ。\",\"βi​ 大，傾向 exploration，不需要看太遠，γ 選小一些\",\"βi​ 小，傾向 exploitation，需要看遠一些，γ 選大一些\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"左邊是 β 選擇的分布，右邊是 γ 的分布。\"]},\"8\":{\"h\":\"RL Loss\",\"t\":[\"既然有 NN 去逼近，那也就會有 Loss。NGU 計算 Loss 的方式是採用 Transformed Retrace Double Q-learning Loss。\",\"Retrace 是一個可以用來評估或是用在 control 上的 RL 演算法。在這邊我們在意的是評估的部分，Retrace 可以幫助我們去評估如果我們 follow policy μ，在目標的 policy π 的 action value function Qπ 可以拿到多少 Reward。\",\"首先定義從 policy μ 當中取得的 trajectories τ\",\"τ=(xt​,at​,rt​,xt+1​)t∈N​\",\"考慮有限的 sampled sequences，定義 Retrace operator\",\"T^Q(xt​,at​)=Q(xt​,at​)+s=t∑t+k−1​γs−t(i=t+1∏s​ci​)δs​\",\"其中\",\"δt​cs​​=rt​+γa∈A∑​π(a∣xt+1​)Q(xt+1​,a)−Q(xt​,at​)=λmin(1,μ(as​∣xs​)π(as​∣xs​)​)​\",\"實際上訓練的 NN 會有兩個，就跟 DQN 一樣，一個是 target network，一個是 online network。Target network 就可以透過 Retrace operation 去得到目標 yt​^​\",\"yt​^​=T^Q(xt​,at​;θ−)\",\"θ− 是 target network 的 parameter。\",\"有了目標，也就能夠得到 Loss\",\"L(xt​,at​,θ)=(Q(xt​,at​,θ)−yt​^​)2\",\"Tips\",\"上面提及的是單純的 Retrace Double Q-learning Loss，實際上還會為了讓 NN 更好學習，改成 Transformed 版本。\",\"ThQ(x,a)=Eμ​[h(h−1(Q(x,a))+t≥0∑​γt(s=1∏t​cs​)δth​)]\",\"其中\",\"δth​=rt​+γa∈A∑​π(a∣xt+1​)h−1(Q(xt+1​,a)−h−1Q(xt​,at​))\",\"∀z∈R,h(z)∀z∈R,h−1(z)​=sgn(z)(∣z∣+1​−1)+ϵz=sgn(z)((2ϵ1+4ϵ(∣z∣+1+ϵ)​−1​)−1)​\",\"但數學有點太難，我還沒有理解這一段做了什麼。\"]},\"9\":{\"h\":\"NGU Agent\",\"t\":[\"NGU 基本上使用了 R2D2，只不過輸入上會丟\",\"Action at−1​\",\"Extrinsic Reward rt−1e​\",\"Intrinsic Reward rt−1i​\",\"βi​\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"NGU 採用分散式學習，有許多的 actor 使用不同的 βi​ 取得不同的 experience 丟在 replay buffer，然後再讓 learner 使用 experience 去更新參數學習。\",\"最後只需要設定 β=0，就可以得到單純 exploitation 的模型當成最後的結果。\"]},\"10\":{\"h\":\"NGU 的問題\",\"t\":[\"實作上 NGU 有時會很不穩定、難以收斂，尤其當 rti​ 和 rte​ 的大小、分布相當不同時 \",\"Agent57 的作者認為是因為 NGU 只用了一個 NN 去學習導致\",\"不是那麼地 general \",\"解決了一些 hard-exploration 的問題，卻在一些簡單的問題做得很差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"每種 policy(不同 βi​ 的選擇) sample 的 experience 數量相同 \",\"有些 policy 對於學習是並沒有幫助的，但是卻跟其他人有同樣的影響力\",\"有些環境需要更多的 exploration，有些則不需要\",\"無法好好處理 long-term credit assignment 問題 \",\"例如在 skiiing 以及 solaris 就做得頗差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\"]},\"11\":{\"h\":\"Methodology\"},\"12\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"Agent57 首先針對 State-Action Value Function 拆開來，用兩個 NN 分別去針對 Extrinsic 以及 Intrinsic Reward 處理。\",\"Q(x,a,j;θ)=Q(x,a,j;θe)+βj​Q(x,a,j;θi)\",\"x: state\",\"a: action\",\"j: 表示使用的是哪一個 policy 的 one-hot vector\",\"θe: 近似 Extrinsic Reward re 的 NN\",\"θi: 近似 Intrinsic Reward ri 的 NN\",\"θ: θe∪θi\",\"兩個 Q-Network 都會接收同樣的 state 和 action，並且也是 follow 相同的 policy π。\",\"π(x)=arga∈Amax​Q(x,a,j;θ)\",\"兩個模型都是使用 Transformed Retrace Loss，跟 NGU 是一樣的，不過在計算 Loss 時 reward 的部分是分別給 re 和 ri。\",\"細節上，因為是一次更新 B 個 batch，每個 batch sample 的 sequence 大小為 H，因此 Loss 會有兩組總和。\",\"L(D,θ,θ−,π,μ,r,h)=b=0∑B−1​s=t∑t+H−1​(Q(xsb​,asb​;θ)−T^r,hμ,π​Q(xsb​,asb​;θ−))2\",\"D 表示從 μ sample 出來的 trajectories\",\"θ 為 online network 的參數\",\"θ− 為 target network 的參數\",\"π 為目標 policy\",\"μ 為當前 policy\",\"r 表示 reward，上面的差異就是這裡傳入的分別是 re 和 ri\",\"h 為 Transformed Retrace Operator 的 h\",\"xsb​ 是在 batch b、時間 s 的 state\",\"asb​ 是在 batch b、時間 s 的 action\",\"於是 Agent57 的模型變成底下的樣子。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"Note: 雖然兩個模型都會把 intrinsic 以及 extrinsic reward 輸入進去，但 Loss 在計算上分別都只會拿自己的。\",\"Tips\",\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子，也就是說這種做法的正確性是被確保的(無論是否有使用 Transformed 的版本)。\",\"不過實際上訓練時因為拆開來訓練，能夠使模型更好去學習各自的 reward，以達到更好的訓練成效。\",\"透過拆開訓練，解決了 NGU 不穩定、難以收斂的問題。\"]},\"13\":{\"h\":\"Adaptive Exploration over a Family of Policies (Bandit)\",\"t\":[\"「每種 policy sample 的 experience 數量相同」這個問題 Agent57 透過加上 Meta-controller 來解決。\",\"Tips\",\"如果每個 actor 都能夠學習什麼時候該 exploit、什麼時候該 explore，選擇出現傾向，不同 policy 就有不同重要程度了\",\"舉一個例子來說，NGU 會把每個 actor 都當成是工廠生產出來的機器人，每一個 actor 一開始都是一樣的。\",\"接下來依照你的需求不同，你分別把這幾個 actor 加上不同的偏好，有些傾向 exploration，有些傾向 exploitation。\",\"這些 actor 就會去環境當中互動，蒐集一些 experience 給你學習。\",\"另一方面，Agent57 的 actor 天生就有一些自己的偏好，有人天生愛探險，有人天生愛保險。\",\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的 reward。\",\"最後你一樣可以透過這些 actor 蒐集的 experience 去學習。但是 policy 不會被固定下來，具有更高的靈活性。\",\"照著這樣的想法，Agent57 讓每個 actor 前面都加上一組 Meta-controller，在每一個 episode 開始之前，透過它決定接下來要使用的 (βj​,γj​)。此外，Meta-controller 也會依據得到的 reward 去調整選擇不同 j 的機率。\",\"如此一來，每個 actor 就會因為 Meta-controller 的存在，產生出選擇 policy 的傾向，進而使得整體訓練採用的 experience 中 policy 的比例改變。\",\"Warning\",\"細節上，每個 actor 選擇 action 都是採用 ϵl​-greedy，其中的 l 表示不同的 actor。亦即，不同 actor 採用不同的 ϵ 大小，也因為如此，Meta-controller 是每個 actor 各有一個。\"]},\"14\":{\"h\":\"Upper Confidence Bound Algorithm (UCB)\",\"t\":[\"Agent57 把 Meta-controller 簡單設計成一個 Multi-Arm Bandit (MAB) 問題，也就是說我現在面前有 N 個 action {0,…,N−1} 可以選擇，在時間 k 你選擇 Ak​，目標是在整個 horizon K 當中你可以得到最好的 return，也就是讓底下的期望值最大化。\",\"Eπ​[k=0∑K−1​Rk​(Ak​)]\",\"過去對於 MAB 在 reward 的分布是固定的狀況下會使用 UCB 來解決它。基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界，把這個上界當成是它預期的 return，選擇其中最大的當成這次的選擇。\",\"未知/嘗試次數少的選擇 (不確定性高，要傾向 exploration) \",\"平均 Return 低 ➡️ UCB 高 ➡️ 探索機率高\",\"平均 Return 高 ➡️ UCB 更高 ➡️ 探索機率更高\",\"已知/嘗試次數多的選擇 (不確定性低，要傾向 exploitation) \",\"平均 Return 低 ➡️ UCB 低 ➡️ 嘗試機率低\",\"平均 Return 高 ➡️ UCB 高 ➡️ 嘗試機率高\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a)+βNk−1​(a)log(k−1)​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a)μ^​k​(a)​=m=0∑k−1​1{Am​=a}​=Nk​(a)1​m=0∑k−1​Rk​(a)1{Am​=a}​​\",\"也就是說\",\"Nk​(a) 用來表示一個 action a 至今被嘗試的次數\",\"μ^​k​(a) 用來表示一個 action a 至今平均的 Return\",\"從式子當中也可以觀察到，確實它會傾向讓 平均 Return 高 或是 嘗試次數少 的選項有更高機率被選擇到。\"]},\"15\":{\"h\":\"Sliding-Window UCB\",\"t\":[\"然而，如果 reward 的分布會變動的話，單純的 UCB 並不會是一個好的選項，因為過去的經驗即便在現實狀況改變仍然有大影響力。而隨著 agent 更新、行為模式改變，reward 的分布也會變動。\",\"這裡的經驗指的是一個 action 採取的次數以及得到的 Return 平均 (Nk​(a) 和 μ^​k​(a))。\",\"因此 Sliding-Window UCB 加上了一個 window length τ∈N∗ 來限制要考慮多久之前的經驗。\",\"τ 的選擇應遠比 K 小。\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)log(min(k−1,τ))​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a,τ)μ^​k​(a,τ)​=m=max(0,k−τ)∑k−1​1{Am​=a}​=Nk​(a,τ)1​m=max(0,k−τ)∑k−1​Rk​(a)1{Am​=a}​​\",\"僅僅是加上 τ 而已，剩餘的都是相同的。\"]},\"16\":{\"h\":\"Simplified Sliding-Window UCB\",\"t\":[\"最後，Agent57 對 Sliding-Window UCB 做了兩個小修正\",\"log 對於結果並不會有影響，可以移除\",\"多加上 ϵ-greedy\",\"Ak​=⎩⎨⎧​kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)1​​Yk​​∀0≤k≤N−1∀N≤k≤K−1,Uk​≥ϵUCB​∀N≤k≤K−1,Uk​<ϵUCB​​\",\"其中\",\"ϵUCB​ 是一個 hyperparameter\",\"Uk​ 是一個 [0,1] 之間均勻分布的隨機值\",\"Yk​ 是一個 {0,…,N−1} 之間均勻分布的隨機 action\",\"Tips\",\"透過 Bandit，每個 actor 能夠調整自己的 (γ,β)，解決了 NGU「不是那麼地 general」、「每種 policy sample 的 experience 數量相同」這兩個問題。\"]},\"17\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"原先 R2D2 在 Replay buffer 的設計是採用 trace length 80 搭配 replay period 40，作者在實驗當中發現如果採用 trace length 160 搭配 replay period 80，也就是 long trace 的話，對於 long-term credit assignment 的問題似乎能夠得到改善。\",\"Tips\",\"透過 long trace 解決了 NGU「無法好好處理 long-term credit assignment」的問題。\"]},\"18\":{\"h\":\"High-level architecture\",\"t\":[\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"19\":{\"h\":\"Actors\",\"t\":[\"每個 episode 開始前，透過各自的 Meta-Controller 選擇出一組 (γj​,βj​)\",\"透過上一個 trajectory (xt​,rt−1e​,rt−1i​,at−1​,ht−1​) 估計當前 state-action value Q(xt​,⋅,j,θl​)\",\"透過 ϵl​-greedy 選擇 action\",\"計算 intrinsic reward rti​\",\"環境中取得 observation xt+1​, extrinsic reward rte​\",\"若已經又經過 400 個 frames，更新模型參數\",\"重複 2 直到 episode 結束\",\"將 trajectories 交給 replay buffer\",\"ϵl​ 的選擇根據 Dan Horgan, John Quan, David Budden, et al. (2018) 如下\",\"ϵl​=ϵ1+αL−11​\",\"其他部分基本上都跟 NGU 相同。總之，Actors 去跟環境互動，取得 experience 之後交給 replay buffer，Learner 會從 replay buffer 當中 sample 一些 experience 學習，然後繼續跟環境互動。\"]},\"20\":{\"h\":\"Results\"},\"21\":{\"h\":\"Settings\",\"t\":[\"Agent57 在 γ 的分布上有做了一點調整，範圍變成 [0.99,0.9999]，具體來說如下圖\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"其他 Hyperparameter 的設定詳閱論文的 Appendix G，這裡就不贅述。\",\"對於每個實驗的 Agent 都另外加上一個 Evaluator 去紀錄訓練過程當中的 undiscounted episode returns。\",\"此外，他們並不是採用 Human Normalized Scores (HNS)，而是 Capped Human Normalized Scores (CHNS)，這個測量標準比較強調那些 HNS 比較差的結果，也限制了數值範圍，因此會比較能夠好好評估 general performance。\",\"CHNS=max{min{HNS,1},0}\",\"其中\",\"HNS=Humanscore​−Randomscore​Agentscore​−Randomscore​​\"]},\"22\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"我們透過 intrinsic 以及 extrinsic 拆開來解決 NGU 的缺陷，這裡要來實驗這一個做法實際上帶來多少影響。\",\"作者建構一個簡單的 15×15 Gridworld random coin。在每個 episode 開始之前他們把一個 agent 以及一個 coin 隨機地放在地圖上的任意格子。Agent 能夠上、下、左、右移動，並且每個 episode 最多 200 個 steps。當 Agent 走到 coin 會得到 reward 1，然後結束這個 episode。\",\"接著作者比較 NGU 以及 NGU 加上 separate network 的做法。如同前面提及 βj​ 如果選擇較大，由於 intrinsic reward 有較大的影響，agent 會偏向 exploration，反之則是 exploitation。細節上，βj​ 的設定會透過 β 來調整整體 βj​ 的大小。\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"作者比較兩個模型在不同 β 的大小下，各自最傾向 exploration (βj​=maxj​βj​) 以及最傾向 exploitation (βj​=0) 的設定取得的 Extrinsic Reward。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"x 軸表示 β (注意並不是 βj​)\",\"y 軸表示 extrinsic reward\",\"紫色圓點表示 βj​=0，最傾向 exploitation 的狀況\",\"綠色圓點表示 βj​=maxj​βj​，最傾向 exploration 的狀況\",\"從結果可以發現到 NGU 在不同 β 的設定下會大程度影響到最終 exploitation 的結果，即便這個環境設定是相當簡單的，最終 Return 的趨勢仍然是隨著 β 越大變得越小。\",\"另一方面，加上了 separate network 的狀況下 exploitation 的 return 基本上都相當接近 1.0，也就是說能夠順利到達 coin 所在的位置。\",\"在 exploration 的部分也可以發現到兩者的發展方向會稍有不同。但整體來說兩者都能在最後趨近於 0.0。\",\"由此可見，當 β 提升，由於 intrinsic reward 與 extrinsic reward 的大小相差越來越懸殊，導致 NGU 並無法好好只透過一個 NN 去學習，進而影響到結果，較不具有彈性。相對的，增加 separate network 確實能夠帶來相當好的效益。\",\"此外，作者也發現如果把 Agent57 的 separate network 移除，performance 會掉 20% 以上，可見 separate network 的重要性。\",\"作者也發現到 separate network 在最傾向 exploration 的模型會盡可能避開 coin，反之會走出最短路。\",\"Tips\",\"值得一提的是，這個結果如果在取得 coin 之後仍然不會停止的話就不會出現。\"]},\"23\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"在 trace length 以及對應的 replay period 有多少影響呢？\",\"作者將 R2D2 以及 Agent57 分別用 small trace 以及 long trace 來比較，作者認為在這兩者都有一個共通點：Long trace 會導致訓練前期較為緩慢，但最後能取得更好的 performance。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"在 10 個比較難的遊戲當中測試的結果\",\"尤其在 Solaris 這一款遊戲，可以看到比較明顯的結果。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"24\":{\"h\":\"Adaptive Exploration\",\"t\":[\"最後是針對 Meta-Controller 的實驗。作者將 R2D2+sep. network 以及 NGU+sep. network 拿來比較加上 Meta-Conroller 以及沒有的狀況。\",\"在 10 個比較困難的遊戲當中，可以發現到加上 Meta-Controller(圖片中以 bandit 表示)後可以得到更好的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"此外，從上面的圖片中也可以觀察到這樣的 improvement 在 NGU 當中是小許多的。可以認為 separate networks 跟 meta-controller 之間有一些重疊的 benifit\",\"另一方面，有了 meta-controller 之後，即便 discount factor γ 異常地大(如 γ=0.9999)，模型還是能夠順利學習。在下表當中可以看到 high gamma 的 R2D2，在搭配了 meta-controller 之後得到的成效在 10 款比較困難的遊戲當中有些甚至是能夠比 Average Human 還要強。\",\"Games\",\"R2D2(Retrace) high gamma\",\"Average Human\",\"beam rider\",\"349971.96 ± 5595.38\",\"16926.50\",\"freeway\",\"32.84 ± 0.06\",\"29.60\",\"montezuma revenge\",\"1664.89 ± 1177.26\",\"4753.30\",\"pitfall\",\"0.00 ± 0.00\",\"6463.70\",\"pong\",\"21.00 ± 0.00\",\"14.60\",\"private eye\",\"22480.31 ± 10362.99\",\"69571.30\",\"skiing\",\"-4596.26 ± 601.04\",\"-4336.90\",\"solaris\",\"14814.76 ± 11361.16\",\"12326.70\",\"surround\",\"10.00 ± 0.00\",\"6.50\",\"venture\",\"1774.89 ± 83.79\",\"1187.50\",\"因此作者認為 meta-controller 提供了更大的普遍性，即便在參數比較異常的狀況下仍然能有很不錯的學習成果。\",\"最後，作者也觀察了在幾款遊戲訓練過程中當中 Meta-Controller 在每個 bandit 選擇中最大的 return 分別落在哪個 bandit，可以發現到不同的遊戲會有不同的偏好。從這裡也可以了解到實際上讓每個 actor 自己調整 policy、適應不同的環境，實際上是有幫助的。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"25\":{\"h\":\"Summary\",\"t\":[\"最後比較 R2D2、NGU、Agent57、MuZero 在所有 Atari games 的優劣，可以發現到 MuZero 雖然在 uncapped mean 有最好的結果，但是在 capped mean 卻是最差的。顯示了 MuZero 在限定幾款遊戲有特別出色的成效，但並不 general。\",\"同時也可以看到 Agent57 有最大的 Capped Mean 100，亦即 Agent57 能夠在所有的 Atari games 當中獲得比人類平均還要好的成果，除了展現驚人的成果以外，也說明了 Agent57 的普遍性。\",\"同時也能在 R2D2 與 R2D2 bandit 的比較當中明顯看到在所有的成績都有所提升，再次說明了 Meta-Controller 帶來的效益。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"最後，Agent57 透過 separate networks、Meta-Controller、long trace 解決了 NGU 的四個缺陷，最終在所有的 Atari games 當中都獲得了超過人類的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"26\":{\"h\":\"Discussion\"},\"27\":{\"h\":\"Contribution\",\"t\":[\"提出透過 separate networks 解決訓練不穩定、難以收斂的問題\",\"提出 Meta-Controller 來讓每個 actor 自適應不同環境，使模型具有更好的普遍性，並且不同 policy 得以有不同程度的影響\",\"第一個能夠在所有 Atari games 都獲得比 Average Human 更好的成效\"]},\"28\":{\"h\":\"值得一看的文章們\",\"t\":[\"Agent57: Outperforming the human Atari benchmark\",\"Recurrent Neural Networks in Reinforcement Learning\",\"MAB - UCB <> TS 基本概念\",\"Safe and efficient off-policy reinforcement learning.\",\"Never Give Up: Learning Directed Exploration Strategies\",\"Adapting Behaviour for Learning Progress\",\"Agent57\",\"Distributed Prioritized Experience Replay\",\"Recurrent experience replay in distributed reinforcement learning\"]},\"29\":{\"c\":[\"Note\"]},\"30\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICML\"]},\"31\":{\"h\":\"AlexNet: ImageNet Classifications with Deep Convolutional Neural Networks\"},\"32\":{\"h\":\"Basic Information\",\"t\":[\"Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton @ University of Toronto\",\"2012 NeurlIPS\"]},\"33\":{\"h\":\"問題描述\",\"t\":[\"這是一篇將Deep Learning(深度學習)與Convolutional Neural Networks(卷積神經網路，以下簡稱CNN)運用在Computer Vision(計算機視覺)領域的開拓性論文。作者們train了一個Deep Convolutional Neural Network來分類ImageNet LSVRC-2010資料集中的120萬張高解析度圖像，並得到了相較前人方法顯著優異許多的表現。\"]},\"34\":{\"h\":\"Related Works\",\"t\":[\"Softmax function\",\"Non-saturating neurons\",\"\\\"dropout\\\" regularization method\"]},\"35\":{\"h\":\"Softmax function\",\"t\":[\"在Fully Connected Layer的最後，使用Softmax function將平坦化後的向量轉成機率分佈。通常會寫成以下形式:\",\"簡單來說就是將一個K維的向量壓縮成一個K維的機率向量，且所有元素總和為1。\"]},\"36\":{\"h\":\"Non-saturating neurons\",\"t\":[\"可以將saturate理解成有沒有將值擠壓到一個特定的區間，正式的定義可以寫成這樣:\",\"舉例來說，常用的sigmoid函數會將值擠壓到區間[0, 1]: 因此，他就是一個saturating的activation function。 \",\"像一樣很常用的Relu函數則沒有被擠壓的上界: 因此就是一個non-saturating的activation function。 \"]},\"37\":{\"h\":\"Dropout\",\"t\":[\"Dropout是用來解決Overfitting問題的一個基本技巧，透過給定一個特定的機率，代表每個Neuron有多大的機率會被\\\"Dropout\\\"。\\\"Dropout\\\"代表無論input是多少，這些Neuron的output永遠都是0。在Backpropagation的過程中，如果前一層的Activation為0，也會導致Gradient是0，使這條Weight無法被更新。因此，如果一個Neuron被\\\"Dropout\\\"的話，可當作是這個Neuron並不存在。\",\"Dropout Layer的運作方式為，每一次的iteration都會重新隨機選擇一部份的Neuron來Dropout，藉此避免Neuron間co-adapting太多的問題，大幅降低overfitting發生的可能。\"]},\"38\":{\"h\":\"Methodology\",\"t\":[\"本研究訓練的Deep CNN模型包含了5層的convolutional layers與3層的fully-connected layers，而這樣的深度在處理大量圖像分類是必須的。為了降低成本與訓練時間，作者們使用了一些優化，將分為Dataset, Architecture, Reduce Overfitting與Details of learning四個部份。\"]},\"39\":{\"h\":\"Dataset\",\"t\":[\"ImageNet, 包含了超過1500萬張有標注的high-resolution圖像，分為大概22000個類別。這些圖像都是從網路上收集，並以人工標記(Amazon’s Mechanical Turk crowd-sourcing tool)。每年舉辦的ILSVRC會使用ImageNet一個subset，包含大約1000個類別的1000個圖像(120萬 for training, 5萬 for validation, 15萬 for testing)。本研究使用ILSVRC-2010作為資料集，並參加了ILSVRC-2012的競賽。\",\"ImageNet包含了不同解析度的圖像，所以研究者們透過簡單的rescale與裁切的方式，將圖像down-sampled到256×256的解析度。\"]},\"40\":{\"h\":\"Architecture\",\"t\":[\"AlexNet的架構如圖所示，包含5層的convolutional layers與3層的fully-connected layers。\",\"作者也將AlexNet的主要features分為四點，以下根據重要性排序:\"]},\"41\":{\"h\":\"ReLU Nonlinearity\",\"t\":[\"如同前面Non-saturating neurons所提，一般的模型通常會使用像f(x)=tanh(x)及f(x)=(1+e−x)−1等saturating nonlinearity functions。而AlexNet使用f(x)=max(0,x)這個non-saturating nonlinearity functions(使用這種nonlinearity的Neurons也叫做ReLUs)。 > 這張圖展示了(在CIFAR-10上)在一個4層的CNN上，使用Relu會比使用tanh的Neurons快上6倍。 \",\"如果使用傳統的saturating neuron，是沒有辦法對這麼大的神經網路進行實驗的(Vanishing Gradient問題)，也突顯了non-saturating neurons的重要。\"]},\"42\":{\"h\":\"Training on Mulitiple GPUs\",\"t\":[\"單一個GTX 580 GPU只有3GB的記憶體，因此，只使用一個GPU是放不下用120萬個training examples訓練出來的networks的，所以作者們將Network橫跨到兩個GPU上，將各半的Neurons放在各個GPU上，並讓GPU只在特定的layers進行溝通(例如第2,4,5層Convolution layer的Kernel只會連接到前一層中在同一GPU的Kernel)，精準調控GPU之間的通訊量。\",\"這樣的作法讓top-1跟top-5 error rates分別下降了1.7%和1.2%。相較於單個GPU，在訓練時間上也有些微的縮短。\",\"Info\",\"雖說以現在GPU的發展，一個GPU就放得下了，應該可以不用做這件事情，但如果是為了平行運算的效能優化就不一定了XD\"]},\"43\":{\"h\":\"Local Response Normalization\",\"t\":[\"作者們也使用了以下的normalization技巧:\",\"其中ax,yi​為經過位置在(x,y)的kernel i，經過ReLU nonlinearity的activity，bx,yi​則是做完normalization的值。 N是該層的總kernel數, 而n則為與之相鄰的kernel數。 而n,k,α,β為hyper-parameters，經過調整後為n=5,k=2,α=10−4,β=0.75。\",\"Local response normalization(LRN)實現了一種側抑制的形式，透過相鄰的Neurons間的相互抑制來減少雜訊，提昇training的效度。 這樣的normalization方式分別讓top-1 與 top-5 error rates減少了1.4%與1.2%。\",\"Info\",\"LRN的概念在後來就很少被使用了，比較常被Batch Normalization(BN)取代，甚至後來Brock等人提出說在殘差神經網路(ResNet)中可以不做normalization。\"]},\"44\":{\"h\":\"Overlapping Pooling\",\"t\":[\"傳統上的General Pooling為Non-overlapping pooling，但在AlexNet中使用Overlapping pooling。\",\"s代表pooling units間的間隔(每個s pixels做一次pooling)，z則代表每次做pooling的大小(z×z的大小)。 如果讓s=z，則是傳統的Non-overlapping pooling。 如果讓s<z，則是Overlapping pooling。\",\"AlexNet選擇讓s=2,z=3，相較於s=2,z=2，在top-1跟top-5 error rates上，分別下降了0.4%跟0.3%。且作者們也發現，使用Overlapping Pooling的models較不容易發生Overfitting的情形。\"]},\"45\":{\"h\":\"Reduce Overfitting\",\"t\":[\"雖說ILSVRC的圖像有1000個類別，但AlexNet具有6000萬個parameters，容易導致嚴重的Overfitting。因此，作者透過以下幾種方法來避免Overfitting：\",\"Data Augmentation\",\"Dropout\"]},\"46\":{\"h\":\"Data Augmentation\",\"t\":[\"以下採用兩種形式的data augmentation:\",\"Image extraction and reflection.\",\"Altering RGB intensities.\"]},\"47\":{\"h\":\"Image extraction and reflection\",\"t\":[\"首先從256×256的圖像中隨機提取224×224的patches(跟他們的horizontal reflections)，並直接在這些patches上訓練。而在測試時，抽出其中5個patches(四個角落和中間)以及他們的horizontal reflections(加起來共10個)，並將softmax layer做出的預測進行平均。\"]},\"48\":{\"h\":\"Altering RGB intensities\",\"t\":[\"簡單來說，就是對於每個pixel的RGB值進行PCA降維。 對於每個RGB image pixel\",\"Ixy​=​IRxy​​IGxy​​IBxy​​​​\",\"加上\",\"[p1​​p2​​p3​​]​α1​λ1​α2​λ2​α3​λ​​\",\"其中pi​與λi​分別為RGB pixel的3×3 Covariance Matrix的第i個eigenvector與eigenvalue。 αi​則為隨機變量，對於特定image的每個pixel來說，只會被提取一次\",\"上述的作法其實捕捉了自然圖像的一個重要性質，就是物件的identity對於顏色的照度與強度是不變的(就是說將一張狗的圖片變換顏色與亮度，他也不會變成貓)。這樣的作法成功讓top-1 error rate降低了超過1%。\"]},\"49\":{\"h\":\"Dropout\",\"t\":[\"AlexNet在Fully Connected layer的前兩層使用了名為Dropout的技巧。就如同上面所提，AlexNet將Dropout的機率p=0.5，代表每個Neuron每次有21​的機率被Dropout，造成每次iterate，Neural Network都會有不同的架構。因為Dropout減少了Neurons間的co-adaptations，促使Neurons去學習更加robust的features。雖說Dropout大致會將iteration的數量加倍，但他也大幅降低了Overfitting的發生。\"]},\"50\":{\"h\":\"Details of learning\",\"t\":[\"AlexNet使用Stochastic Gradient Descent(SGD)來尋找參數，batch size為128, momentum為0.9\",\"其中i是指第幾次iteration，v是momentum variable，⟨∂w∂L​​wi​​⟩代表的是在第i的batch Di​上的平均。\",\"初始權重的部份，使用標準差是0.01，平均是0的高斯分佈來初始化每一層的權重，並將第2, 4, 5層Convolution layers與Fully Connected layers的bias都初始化為1，剩下的bias都初始化為0。 而每一層的learning rate都是一樣的，初始值都為0.01，且當validation error rate停滯時就將learning rate除以10(其實我也不知道為什麼，但作者說是Heuristic)。\",\"AlexNet共使用兩張GTX 580 3GB GPU，訓練了約5~6天，經過了約90個cycles，訓練集為ImageNet中的120萬張image。\"]},\"51\":{\"h\":\"Results\"},\"52\":{\"h\":\"Quantitative Evaluation\",\"t\":[\"上表為AlexNet參加ILSVRC-2010與ILSVRC-2012的結果。可以注意到在ISVRC-2010中，top-1與top-5 error rates分別為37.5%與17.0%。雖說ILSVRC-2012的測試集label沒有公開，所以表上的error rates為validation error rates(與test error rates差不到0.1％)，但還是可以看出AlexNet遠比第二名有著更好的準確率。\"]},\"53\":{\"h\":\"Qualitative Evaluation\",\"t\":[\"上圖中可以看到在GPU1中的kernel，學到的大多都是跟顏色無關的特性，而GPU2中的kernel學到的大多都是跟顏色相關的。這個是前面提到Kernel間特殊的連接方式所造成的結果，不論重新設定初始權重跑幾次都會如此。\",\"而這張圖可以看到說，不論是prediction還是classification，AlexNet的表現都非常出色。\"]},\"54\":{\"h\":\"Discussion\"},\"55\":{\"h\":\"作者觀點\",\"t\":[\"AlexNet的研究表明了一個大型的深度CNN是能夠透過只用supervised learning的方式取得非常好的效果，而最重要的關鍵其實在於深度，因此算力是非常重要的(才能支撐更大、更深的模型)。期許未來算力的大幅進步能夠支撐用來處理影片序列(非常多的圖片and時間資訊)的超大超深CNN(我個人覺得已經是現在進行式了)。\"]},\"56\":{\"h\":\"我的觀點\",\"t\":[\"而AlexNet的出現也是受惠於GPU發展的產物，包括後來越來越深的模型。雖說我們現在會覺得AlexNet其實也沒有很深，但以當時來說，是非常尖端且突破性的研究，也帶起了後來在各處使用Convolution Neural Network，以及model越來越deep的風潮。總結來說，AlexNet將Sigmoid替換成了ReLU，避免了Vanishing Gradient的問題；也透過Dropout以及Data augmentation，減少Overfitting的狀況；更點出了『深度很重要』的觀念，促使了後來越來越深的Model與算力發展。\"]},\"57\":{\"h\":\"心得\",\"t\":[\"在讀paper的時候會發現，這篇其實沒有用到什麼複雜的技巧，大多都是大學部ML/DL相關課程就會學到的基本知識。考究到這篇論文的時間是在2012，在當時這些技巧應該都是非常新穎的概念，甚至有些東西是本篇論文的作者在不久前提出的(超酷XD)。自從這篇論文以來，Deep Convolution Neural Network在Computer Vision上才開始蔚為流行，成為大學ML/DL課必教的概念，而AlexNet也從此被視為在Computer Vision領域最有影響力的論文之一。在讀這篇AlexNet的時候，除了感受到這個領域的快速發展外，還真的有一種站在巨人肩膀上的感覺。\"]},\"58\":{\"h\":\"References\",\"t\":[\"ImageNet Classifications with Deep Convolutional Neural Networks\",\"What does the term saturating nonlinearities mean?\",\"The Vanishing Gradient Problem\",\"Gradient Vanishing Problem –- 以 ReLU / Maxout 取代 Sigmoid actvation function\",\"Difference between Local Response Normalization and Batch Normalization\",\"世上最生動的 PCA：直觀理解並應用主成分分析\",\"A Step-by-Step Explanation of Principal Component Analysis (PCA)\",\"機器/深度學習-基礎數學(三):梯度最佳解相關算法(gradient descent optimization algorithms)\"]},\"59\":{\"c\":[\"Note\"]},\"60\":{\"c\":[\"Paper Read\",\"Supervised Learning\",\"Convolutional Neural Networks\",\"Computer Vision\"]},\"61\":{\"h\":\"DACS: Domain Adaptation via Cross-domain Mixed Sampling\"},\"62\":{\"h\":\"Basic Information\",\"t\":[\"2020 Release\",\"2021 WACV(Winter Conference on Applications of Computer Vision)\",\"Chalmers University of Technology(查爾摩斯理工大學)與 Volvo Cars 共同發表\"]},\"63\":{\"h\":\"What is Domain Adaption\",\"t\":[\"Image from Medium\",\"所謂的 Domain 就是用來描述一群資料他們的分布狀況。\",\"Domain Adaption 的目標是把兩個不同分佈的 Domain (Source Domain 以及 Target Domain) 投射到同一個平面上，使得同類型的資料會相近，反之則相遠。\",\"舉一個在 CV 上的例子。如果我們想要訓練一個模型去做自駕車的街景物件偵測，很多時候我們並不會直接去蒐集真實的資料，像是直接有一台車會去蒐集真實街景資料，這樣所需要的成本會過大。時常我們會訓練在合成資料上(synethic data)，然後再應用在真實的世界當中。\",\"Image from Medium\",\"不過這種情況下一個直覺的問題是，在 虛擬世界(Source Domain) 上也許我們能夠對各種物件去做標記 label，但是對於真實世界(Target Domain)往往會有許多我們沒有的 label、環境與虛擬世界有差距，這種差距被描述為 Domain Shift。當兩個 Domain 相差過大，Domain Shift 過高，就會導致單純在 Source Domain 上訓練的模型難以直接 apply 到 Target Domain 上。\",\"因此，Domain Adaption 想解決的就是盡可能地將 Domain Shift 降低，讓我們得以用較低的成本在虛擬環境中訓練模型，然後應用在真實的環境當中。\"]},\"64\":{\"h\":\"問題描述\",\"t\":[\"近年來透過 CNN 處理 semantic segmentation(影像分割) 的模型雖然有許多，也獲得不錯的成果，不過如果遇到新的 domain，往往就會 work 不太好，尤其是從 synethic data 轉變到 real data 上的時候。\",\"問題在於不同的 domain，各自的 domain distribution 會不同。只訓練在 source domain 的模型對於 target domain 的狀況缺乏認知，導致預測失準。\",\"Info\",\"這就像是同理心，因為缺乏對他人的理解，擅自用自己的思維解讀，就會導致互相的不理解。\",\"Image from Liang-ChiehChen et al. (2015)\",\"可以發現單純用 CNN 就可以得到相當好的影像分割結果。\",\"Image from Yiheng Zhang et al. (2018)\",\"直接把訓練在虛擬環境的模型應用在真實環境，結果相當糟糕。\"]},\"65\":{\"h\":\"Related Works\"},\"66\":{\"h\":\"Domain Alignment\",\"t\":[\"透過 adversarial learning (對抗式學習) 去拉近 source domain 以及 target domain。\",\"我們可以想成現在 Segmentation Network 就是 GAN 的 Generator，然後會有一個 Discriminator 去判別現在給我的究竟是 source domain 還是 target domain 的預測結果。\",\"Image from Yi-Hsuan Tsai et al. (2018)\",\"兩個 Domain 中各取圖片，經過相同的 Segmentation Network，將產出的 semantic maps 做對抗式學習\",\"Info\",\"依照 alignment 的不同，可以分成 pixel level, feature map level, semantic level 等不同的做法。\",\"這樣的做法之所以可行，是源自於即便 domain 不同，在 semantic maps 上的 spatial layout 以及 local context 通常並不會差太多。\",\"DACS 的做法之所以能夠成功，也有部分是源自於這樣的相似性帶來的好處。\",\"Tips\",\"同樣以自駕車的例子來說，即便 synethic data 和 real data 的 domain 有相當大的差異，不過像是馬路、汽車、行人都還是會跟地板黏在一起，其他像是路燈、號誌、天空之類的就通常會像是在半空中。這類的 spatial layout 就相當地雷同。\",\"Image from Yi-Hsuan Tsai et al. (2018)\"]},\"67\":{\"h\":\"pseudo labelling (or self-training)\",\"t\":[\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"最初是為了解決 半監督式學習(Semi-Supervise Learning, SSL) 而被提出的。\",\"Info\",\"所謂的半監督式學習也就是說 target domain 的資料上只有一些 labeled data，其他絕大多都是 unlabeled data，這種狀況下訓練模型就被稱為半監督式學習。\",\"而半監督式學習困難的點在於雖然對於 Target Domain 有部分的認知，但是並不全面。\",\"一個簡單的方法是想辦法給這些 unlabeled data 一些 pseudo label。那我們就可以用 supervise learning 的方法解決了。\",\"舉例來說，先在 labeled data 上訓練一個模型，透過這個模型我們就有辦法給 unlabeled data 做 prediction，而 prediction 的結果就當作是他的 pseudo label，就可以再拿去 fine-tune model 了。\",\"Image from Sylwia Majchrowska et al. (2021)\",\"但主要的問題來自於 Domain Shift，畢竟 Source Domain 和 Target Domain 還是存在差異的，並不是所有的 Target Data 都能夠透過 Source Data 去轉移出來。\",\"尤其在 Unsupervised Domain Adaption(UDA) 來說是相當大的問題，在 UDA 當中通常 Domain Shift 都會特別大。\",\"Info\",\"所謂的 UDA 也就是說我們對於 Target Domain 的資料不存在任何 label。換句話說，我們對於 Target Domain 缺乏 label 上的認知。\",\"對於 UDA 來說由於缺乏對於 Target Domain 的認識，一個常見的問題是產出的結果通常會傾向去預測結果為常見的 class。\",\"Info\",\"對陌生人的認識，往往先從貼標籤開始。\",\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的 class，如果出現道路或甚至機車，有可能就被誤判成人行道。或是汽車比卡車更常見，導致卡車時常被預測成汽車。\",\"Image from Yang Zou et al (2018)\",\"看 column 4，只有 pseudo labeling 的例子\",\"Info\",\"雖然已經有 paper 提出如 CBST 的方法來降低這種問題，但在邊界上往往還是難以有好的結果。\"]},\"68\":{\"h\":\"Mixing\",\"t\":[\"Mixing 基本上就是從 training image 拿出兩張，透過一些方式混在一起，產生一個新的 training image。最初被用於把 unlabeled image 混合成新的圖片，是一種 data augumentation 的技巧。\",\"像是 Mixup 這種 data augumentation 方法也是屬於 Mixing 的一種。\",\"DACS 當中使用的是 ClassMix 這種 Mixing 方法。\",\"具體來說，ClassMix 的步驟\",\"把兩個圖片 (A,B) 先轉成 semantic map (SA​,SB​)\",\"把 SA​ 其中一半的 classes 對應的 semantic map 做出一個 binary mask (M)\",\"把 mask M apply 在 A 上，跟 B 合成出 XA​。\",\"把 mask M apply 在 SA​ 上，跟 SB​ 合成出 XA​ 對應的 semantic map YA​\",\"Image from Viktor Olsson et al. (2020)\",\"這樣的做法有趣的是能夠將 semantic segmentation 在邊界上往往會出現誤差的問題解決。\",\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清。但透過剪貼則可以造成不同環境的突兀感，進而解決這個問題。因此這時候 pseudo labelling 就能夠比較好發揮作用。\",\"Image from Viktor Olsson et al. (2020)\"]},\"69\":{\"h\":\"Methodology\"},\"70\":{\"h\":\"Naive Mixing to UDA\",\"t\":[\"最 Naive 的做法就是照著 ClassMix 的方法，將 unlebelled dataset Mixing 成新的 dataset，把 labelled dataset 以及 mixed dataset 拿去訓練。\",\"Info\",\"在 UDA 當中，unlabelled dataset 就是 target domain dataset。\",\"Image from Wilhelm Tranheden at al.\",\"但是這種做法實際上效果很糟糕。像是 sidewalk 被預測成 road，rider 被預測成 person 之類的，許多的 class 都被其他 class 覆蓋。這樣的問題只在 target domain 上會發生，這跟前面提到只使用 pseudo labelling to UDA 會造成的問題是吻合的。\",\"Image from Wilhelm Tranheden at al.\",\"單純的 Naive Mixing 往往在邊界上會有許多誤判的 class\",\"Tips\",\"這種相似的 class 相鄰而導致的誤判被稱為 class conflation\"]},\"71\":{\"h\":\"Domain Adaption via Corss-domain mixed Sampling (DACS)\",\"t\":[\"DACS 的核心做法是不單只是跟 Target Domain 去 mixing，而是將 Source 跟 Target 一起 Mix。如此一來， Target Domain 以及 Source Domain 的關聯性就能被連結起來，降低 Domain Shift。\",\"Image from Wilhelm Tranheden at al.\",\"詳細的步驟具體來說\",\"從 Source Domain (DS​) 取出圖片與 lebel (XS​,YS​)\",\"從 Target Domain (DT​) 取出圖片 XT​\",\"透過 segmentation network fθ​ 取得 XT​ 的 pseudo label YT​^​\",\"將 (XS​,YS​),(XT​,YT​^​) 經過 ClassMix 得到 (XM​,YM​)\",\"把 (XS​,YS​),(XM​,YM​) 拿去訓練。\",\"Image from Wilhelm Tranheden at al.\",\"在 Loss 的設計上也相當直覺，就是希望 XS​ 的預測結果要接近 YS​，XM​ 的結果要接近 YM​。\",\"H: Cross-Entropy\",\"λ: 調整 Mixing 部分的影響程度\",\"L(θ)=E[H(fθ​(XS​),YS​)+λH(fθ​(XM​),YM​)]\"]},\"72\":{\"h\":\"Results\"},\"73\":{\"h\":\"實驗設定\",\"t\":[\"在 segmentation network 的設定上參考了許多過去的研究，選擇採用 DeepLab v2 搭配 ResNet101 作為 backbone。\",\"ResNet101 是 pretrained on ImageNet 跟 MSCOCO。而 Hyperparameter 的設定基本上跟 Yi-Hsuan Tsai et al. (2018) 一樣。\",\"在 Mixing 的方法上雖然任何 based on binary mask 的 Mixing 都可以使用，不過這裡最主要都是使用 ClassMix。\"]},\"74\":{\"h\":\"Dataset\",\"t\":[\"在 synthetic-to-real 有一些常見的 benchmarks。\",\"GTA5 -> Cityscapes\",\"SYNTHIA -> Cityscapes\",\"GTA5 以及 SYNTHIA 都是虛擬世界當中的影像，而 Cityscapes 則是現實世界當中的影像。\"]},\"75\":{\"h\":\"Cityscapes\",\"t\":[\"照片是在城市當中開車拍下的各種照片\",\"Image from Marius Cordts et al. (2016)\",\"2975 training images\",\"19 classes\"]},\"76\":{\"h\":\"GTA5\",\"t\":[\"照片是在 GTA5 下拍攝的\",\"Image from Stephan R. Richter et al.\",\"24966 synthetic training images\",\"19 classes \",\"可對應到 Cityscapes 的 classes\"]},\"77\":{\"h\":\"SYNTHIA\",\"t\":[\"照片是在 Unity 建構的 virtual city 下拍攝\",\"Image from GermanRos et al. (2016)\",\"9400 synthetic training images\",\"16(or 13) classes \",\"都會對到 Cityscapes 的 classes\",\"13 個 classes 的版本是少了 Wall, Fence, Pole\"]},\"78\":{\"h\":\"GTA5 -> Cityscapes\",\"t\":[\"Image from Wilhelm Tranheden at al.\",\"其他的 Model 都是 DeepLab-v2，他們選擇其中 Performance 最好的，但 Backbone 並不一定要是 ResNet 101\",\"Image from Wilhelm Tranheden at al.\",\"Source 是只有使用 source domain 去 train 的模型\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，只對簡單的 class 像是 Road, Build, Veg, Sky, Person, Car 這些普遍做得不錯的 class 有還不錯的 Performance\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 有點偏以及 Train 真的很糟\"]},\"79\":{\"h\":\"SYNTHIA -> Cityscapes\",\"t\":[\"考慮到 SYNTHIA 有些 paper 使用 16 個 classes，有些是 13 個 class 的版本，所以在數據上 mIoU 有兩列分別表示 13 個平均跟 16 個的平均。\",\"Image from Wilhelm Tranheden at al.\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，甚至對 Road 的 Performance 都不太好\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 頗偏\"]},\"80\":{\"h\":\"Some issues about evaluation\",\"t\":[\"他們認為在其他的 paper 有不少人最後給的結果之所以那麼好看是因為\",\"Cityscapes 並沒有 testset\",\"他們選擇用 validation set 判斷要不要 early stop，這個 validation set 也跟最後評估的 set 是一樣的\",\"針對 validation set 挑選 hyperparameters (?)\",\"所以他們認為這樣不太公平，畢竟在 Validation set 做得很棒不能直接表達在整體會表達很棒。 他們也試著用相同的手段訓練模型，然後拿到了\",\"GTA5 \",\"Baseline: 35.68% (+2.83%)\",\"DACS: 53.84% (+1.7%) (BEST)\",\"SYNTHIA \",\"DACS (13 classes): 55.98% (+1.17%) (1.02% to BEST)\",\"DACS (16 classes): 49.10% (+0.76%) (0.7% to BEST)\"]},\"81\":{\"h\":\"Contribution\",\"t\":[\"Apply SSL method on ClassMix to UDA\",\"Introduce a simple framework with high-performance\",\"Beat SOTA in GTA5 to Cityscape\"]},\"82\":{\"h\":\"值得一看的文章們\",\"t\":[\"【Day 24】半監督式學習（Semi-supervised Learning）（上）\",\"【Day 25】半監督式學習（Semi-supervised Learning）（下）\",\"Notes on “DACS: Domain Adaptation via Cross-domain Mixed Sampling”\",\"物件偵測的領域自適應 (Domain Adaptation)\",\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"Domain Adaptation in Computer Vision: Everything You Need to Know\",\"Semi-supervised semantic segmentation needs strong, varied perturbations\",\"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\",\"Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training\",\"Learning to Adapt Structured Output Space for Semantic Segmentation\"]},\"83\":{\"c\":[\"Note\"]},\"84\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"WACV\"]},\"85\":{\"h\":\"DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\"},\"86\":{\"h\":\"Basic Information\",\"t\":[\"Lukas Hoyer, Dengxin Dai, Luc Van Gool @ ETH Zurich & MPI for Informatics\",\"2022 CVPR\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"與過去的方法相比， DAFormer 在 UDA semantic segmentation 領域當中做出了劃時代的貢獻。\"]},\"87\":{\"h\":\"問題描述\",\"t\":[\"如同過去看過的 UDA 問題描述，這一篇同樣也是先說明了 semantic segmentation 在 UDA 上的重要性。由於標記 semantic segmentation labels 的成本過高，以致於開始將研究的方向轉向如 weak-supervised 或是 semi-supervised learning，最終則是 unsuvervised learning 的 UDA。\",\"在這一篇論文當中主要探討的是過去 UDA 的模型都是採用如 DeepLab 搭配 ResNet 或是 VGG 等架構，但是這些架構在 semantic segmentation 領域都已經是過時的產物，有許多新的架構可以得到更高的 mIoU。作者懷疑會不會其實我們應該要試著採用更好的 backbone 去訓練，可以得到更好的結果。\",\"不過直覺上，如果我們用更加強大的 backbone，那麼就會有更高的機會在 source domain 上 overfitting，因此這一篇 paper 的目標是在改採用更佳的 backbone 的同時，避免 overfitting 的問題。\"]},\"88\":{\"h\":\"Related Works\",\"t\":[\"Sematic Image Segmentation\",\"Unsupervised Domain Adaptation (UDA)\",\"Transformer\",\"Self-training\"]},\"89\":{\"h\":\"Methodology\"},\"90\":{\"h\":\"Self training for UDA\",\"t\":[\"一開始我們一樣先看一下這一篇論文當中會用到的 Notation 以及他對於 self training 的描述。這裡已經預設包含了 Knowledge Distillation。\",\"gθ​ 表示 student model\",\"hϕ​ 表示 teacher model\",\"NS​ 表示 Source Domain 的資料數量\",\"NT​ 表示 Target Domain 的資料數量\",\"XS​={xS(i)​}i=1NS​​ 表示 Source Domain 的資料\",\"XT​={xT(i)​}i=1NT​​ 表示 Target Domain 的資料\",\"YS​={yS(i)​}i=1NS​​ 表示 Source Domain 對應的 labels\",\"YT​={yT(i)​}i=1NT​​ 表示 Target Domain 對應的 labels，在 UDA 預設是不會知道的\",\"H,W 分別表示圖片的高寬\",\"YS​,YT​ 都具有 C 個共通的 classes\",\"最 Naive 的方法是把套上 Categorical Cross Entropy Loss (CCE Loss) 期待預測的 label 跟目標相同。\",\"LS(i)​=−j=1∑H×W​c=1∑C​yS(i,j,c)​loggθ​(xS(i)​)(j,c)\",\"然而這種方法的 performance 以及一般性都並不是很理想。Self training 的方法會使用 pseudo labelling，透過產生假想的 label 去學習。於是 pseudo label 就不是單純的 one-hot，而是包含了機率的概念，我們會選其中最大的當成是最後的答案 pT(i,j,c)​。\",\"pT(i,j,c)​=[c=argmaxc′​hϕ​(xT(i)​)(j,c′)]\",\"此外，我們也可以去定義當前 pseudo label 信心度的標準 qT(i)​。也就是說，會期待預測出來的 label 至少信心度要超過 τ，這樣的結果有多少。\",\"qT(i)​=H⋅W∑j=1H×W​[maxc′​hϕ​(xT(i)​)(j,c′)>τ]​\",\"Info\",\"這裡的 [⋅] 是 Iverson Bracket，只是單純符合條件給 1，否則給 0 的符號。\",\"[P]={10​ifPistrueotherwise​\",\"有了評斷信心水平的標準，就可以結合起來形成新的 Loss。\",\"LT(i)​=−j=1∑H×W​c=1∑C​qT(i)​pT(i,j,c)​loggθ​(xT(i)​)(j,c)\",\"也就是說我們會期待產生出來的 pseudo label 除了越準確越好，也會期待其信心水平也要是高的。\",\"Pseudo label 的產生方式可以是 offline 也可以是 online，這裡考慮到 online 的實作比較簡單，所以採用這個方法。與 ProDA 相同，根據過去的研究，這裡會採用 Exponential Moving Average (EMA) 去更新 teacher model。\",\"ϕt+1​←αϕt​+(1−α)θt​\",\"此外，student model 的訓練上也是使用 augumented data。包含了 DAFormer、Color Jitter、Gaussian Blur、ClassMix。\"]},\"91\":{\"h\":\"DAFormer Network Architecture\",\"t\":[\"首先，針對 backbone network 過於老舊的部分作者先透過一些實驗去尋找好的架構，他們後來發現 Transformer based model 會有更好的 mIoU。這裡選用的 Transformer 是 SegFormer。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Src-Only: 只訓練在 GTA dataset\",\"UDA: 使用 GTA dataset 作為 source domain 加上 UDA 方法 adapt Cityscape dataset\",\"Oracle: 直接使用 supervised learning 訓練 Cityscape dataset\",\"上述的三者分數都是以 Cityscape dataset 去評估取得 Rel 用來比較 UDA 在 Oracle 的 scale 下有多強。\",\"Rel=OracleUDA​\",\"可以發現到 SegFormer 的表現都比起其他架構來得好許多，並且有趣的是 DeepLabV3+ 並沒有得到比 DeepLabV2 更好的表現。\",\":::spoiler 更多關於模型選擇的實驗\",\"由於 backbone 實際上包含了 Encoder 以及 Decoder 兩個部分，作者進一步去分析究竟是哪一個部分使最後得到好的結果。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"可以發現到當 Encoder 不採用 MiT-B5 這種包含了 Transformer 的 encoder，得出的 performance 會有大量的下降，也就是說，Transformer 在這裡能夠提供更好的幫助。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"進一步去研究不同大小的 Encoder 會有怎樣的影響，可以發現到通常越大的模型能夠提供更好的效益。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"使用 Transformer based encoder 有另一個有趣的好處是，他可以很好地把不同的 classes 分開，即便這些 classes 有許多相像的地方。圖中圈起來的是各種交通工具，可以發現到 MiT-B5 可以有更好的 feature separation。\",\"此外，Transformer 當中包含的 self-attention 與傳統的 CNN 不同，即便在 testing 階段能夠動態地依據當下的輸入資料的相似性來產生對應的Affinity-map，再依據得到的Affinity-map做出預測。\",\"中文敘述參考 [論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\":::\",\"於是，根據實驗的結果我們選擇使用 SegFormer 做為新的 backbone。\",\"不過過去使用 Transformer based backbone 解決 semantic segmentation 通常會有個通病是在 decoder 的部分只能取得 local information。於是作者嘗試修改 decoder 的部分，把 encoder 給出不同 level 的 feature maps 處理成相同 channels 數量以及大小，再使用不同的 dilation rates 去處理。如下圖所示。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Training Strategies for UDA\",\"這一篇 paper 最主要的貢獻，就是解決了使用更好的 backbone 同時避免 overfitting source domain 的方法，具體而言有三個部分：Rare Class Sampling(RCS), Thing-Class ImageNet Feature Distance(FD), Learning Rate Warmup for UDA。以下就分別說明這三個部分的作法。\"]},\"92\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"作者在實驗的過程當中發現到 DAFormer 在 Rare Classes 的 performance 在不同 random seed 的情況下有很大的不同。作者認為這是因為若這些 rare classes 在訓練後期才出現，模型很可能已經被 common classes 干擾形成 bias，以致於難以 re-learn。\",\"於是，對於這些 rare classes，我們就希望讓他在訓練過程當中出現的頻率可以更高，也就有更高的機會可以學更多次、更早看到它。\",\"定義一個 source domain class c 出現的頻率 fc​ 如下。\",\"fc​=NS​⋅H⋅W∑i=1NS​​∑j=1H×W​[yS(i,j,c)​]​\",\"而一個 class c 被 sample 到的機率 P(c) 就可以用 softmax with temperature 去定義如下。\",\"P(c)=∑c′=1C​e1−fc′​/Te(1−fc​)/T​\",\"也就是說，我們會盡可能讓出現頻率越低的 class 有較高的機會被 sample 到。\"]},\"93\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"通常在 UDA 的 backbone 所使用的 semantic segmentation network 都會使用 ImageNet pretrained models 去初始化權重。我們理想上會預期那些 ImageNet 當中有包含的 classes 理應因此得到較好的結果。\",\"然而，如 train 和 bus 這兩個 classes 卻反而往往得到很糟糕的結果。並且透過觀察訓練過程作者發現到，其實在訓練初期其實是能夠辨別這些 classes 的，但卻隨著訓練過程慢慢地變糟。\",\"作者認為這是好的 features 都被 Loss function LS​ 搞壞所導致。\",\"因此，作者把這些 \\\"bottleneck features\\\" 拿出來，希望他們在 ImageNet 的 feature 以及訓練模型的 feature 之間的距離可以拉近，避免模型\\\"忘記\\\"這些 features。\",\"不過也考慮到 ImageNet 幾乎都是訓練在 Thing-Class 上，Stuff-Class 如 road, sky 就基本上沒有。因此這裡的拉近只會針對 Thing-Classes Cthings​ 處理。\",\"定義 xS(i)​ 的第 j 個 pixel 的 Feature Distance d(i,j) 如下。\",\"d(i,j)=∥FImageNet​(xS(i)​)(j)−Fθ​(xS(i)​)(j)∥2​\",\"定義 Mask Mthings(i,j)​ 如下。\",\"Mthings(i,j)​=c′=1∑C​yS,small(i,j,c′)​⋅[c′∈Cthings​]\",\"這裡的 yS,small(i,j,c′)​ 只是為了 downsample size，採用了 Average Pooling。\",\"yS,small(i,j,c′)​=[AvgPool(ySc​,H/HF​,W/WF​)>r]\",\"如此一來就能在 Loss 上多加上一項去 regularize。\",\"LFD(i)​=∑j​Mthings(i,j)​∑j=1HF​×WF​​d(i,j)⋅Mthings(i,j)​​\",\"於是乎最後的整體 Loss function 也就形成。\",\"L=LS​+LT​+λFD​LFD​\",\"λFD​ 是一個 hyperparameter。\"]},\"94\":{\"h\":\"Learning Rate Warmup for UDA\",\"t\":[\"過去訓練 CNN 或是 Transformer 都會習慣使用 linear learning rate warmup，這裡也加進來，他們透過實驗發現這很不錯。\",\"ηt​=ηbase​⋅t/twarm​\"]},\"95\":{\"h\":\"Results\"},\"96\":{\"h\":\"實驗設定\",\"t\":[\"在 Dataset 的使用上如同過去我們看過的 DACS 與 ProDA，都是採用 UDA 當中常見的 datasets：Cityscapes、GTA5、SYNTHIA。\",\"實作上採用了常見的 mmsegmentation framework，Network 的架構如同前面所述，encoder 採用 MiT-B5 encoder，decoder 的部分作者另外的調整時選用的 dilation rate 分別是 1, 6, 12, 18。Encoder 已經 pretrain 在 ImageNet-1K 上。\",\"至於詳細的 hyperparameter 設定請詳閱 paper。\"]},\"97\":{\"h\":\"Summary\",\"t\":[\"這裡先簡單總結一下。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"首先看到上面的表格，可以觀察到\",\"加上 Warmup 之後 performance 提升了約 6.4 mIoU (row 1 & 2)\",\"加上 RCS 之後 performance 提升了約 5.8 mIoU (row 2 & 4)\",\"加上 FD 之後 performance 提升了約 3.5 mIoU (row 2 & 6)\",\"加上 Warmup、RCS、FD 之後 performance 提升了約 14.4 mIoU (row 1 & 7)\",\"再多一點調整後可以再提升約 0.8 mIoU\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"作者也給出每個 classes 在加上不同的調整後得出的結果，可以看到所有 class 經過 DAFormer 都可以有獲得提升，甚至那些 rare classes 也變得能夠預測了。\"]},\"98\":{\"h\":\"Learning Rate Warmup\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"經過實驗後發現無論是採用 DeepLabV2 或是 SegFormer，如果搭配 Learning Rate Warmup 都對於 performance 有所提升。\"]},\"99\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"上圖展現出 Rider 和 Bicycle 這兩個 Class 預測的結果，在沒有使用 RCS 的情況下(藍色線)，IoU 的變化很大程度跟 Random Seed 的選用有關，這一點尤其在 Bicycle 最明顯。並且也可以觀察到那些比較早開始有所提升的 Random Seed 最後得到的 IoU 也會最大。\",\"因此作者認為這是跟圖片被 sample 到的時間有所相關，進而提出 RCS 去提升 Rare Class 被 Sample 的機率(橘色線)，可以發現搭配了 RCS 後，IoU 的變化就比較不與 Random Seed 的選擇相關，並且普遍最後的 IoU 都會高過於原本的狀況。\"]},\"100\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"在上圖的橘色線是原本的模型隨著訓練後對於不同 class 預測的 IoU 變化。可以觀察到 Train 這個類別居然會隨著訓練時間預測結果越糟糕，而最一開始的結果其實是還不錯的。\",\"作者認為這是因為 MiT-B5 太強，導致 overfit source domain，進而產生這樣的結果。\",\"透過加上 FD 之後，可以看到在綠色線的部分，成功避免了預測結果變差的狀況。\",\"此外，作者也注意到 Cityscapes 的圖片由於是透過車子上裝設攝影鏡頭去蒐集的，所以圖片底下的部分實際上並不是跟街景相關，而是自駕車車體。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"此外，在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示。\",\"Image from ResearchGate\",\"因此，作者進一步去忽略畫面上方 15 pixels 以及畫面下方 120 pixels 的 pseudo label。另外也考慮到 Transformer 的表達能力可以更強，進一步提高 α 到 0.999，最終得到更好的結果，如上面 summary 所示。\"]},\"101\":{\"h\":\"DAFormer Decoder\",\"t\":[\"Image from ResearchGate\",\"作者進一步去比較自己改良的 decoder 跟其他架構相比，發現到 DAFormer 搭配 Depthwise Separable Convolution 確實能夠得到好的結果。儘管 UperNet 在 Oracle 上可以得到較好的結果，但是在 UDA 上 DAFormer 仍然有更好的 performance。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"最終也可以看到，與過去的 SOTA 相較之下，DAFormer 成功在幾乎所有的 class 上 outperform 其他 SOTA，並且最終的 mIoU 與過去的 SOTA 都有相當大的改進。\"]},\"102\":{\"h\":\"Contribution\",\"t\":[\"研究不同的 backbone 架構對於 UDA performance 的影響\",\"成功將 Transformer 的成功帶進 UDA 領域 \",\"提出了三個方法避免 overfitting 的問題\",\"只需要一張 RTX 2080 Ti GPU 訓練 16 個小時，與過去的資源消耗相較減輕甚多\"]},\"103\":{\"h\":\"值得一看的文章們\",\"t\":[\"[論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\"[CVPR22] DAFormer: Improving Network Architectures for Domain-Adaptive Semantic Segmentation\",\"DAFormer Github\",\"DAFormer(CVPR2022)阅读笔记\",\"DAFormer Extension Paper\"]},\"104\":{\"c\":[\"Note\"]},\"105\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"CVPR\"]},\"106\":{\"h\":\"Playing Atari with Deep Reinforcement Learning\"},\"107\":{\"h\":\"Basic Information\",\"t\":[\"2013 NeurIPS\",\"Volodymyr Mnih, Koray Kavukcuoglu David Silver et al.\",\"這個論文提出的做法稱為 DQN(Deep Q-Networks)\"]},\"108\":{\"h\":\"問題描述\",\"t\":[\"過去在 RL 領域當中把一些 high-dimensional 的感官資料（如：視覺影像、語音資料等）作為 agent 的輸入去學習一直是一個很大的挑戰。然而我們也看到近幾年 Deep Learning 已經能夠在這種資料上去擷取特徵，進而去完成許多複雜的任務。\",\"所以「能不能把 Deep Learning 的成功也放進 RL 當中呢？」這樣的想法自然而然就出現了。\",\"不過從 Deep Learning 的角度來看 RL 的話，會有幾個明顯的問題。\",\"RL 的訓練資料（如：Reward）需要透過與環境互動取得，但數值範圍往往很 sparse，而且也往往會經過一段時間的延遲才取得 與 Deep Learning 相較之下，DL 的資料通常都會先 Label 好，可以直接把資料之間的關聯建構起來。\",\"RL 的訓練資料具有高度相關性 在 DL 當中我們會預設資料之間是沒有什麼相依性的，但在 RL 當中同一個 episode 的 state、action、reward 之間都會具有相當高的相關性。\",\"RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化 DL 往往假設資料的分布會維持住。\",\"這一篇論文成功將 CNN 應用在 RL 上，也避免了上述提及的幾個問題。\"]},\"109\":{\"h\":\"Related Works\",\"t\":[\"Q-Networks\",\"TD-gammon\",\"收斂性相關研究 \",\"Residual algorithms: Reinforcement learning with function approximation.\",\"Q-learning\",\"Neural fitted Q-learning (NFQ)\"]},\"110\":{\"h\":\"Q-Networks\",\"t\":[\"在 RL 當中我們會透過 MDP 去 model 整個問題，而 RL 的目標就是要讓整體的 reward 總和最大化。\",\"定義 optimal action value function Q∗ 如下\",\"Q∗(s,a)=πmax​E[Rt​∣st​=s,at​=a,π]\",\"也就是在 state s 採取 action a 並 follow policy π 得到的最大 return。其中 Return 的定義如下，這裡考慮有 discount 的版本。\",\"Rt​=t′=t∑T​γt′−trt′​\",\"γ 為 discount factor\",\"rt​ 表示在時間 t 取得的 reward\",\"既然 RL 的目的是要讓整體的 return 最大化，也就是要找到 Q∗ 了。Q-Network 就是用 Neural Network 來近似 Q∗，也就是要讓底下的 Loss 最小化。\",\"Li​(θi​)yi​​=Es,a∼ρ(⋅)​[(yi​−Q(s,a;θi​))2]=Es′∼ε​[r+γa′max​Q(s′,a′;θi−1​)]​\",\"需要特別注意到對於 θi​ 來說，他要去近似的是 θi−1​ 的模型得出來的結果，也就是說，Q-Network 透過固定訓練的目標(Target Network)，解決了前面提及的第三個問題「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"111\":{\"h\":\"TD-gammon\",\"t\":[\"一個 model-free RL 算法，透過一個 multi-layer perceptron 和一層 hidden layer 去預測 value function，成功在雙陸棋上面 outperform 人類。\",\"不過這裡的成功只停止在雙陸棋上，並無法繼續擴充到其他的領域。\"]},\"112\":{\"h\":\"收斂性相關研究\",\"t\":[\"過去的研究當中發現到如果是 model-free 搭配 non-linear function approximators 或是 off-policy learning 的話會導致 Q-network 發散，無法收斂。\",\"後續的研究中則發現到 Q-network 無法收斂的問題可以透過 gradient TD 舒緩，並且證明了底下兩個狀況是可以確保收斂。\",\"固定 policy，使用非線性的 approximator\",\"使用線性的 approximator 去學 control\",\"然而這些研究都並未能夠給出用非線性去學 control 的方法。\"]},\"113\":{\"h\":\"NFQ\",\"t\":[\"跟這一篇 paper 最相近的一個研究，他們會先透過 Computer Vision 的模型萃取出圖片的特徵，然後再把這些特徵丟去給 RL 訓練。\",\"不過 DQN 與 NFQ 不同的地方在於 DQN 是 end-to-end，也就是說可以直接從 visual input 去訓練，而 NFQ 不是。\"]},\"114\":{\"h\":\"Methodology\",\"t\":[\"在 TD-gammon 當中我們看到了使用 Neural Network 去學習 value function 有還不錯的成效，DQN 稍微修改了這個做法，將 Q-Network 和 Experience Replay 結合起來。\",\"Experience Replay 會將 agent 跟環境的互動過程當中的 experience 記錄在 replay memory D 當中。當要去更新模型的時候，我們是從 replay memory 當中取得隨機幾筆去更新。\",\"Info\",\"在時間 t 的 experience 包含了 state, action, reward, next state\",\"et​=(st​,at​,rt​,st+1​)\",\"因此 experience 就定義成\",\"D=e1​,e2​,…,eN​\",\"在經過 experience replay 之後，agent 會透過 ϵ-greedy 去選擇 action。\",\"實作上 experience 只會儲存最後 N 筆，並且 history 當中的 frames 只會取出最後 4 個，拿出來做一些 preprocess ϕ(s) 之後作為實際上儲存進 experience 的 state。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"透過 Q-Network 中的 Target Network 以及 Experience Replay，DQN 順利避免了最初提及的兩個問題「RL 的訓練資料具有高度相關性」以及「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"115\":{\"h\":\"Results\"},\"116\":{\"h\":\"實驗設定\",\"t\":[\"實驗做在 7 個 Atari games，Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest 以及 Space Invaders。\",\"每個遊戲的 reward 一開始都不太相同，實驗上調整了 reward 的大小，使得所有 positive reward 固定為 1；negative reward 為 −1，其餘則為 0 表示不影響。\",\"並且會使用 frame skipping 的技巧，讓 agent 只會每經過 k 個 frames 才會去擷取畫面，並且做出相對應的 action。至於那些被忽略的 frames，就持續上一個做出的 action。除了 Space Invaders 因為遊戲當中的雷射會跑很快，所以設定 k=3，其他遊戲則都是 k=4。\"]},\"117\":{\"h\":\"評估方式\",\"t\":[\"在 Deep Learning 當中如果要評估一個 Network 的好壞，可以單純透過觀察模型在 validation set 上的 performance 即可，但是在 RL 當中並沒有 validation set，因此評估一個 agent 的好壞就相對困難。\",\"過去會透過多次遊戲中 agent 獲得的 reward 平均去評估，也就是說理想上每經過一輪更新，模型能夠得到的 reward 應該要慢慢變大。不過作者發現在他們的模型得出來的結果往往會是很不穩定的。作者推測是因為權重即便只有小的變化也會對 policy distribution 有大的影響，導致接下來會經過的 state 就很不相同。\",\"因此作者改成 Q 的平均去評估，也確實發現會平滑許多。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\"]},\"118\":{\"h\":\"比較基準\",\"t\":[\"最後將 DQN 跟幾個 Baseline 去比較\",\"Sarsa \",\"On-control policy\",\"Linear approximator\",\"人工提取 features\",\"Contingency \",\"跟 Sarsa 類似，但有包含了部分的學習過程\",\"HNeat Best \",\"訓練的過程包含了一點專家系統的概念\",\"事先標記好 object 的位置以及類型\",\"HNeat Pixel \",\"訓練的過程包含了一點專家系統的概念\",\"事先處理好了 8 color channel representation\",\"Human\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"最後得出來的結果，DQN 在幾乎所有的遊戲當中都 outperform 所有的算法，證明了 DQN 的成功。\"]},\"119\":{\"h\":\"Contribution\",\"t\":[\"成功結合 Deep Learning 以及 Reinforcement Learning\",\"直接從 raw RGB 當作輸入，不需要事先經過其他的分解\",\"透過 Experience Replay 以及 Target Network 解決過去 Deep Learning 結合 RL 時訓練不佳的問題\"]},\"120\":{\"c\":[\"Note\"]},\"121\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"NeurIPS\"]},\"122\":{\"h\":\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"},\"123\":{\"h\":\"Basic Information\",\"t\":[\"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov @ Toronto University\",\"2014 JMLR\"]},\"124\":{\"h\":\"問題描述\",\"t\":[\"在近年來發現到 Neural Network 參數越多就有越強大的表達能力，並且通常會有更好的表現。不過隨著參數量的上升，我們也發現到模型越來越會傾向於 Overfitting。\",\"這樣的狀況之所以會出現有可能有幾個原因\",\"資料不足，導致過度複雜的模型直接學習到 dataset 本身\",\"資料具有偏差，導致複雜的模型學習過於有偏差\",\"一個理想上必定能夠避免 Overfitting 問題的解法是把所有可能的 Network Parameters 給出的輸出取平均，那就必然會考慮到所有的面向。然而，很顯然地，這樣的方法會有過多的計算量。\",\"作者從 theory of the role of sex in evolution 當中獲得啟發。在有性生殖的過程當中會融合雙親的基因，過程當中也會有一些機率出現 random mutation。從生物學的角度來看，有性生殖能夠讓子代有部分雙親的優勢，使得子代能夠適應整個環境。\",\"作者從啟發當中發想提出了 Dropout，有效地避免 Overfitting。\"]},\"125\":{\"h\":\"Related Works\"},\"126\":{\"h\":\"Denoising Autoencoders (DAEs)\",\"t\":[\"Dropout 在做的事情可以視為是在 Neural Network 的 hidden units 加上 noise。\",\"過去類似的作法出現在 DAEs 當中。DAEs 作為一種 Autoencoder 的類別，同樣也包含了 Encoder 以及 Decoder。理想上將輸入經過 Encoder 與 Decoder 之後能夠還原出輸入的原貌。\",\"DAE 的做法是對 input 加上 noise，藉此讓模型能夠學習到更多的特徵，避免了 Overfitting。\",\"Image from wikipedia\",\"與 DAE 不同，Dropout 針對 hidden units 加上 noise，並且發現這樣的做法實際上對於訓練有相當好的幫助。\"]},\"127\":{\"h\":\"Methodology\"},\"128\":{\"h\":\"Overview\",\"t\":[\"整個 Dropout 的做法簡單來說就是兩件事情。\",\"訓練過程中讓每個 hidden units 都有 p 的機率不被使用到\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"測試過程中讓每個 hidden units 的輸出結果都乘上機率 p 作為輸出\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"每個 hidden units 有機率 p 不被選到，n 個 units 組成的模型就像是有 2n 種不同模型一樣，卻又不會有過高的計算量。\"]},\"129\":{\"h\":\"Model Description\",\"t\":[\"首先說明這篇 paper 使用的一些 notation 跟對 Neural Network 的描述。\",\"L 表示 Neural Network hidden layer 數量\",\"l∈{1,…,L} 表示每個 hidden layer 的編號\",\"z(l) 表示 layer l 的輸入\",\"y(l) 表示 layer l 的輸出 \",\"y(0)=x 表示 input\",\"W(l),b(l) 表示 layer l 的 weights 和 biases\",\"f 表示 activation function\",\"於是對於 hidden unit i 可以描述其輸入與輸出如下\",\"zi(l+1)​yi(l+1)​​=wi(l+1)​y(l)+bi(l+1)​=f(zi(l+1)​)​\",\"加上了 dropout 之後，每個 hidden unit 都會有機率 p 在訓練過程中不被考慮到，因此對於上一層的 hidden unit j 來說，他的輸出會被 p 所影響如下。\",\"rj(l)​y~​(l)​∼Bernoulli(p)=r(l)∗y(l)​\",\"而對於這一層 hidden unit i 的輸入輸出就會變成底下的樣子。\",\"zi(l+1)​yi(l+1)​​=wi(l+1)​y~​(l)+bi(l+1)​=f(zi(l+1)​)​\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"而在測試階段，則是把輸出乘上機率 p 如下\",\"Wtest(l)​=pW(l)\"]},\"130\":{\"h\":\"Learning Dropout Nets\",\"t\":[\"Backpropagation\",\"使用 dropout 的模型在訓練過程當中那些被暫時移除的 hidden unit 梯度會是 0\",\"Dropout 也可以搭配其他的 Regularization 方法如 L1，加上後會再進一步提升效果\",\"Unsupervised Pretraining\",\"Dropout 也可以應用在 pretrained model fine-tuning\",\"由於 dropout 最後輸出會乘上 p，因此 pretrained weight 要先乘 1/p\"]},\"131\":{\"h\":\"Results\"},\"132\":{\"h\":\"Datasets\",\"t\":[\"為了測試 Dropout 的性能以及通用性，作者選了許多不同領域的資料集如下\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"針對不同的 datasets，作者去挑了在該領域的 SOTA 加上 Dropout 去看最終的效能是不是確實有提升。\"]},\"133\":{\"h\":\"Result on Image Datasets\",\"t\":[\"MNIST\",\"MNIST 是一個用於手寫辨識的資料集，包含了許多 28×28 的手寫數字，目的是要辨認出每個圖片是對應到哪個數字 0~9。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以看到 baseline 的 error rate 是 1.60%，單純加上 Dropout 之後就可以降低到 1.35%。\",\"如同前面描述，我們也可以再加上其他的技巧去做更多的處理，可以得到更好的結果。\",\"而在這些許多不同的架構下，在相同的 hyperparameters、相同的 Dropout rate p，可以發現到對應的 test error 在 Dropout 使用的有無有相當大的不同。使用了 Dropout 的架構都可以得到更好的結果。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"SVHN\",\"SVHN(Street View House Number) dataset 包含了許多 32×32 的門牌號碼彩色照片，目標是要辨識出門牌號碼。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"我們將 Conv Net 當作 baseline，也就是對應到 error rate 3.95%。單純在 fully connected layers 加上 dropout 之後可以來到 3.02%。而在所有的 layer 都採用的話則可以進一步降低至 2.55%。\",\"當然，也可以再進一步加上其他的技巧去降低。\",\"CIFAR-10 / CIFAR-100\",\"CIFAR-10 包含了許多 32×32 的彩色圖片，這些圖片會被分類成 10 種不同的類別，目的是要分類每張圖片至正確的類別。而 CIFAR-100 只是類別增加到 100。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"同樣可以觀察到在加上 Dropout 可以得到更好的結果，並且進一步透過其他的技巧可以再做得更好。\",\"ImageNet\",\"ImageNet 包含了許多的高解析度彩色圖片，總共有 22,000 個類別，目標同樣是將每個圖片分類到正確的類別當中。\",\"因為 ImageNet 的類別超級多，因此在評分上我們可以考慮 Top-1 以及 Top-5 兩種方法。也就是說，只要正確的 label 出現在預測機率最高的前 5 個當中，對 Top-5 而言就算他答對。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"同樣可以觀察到 dropout 帶來的好處。\",\"Caution\",\"不過這裡的比較卻乏單純的 Conv Net 去當 baseline，好像不太能直接看出 Dropout 帶來的影響耶。\"]},\"134\":{\"h\":\"Result on Speech Recognition\",\"t\":[\"TIMIT\",\"這個 dataset 當中包含了 680 個講者的演講資料，當中包含了 8 種大主題，目標是要去把每個演講分類到正確的主題當中。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"無論是單純加上 Dropout，或是使用其他的優化手段都有辦法獲得更好的 error rate。\"]},\"135\":{\"h\":\"Result on Text Dataset\",\"t\":[\"Reuters-RCV1\",\"當中包含 800000 篇 newswire 上的文章，50 個主題，目標同樣是要分類。\",\"加上 Dropout 後從 baseline 的 31.05% error rate 降低到了 29.62%。\",\"Info\",\"後續還有一些類似的實驗用來驗證在各個領域採用 Dropout 後都可以獲得很棒的結果，但因為結論都是可以看到 Dropout 或是再額外加上其他優化的技巧都可以得到更好的結果，所以就不再贅述。\"]},\"136\":{\"h\":\"How Dropout Effect Network\",\"t\":[\"Dropout 確實已經透過前面的各個實驗證實了他的效度是相當不錯，不過實際上對於 Network 來說它造成了怎樣的影響呢？\",\"作者認為過去大模型之所以會 overfit datatset 是因為不同的 hidden units 會互相影響，也許某一個 unit 的\\\"錯誤\\\"會被其他的 unit 修正，以至於實際上每個 unit 的效益參差不齊，但是在訓練上的效果看起來還不賴。這個問題被稱為 co-adaptations。\",\"作者認為 Dropout 能夠透過在訓練當中忽略其他的 hidden unit 進而避免 co-adaptation 的問題。\",\"Info\",\"這就像是平常訓練的時候你有隊友可以 cover 你的失誤，所以作為一個團隊來說，你們整體看起來的表現會很不錯。\",\"不過當你們現在處在一個未知的環境當中，反而彼此會難以配合，畢竟都還在熟悉新的環境。\",\"然而 Dropout 就像是有時候你的隊友會請假，但是那些事情還是要處理，所以漸漸地每個人都會有一定的能力水平。現在即便丟到未知的環境當中，每個人也都還是能夠有一定的能力去解決。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"作者使用一個簡單的 Autoencoder 做在 MNIST dataset 上，取出第一層 Layer 擷取出的特徵圖如上。雖然就結果而言他們的 test error 是類似的，但是從擷取出的特徵當中可以很明顯的看出搭配 Dropout 後的 Network 是真的有在學習特徵。\",\"作者也發現到當使用 Dropout 之後 hidden units 的 activation 都會變得比較分散，如下圖所示。而且可以發現到幾乎所有的 activation 都會在 0 附近。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\"]},\"137\":{\"h\":\"Hyperparameter\",\"t\":[\"Dropout 當中有 dropout rate 可以設定，作者設計了兩種狀況來看 p 在不同選擇下的結果。\",\"固定 Layer 當中 hidden unit n\",\"固定 pn\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以發現到在左邊的圖中選擇 0.4≤p≤0.8 之間會比較平坦，大致上也會是比較好的選擇點。\",\"而在右邊的圖當中可以看到如果是固定 pn 的話大致上會在 0.4≤p≤0.7 之間會是比較好的選擇。\",\"因此作者認為選擇 p=0.5 (或是到 0.6) 會是比較好的選擇。\"]},\"138\":{\"h\":\"Effect of Data Size\",\"t\":[\"Data size 通常會對模型的結果有不小的影響。在同樣的網路架構下，如果在很少量資料的狀況下，模型通常會傾向 overfitting。作者比較了同樣架構下不同 data size，dropout 使用的有無造成的結果。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以發現到在資料量小的狀況下使用 dropout 對於模型的訓練沒有什麼幫助。但隨著資料量越來越大，dropout 給出的幫助開始出現，但又會逐漸趨緩。\"]},\"139\":{\"h\":\"Contribution\",\"t\":[\"提出一個簡單避免 Overfitting 的方法\",\"探討 Dropout 在各種狀況下給出的效益\"]},\"140\":{\"h\":\"值得一看的文章們\",\"t\":[\"AutoEncoder (一)-認識與理解\"]},\"141\":{\"c\":[\"Note\"]},\"142\":{\"c\":[\"Paper Read\",\"Regularization\",\"JMLR\"]},\"143\":{\"h\":\"HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\"},\"144\":{\"h\":\"Basic Information\",\"t\":[\"Lukas Hoyer, Dengxin Dai, Luc Van Gool @ ETH Zurich & MPI for Informatics\",\"2022 ECCV\"]},\"145\":{\"h\":\"問題描述\",\"t\":[\"這篇 paper 如同 DAFormer 關注在 UDA for semantic segmentation 。\",\"在過去的方法上由於 UDA 會需要儲存許多不同 Domain 的資訊、額外的 network 架構(如 Knowledge Distilation 時的 Student 與 Teacher)，以及各種新定義的 Loss 資訊，導致 GPU 用量普遍很高，也因此在圖片的輸入上通常會刻意先將輸入圖片的解析度降低。\",\"像是 Cityscapes 的圖片解析度有 2048x1024，但是實作時卻使用 1024x512 的大小。\",\"這種 Low-Resolution(LR) 的方法對於辨識細節相當不利，因為無法好好擷取特徵。然而若直接使用高解析度的圖片，即便 GPU 的記憶體空間足夠，也會因為擷取到過於細節的特徵導致大的物件無法好好辨認。\",\"Info\",\"這就像是不同地方的人行道也許會有不同的地磚設計，但我們都會認為那就是人行道。如果我們看過於細節，導致我們對於不同的設計誤以為是不同類型的物件，那就會在辨識上出現問題。\",\"HRDA 的目標是希望結合 Low-Resolution(LR) 能辨識大範圍特徵的優點，以及 High-Resolution(HR) 能辨識細節的優點，創造出在兩個狀況下都能夠順利辨認的方法。\",\"最後，他們提出的 HRDA 方法甚至大幅超越了過去他們提出的 DAFormer。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"146\":{\"h\":\"Related Works\",\"t\":[\"Sematic Image Segmentation\",\"Unsupervised Domain Adaptation (UDA)\",\"Self-training\",\"DAFormer\"]},\"147\":{\"h\":\"Methodology\"},\"148\":{\"h\":\"Preliminary\",\"t\":[\"UDA 以 self-training 方法處理的大架構如下圖所示，由於過去的研究當中發現 Knowledge-Distillation 會對於 UDA 有正向的幫助，因此我們會訓練出一個 Student Network，依照 Source Label 以及 Target Pseudo-Label 去訓練，而 Teacher Network 則透過 Exponential Moving Average(EMA) 去更新。\",\"Image from Lukas Hoyer's YouTube Channel\",\"這裡先定義一下接下來會用到的基本 Notation。\",\"fθ​ 表示要訓練的模型\",\"NS​ 表示 Source Domain 的資料數量\",\"NT​ 表示 Target Domain 的資料數量\",\"XS={xHRS,m​}m=1NS​​ 表示 Source Domain 的資料\",\"XT={xHRT,m​}m=1NT​​ 表示 Target Domain 的資料\",\"YS={yHRS,m​}m=1NS​​ 表示 Source Domain 對應的 labels\",\"yHRS,m​∈{0,1}HS​×WS​×C\",\"xHRS,m​∈RHS​×WS​×3\",\"xHRT,m​∈RHT​×WT​×3\",\"過去的 UDA 方法通常會將輸入的圖片經過一個 bilinearly downsampling function ζ。例如說 xLRT​=ζ(xHRT​,1/sT​)∈RsT​HT​​×sT​WT​​×3。\",\"經過 fθ​ 可以預測出 source label y^​LRS​=fθ​(xLRS​)，也就可以用 Categorical Cross-Entropy (CCE) 去計算出 Loss。\",\"LSLce​(y^​,y,q)​=Lce​(y^​LRS​,yLRS​,1)=−i=1∑H(y)​j=1∑W(y)​c=1∑C​qij​yijc​logζ(y^​,H(y^​)H(y)​)ijc​​\",\"不過只運用這樣的 Loss 去訓練基本上不會得到太好的結果，過去的經驗上都還需要額外設計一些 Loss function LT 去幫助訓練。HRDA 採用了 DAFormer 的方法。\",\"關於 DAFormer 可以參考過去的文章\",\"DAFormer 透過 teacher model gϕ​ 去 adapt target domain，透過 gϕ​ 可以得到每個 label 預測的機率，我們會從其中取最大的當成是最終的 label pLRT​。\",\"pLR,ijcT​=[c=argmaxc′​gϕ​(xLRT​)ijc′​]\",\"Info\",\"這裡的 [⋅] 是 Iverson Bracket，只是單純符合條件給 1，否則給 0 的符號。\",\"[P]={10​ifPistrueotherwise​\",\"而 student network fθ​ 就可以透過 pseudo label 去學習 target domain 的資訊。\",\"LT=Lce​(y^​LRT​,pLRT​,qLRT​)\",\"這裡的 qLRT​ 是 DAFormer 當中的信心水平。\",\"如同 DAFormer，在實作上會包含：\",\"Transformer Network\",\"Rare Class Sampling\",\"Thing-Class ImageNet Feature Distance\",\"Learning Rate Warmup for UDA\"]},\"149\":{\"h\":\"Overview\",\"t\":[\"HRDA 做的事情簡單來說就是「透過同時考慮 Low-Resolution 以及 High-Resolution 提升模型的預測能力，且提出技巧避免 GPU 記憶體過量」。整體的架構如下圖。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"作者在 YouTube 上的簡短說明當中給出的架構圖也許會有更加直觀的理解。\",\"將圖片切成 Low-Resolution(LR) 與 High-Resolution(HR) 兩個部分\",\"透過 Segmentation Head 得出 LR 與 HR 的預測結果\",\"透過 Attention 混合 LR 與 HR 的優勢，並預測出結果\",\"Fuse 以上的結果，變成最後的預測\",\"Image from Lukas Hoyer's YouTube Channel\"]},\"150\":{\"h\":\"Context and Detail Crop\",\"t\":[\"為了同時保有 Low-Resolution(LR) 能辨識大範圍特徵的優點，以及 High-Resolution(HR) 能辨識細節的優點，HRDA 使用 large LR context crop 以及 small HR detail crop 來分別處理這兩個部分。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"大概念上，對於原本 H×W 的圖片，我們想要從中切出一塊 Low-Resolution 的大圖片(Context Crop)、一塊 High-Resolution 的小圖片(Detail Crop)。\",\"HR 會從 LR 當中切出來。更細節來說，LR 的邊長都會是 HR 的兩倍，hc​=hd​,wc​=wd​,s=2。\",\"large LR context crop\",\"首先將圖片經過裁切出一部分 xc,HR​\",\"xc,HR​=xHR​[bc,1​:bc,2​,bc,3​:bc,4​]\",\"接著透過 ζ 將裁切後的圖片 downsample 得到 Low-Resolution Context Crop xc​。\",\"xc​=ζ(xc,HR​,1/s)\",\"這裡的 bc​ 用來描述裁切的 bounding box，在選擇上會是在圖片範圍當中的 normal distribution，並且選擇了一個 k=s⋅o (o 表示 output stride) 保證選出來的座標可以被 k 整除，藉此確保後續的運作正常。\",\"​bc,1​∼U{0,(H−shc​)/k}⋅k,bc,2​=bc,1​+shc​bc,3​∼U{0,(W−swc​)/k}⋅k,bc,4​=bc,3​+swc​​\",\"small HR detail crop\",\"將 Low-Resolution Context Crop xc​ 再進一步裁切得到 High-Resolution Detail Crop xd​。\",\"xd​=xc,HR​[bd,1​:bd,2​,bd,3​:bd,4​]\",\"其中\",\"​bd,1​∼U{0,(shc​−hd​)/k}⋅k,bd,2​=bd,1​+hd​bd,3​∼U{0,(swc​−hw​)/k}⋅k,bd,4​=bd,3​+wd​​\",\"Get Semantic Segmentation\",\"接下來透過 Feature Encoder Network fE 以及 Semantic Decoder Network fS 就可以分別得到 Context Semantic Segmentation 以及 Detail Semantic Segmentation。\",\"Context Semantic Segmentation y^​c​\",\"y^​c​=fS(fE(xc​))∈Rohc​​×owc​​×C\",\"Detail Semantic Segmentation y^​d​\",\"y^​d​=fS(fE(xd​))∈Rohd​​×owd​​×C\"]},\"151\":{\"h\":\"Multi-Resolution Fusion\",\"t\":[\"如同前面所描述，我們知道 LR 與 HR 各有其好處，作者除了分開兩個部分去產出 LR 與 HR 的結果以外，也希望能夠有一個部分能夠讓模型能同時考慮 LR 與 HR 兩者去作出判斷。\",\"從 Low-Resolution 得到大範圍的資訊、從 High-Resolution 得到細節的資訊。再結合 Scale Attention，透過 Attention 機制去判斷現在應該要注重 LR 還是 HR，又分別要給多少的注意力。\",\"於是我們再加上一個 Scale Attention Decoder fA 去預測 attention scale ac​ 來表示一個點要偏向用 LR 或是 HR。\",\"ac​=σ(fA(fE(xc​)))∈[0,1]ohc​​×owc​​×C\",\"其中 σ 會限制輸出在 [0,1] 之間，越靠近 0 則越相信 LR，反之則越相信 HR。\",\"接下來，由於 Detail Crop(HR) 只有 Context Crop(LR) 的一部分，所以沒有 HR 的地方只能參考 LR，Attention 設為 0。\",\"Image from YuKai Chen @ Medium\",\"就像是這邊的 Attention Map。在 Detail Crop 以外的範圍都是黑的，也就是 Attention 為 0，中間的部分才會有 Detail 與 Context 的交互考量。\",\"寫成數學式也許比較難理解，但他只是希望把多的地方設為 0 而已。\",\"ac′​∈Rohc​​×owc​​,ac′​(i,j)={ac​(i,j)0​ifs⋅obd,1​​≤i<s⋅obd,2​​∧s⋅obd,3​​≤j<s⋅obd,4​​otherwise​\",\"最後要把結果 fuse 在一起，所以要讓大小都相等，對 Detail Crop(HR) 外面補 0，變成跟 Context Crop(LR) 相同。\",\"y^​d′​(i,j)={y^​d​(i−obi,1​​,j−obi,3​​)0​ifobd,1​​≤i<obd,2​​∧obd,3​​≤j<obd,4​​otherwise​\",\"Attention 可以告訴我們要考慮多少的 Context 以及多少的 Detail，於是把每個點跟 Attention 的值相乘，得到 Context 與 Detail 的結果後相加就會是 Fused 之後的預測結果。\",\"y^​c,F​=ζ((1−ac′​)⊙y^​c​,s)+ζ(ac′​,s)⊙y^​d′​\",\"別忘了 Attention 越靠近 0 則越相信 Context，反之則越相信 Detail\",\"訓練時就會使用 Fused 的版本去訓練，Source 採用的 Loss 是單純的 Cross Entropy 如下。額外考慮 Detail Crop y^​dS​,ydS​ 可以讓模型學習到更好的高解析度特徵。\",\"LHRDAS​=(1−λd​)Lce​(y^​c,FS​,yc,HRS​,1)+λd​Lce​(y^​dS​,ydS​,1)\",\"Target Domain 亦然。不過使用的目標是 pseudo labels，且包含了 quality 的項。\",\"LHRDAT​=(1−λd​)Lce​(y^​c,FT​,pc,FT​,qc,FT​)+λd​Lce​(y^​dT​,pdT​,qdT​)\"]},\"152\":{\"h\":\"Pseudo-Label Generation with Overlapping Sliding Window\",\"t\":[\"預測的結果並不是每一個點都會考慮到 Detail Crop 的資訊，然而我們會希望每一個地方都能夠精確地考慮，因此會希望能夠取得所有點的 Detail Crop 資訊。\",\"直覺上會覺得，既然我們想要得到全部 Detail Crop 的資訊，那就直接把原圖交給 Network 去處理即可，尤其預測的過程當中並不包含 backpropagation，也就不會有需要儲存過多資訊導致的記憶體不足問題。\",\"然而由於 HRDA 是 based on DAFormer 做出來的，當中包含了 Transformer 架構，會隱晦地在訓練過程中學習到 positional embedding 資訊，因此當訓練時與預測時採用的解析度大小相同會是最理想的。\",\"因此作者採用相同解析度大小，再使用 Sliding Window 去看過整體的圖片，重疊的部分以平均作為結果，給出更穩健的 pseudo label 預測。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Image from Lukas Hoyer's YouTube Channel\"]},\"153\":{\"h\":\"Results\"},\"154\":{\"h\":\"實驗設定\",\"t\":[\"一如既往，這裡使用的 Datasets 包含了 Cityscapes, GTA5, SYNTHIA。\",\"在 Network 的架構上採用 DAFormer，當中包含了 MiT-B5 encoder 以及 DAFormer 中提出的 decoder。在 Scale Attention 的部份使用的是 SegFormer MLP decoder。\",\"在部分與其他 Model 比較的部份為了公平有時會將 DAFormer 的 backbone 換成 ResNet101 或是 DeepLabV2，底下使用不同架構時會再特別提醒。\"]},\"155\":{\"h\":\"Overview\",\"t\":[\"與過去的 SOTA 比較的結果如下圖所示。\",\"在 GTA5 → Cityscapes 當中全部的 Class 都獲得比過去 SOTA 更好的結果，最後的 mIoU 與 DAFormer 相比多了 5.5 的成長。\",\"在 Synthia → Cityscapes 當中則對大多數的 Class 而言都獲得更好的結果，並且 mIoU 與 DAFormer 相比多了 4.9 的成長。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從圖片上來觀察，可以發現到 HRDA 在細節上的處理比起過去的 SOTA 有更好的處理（如：紅綠燈、騎士），並且對於大的物件也同樣保有更好的結果（如：巴士、汽車）。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"並且如果將 HRDA 應用在過去各種 UDA 的方法上，也可以看出 HRDA 都能夠有效地提升最後的 performance。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"156\":{\"h\":\"Influence of Resolution and Crop Size on UDA\",\"t\":[\"HRDA 最一開始的假設是認為小的解析度會不好預測小物體，反之大的解析度會不好預測大物件，因此這一部分就是要驗這這個假設。\",\"他們比較了 DAFormer 以及 HRDA 在 GTA5 → Cityscapes 中調整 Crop Size 帶來的影響後發現，Crop Size 對於 UDA 與 Supervised Learning 相比更加重要。可以看到左邊的部分，當我們把 Crop Size 調低 4 倍，出來的結果會比沒有調整還要降低約 30 mIoU。\",\"同時，這裡也可以觀察到當我們把解析度提升，無論是對 UDA 或是 Supervised Learning 都會有正面的影響。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"進一步去看每個 Class 的影響，可以觀察到：\",\"比對 Row 1 與 Row 3，可以得到 Crop Size 越大，對所有 Class 的預測都有提升\",\"比對 Row 1 與 Row 2，可以得到 Resolution 越大，小物件的預測更加精確，大物件則有降低的趨勢\",\"將兩者結合後，在所有的 Class 的預測可以有更進一步的提升\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"因此驗證了假設是正確的。\"]},\"157\":{\"h\":\"Crop Size Selection\",\"t\":[\"針對 Context Crop Size 與 Detail Crop Size 做了詳細的比較如下表。\",\"可以發現到：\",\"Context Crop Size 越大，得到的結果會更好\",\"Detail Crop Size 越大，得到的結果會更好\",\"同樣的 Crop Size 下，再多搭配另一個 Resolution 都可以獲得更好的結果 \",\"不過相較之下，Low-Resolution 的有無對於最終結果的影響會是更大的\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"此外，HRDA 在 LR Crop 與 HR Crop 使用不同的 Resolution 是相當重要的。固定使用的 Context Crop 的情況下，如果採用低解析度的圖片，或是從低解析度 Upsample 到高解析度，得出來的效果都比起直接使用高解析度圖片還要差。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"158\":{\"h\":\"Memory Usage Comparison\",\"t\":[\"HRDA 最一開始提到的問題就是使用 High Resolution 作為輸入會太過於耗費記憶體，因此這一部分會針對記憶體用量的部分去做討論。\",\"註：這裡之所以是拿 HR0.75​ 作為 baseline 是因為 HR1​ 已經放不進 24GB 的顯卡了，並且 HR0.75​ 已經有比 DAFormer 更好的 performance。\",\"比較之後可以發現到 HRDA 比起單純的 HR 可以有 3.8 mIoU 的成長，並且如果進一步把 Crop Size 縮小，即便只用了 HR 60% 的記憶體，但卻還是有 1.3 mIoU 的成長。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"159\":{\"h\":\"Ablation Study\",\"t\":[\"在這一篇 paper 當中整體包含了幾個方法\",\"混合 Context 與 Detail Crop\",\"使用 Scale Attention 進一步產出合併結果\",\"Overlapping Detail 處理預測階段的 Detail Crop\",\"加上 Detail Loss 進一步提升效能\",\"針對這幾個部分同樣去研究對應的影響。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從上表可以觀察到\",\"Context 與 Detail 相較之下有更大的影響 (Row 1 & Row 2)\",\"單純 Average Context 與 Detail 並不會有更好的 performance (Row 2 & Row 3)\",\"採用 learned scale attention 對於效能提升大約 3.0 mIoU\",\"採用 Overlapping Detail 可以進一步提升 0.9 mIoU\",\"加上 Detail Loss 可以進一步提升 1.4 mIoU\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從圖片當中可以再次觀察到 LR 的結果在細節上會有缺失，HR 當中則對於大物件有較差的表現。\"]},\"160\":{\"h\":\"Contribution\",\"t\":[\"研究了 Crop Size 對於 UDA performance 的影響\",\"採用 multi-resolution fusion 搭配 scale attention 同時將 LR 與 HR 的優點帶走\",\"在相同量級或甚至更小量級的記憶體使用量下，得到比 SOTA 更好的結果\",\"提出一個能夠搭配許多 UDA Framework 的方法\",\"提出新的 SOTA model\"]},\"161\":{\"h\":\"值得一看的文章們\",\"t\":[\"[論文筆記] HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\",\"[ECCV22] HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\",\"Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille (2016)\"]},\"162\":{\"c\":[\"Note\"]},\"163\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"ECCV\"]},\"164\":{\"h\":\"Noisy Networks for Exploration\"},\"165\":{\"h\":\"Basic Information\",\"t\":[\"2018 ICLR\",\"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind\"]},\"166\":{\"h\":\"問題描述\",\"t\":[\"在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"例如在 Alpha Go 當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",\"Info\",\"論文中有提及在 Matthieu Geist, Olivier Pietquin (2014) 有提及一個使用 Neural Network 的方法，不過並沒有保證收斂，因此仍然沒有解決問題。\",\"因此這一篇論文提出一個方法試圖去消除 exploration 效率與品質的問題。\"]},\"167\":{\"h\":\"Related Works\",\"t\":[\"ϵ-greedy、Entropy Regularization\",\"Parameter Space Noise for Exploration\",\"用來加上 Noisy-Net 的各種 RL 架構 \",\"DQN\",\"Double-DQN\",\"Dueling DQN\",\"A3C\"]},\"168\":{\"h\":\"Parameter Space Noise for Exploration\",\"t\":[\"2017 年由 OpenAI 發表在 ICLR 的 paper。其方法與這一篇可說是大同小異。\",\"在過往的研究可以發現到說往往我們在設計讓 agent 有更多的 exploration 的時候都是透過增加 noise 來達成。\",\"Info\",\"舉例而言，ϵ-greedy 就是在 action space 上增加了 noise，讓選擇更多樣，以達成 exploration。\",\"而在 A3C 當中加上 Entropy Regularization，是在 Loss 上鼓勵 policy 的亂度越高越好，達到鼓勵 exploration 的效果。\",\"Image from OpenAI - Better exploration with parameter noise\",\"核心的概念很簡單，過去增加探索的方法大多都是在 action space 上增加 noise，而這裡則選擇在 parameter space 上增加 noise，並且達到了很棒的效果。\",\"Tips\",\"ϵ-greedy 就像是獵人裡面的凱特，行動之前需要先看運氣抽接下來使用的武器，即便自己知道當下用哪一個 action 比較好，卻會受到 ϵ 的限制。\",\"而在 parameter space 加上 noise 就像是可以換個角度去想其他人會怎麼做，試著用那一個人的做法走過一次，得到不同的經驗。\",\"相較之下，action space 加 noise 就比較像是在亂試，反之在 parameter space 上加 noise 就比較有系統性一些。\",\"具體來說就是他們試圖在 parameter 上加上 Gaussian Noise。\",\"θ~=θ+N(0,σ2I)\",\"Action Space Noise\",\"Parameter Space Noise\",\"Videos from OpenAI - Better exploration with parameter noise\",\"Warning\",\"底下的內容只是單純的 Review\"]},\"169\":{\"h\":\"DQN\",\"t\":[\"透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"於是他們定義了底下的 Loss function 去試圖得到 Q∗。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"D 是上一個 replay buffer 的 transition distribution \",\"state s\",\"action a\",\"reward r=R(s,a)\",\"probability y∼P(⋅∣s,a)\",\"θ− 是被固定的參數\",\"Tips\",\"DQN 帶來了幾個好處\",\"Experience Replay 透過儲存 Experience，更新參數是從 replay buffer 中隨意挑一筆，降低了資料之間的相關性，讓 Neural Network 訓練避免偏差。\",\"Target Network 避免了訓練目標經常地變動造成訓練效果差\",\"使用 Neural Network 替代 action value function 避免了 Q-Learning 的 table 維度過大訓練困難的問題\"]},\"170\":{\"h\":\"Double-DQN\",\"t\":[\"在 DQN 當中我們需要同時訓練兩個 model，也就是 θ 與 θ−。然而 DQN 的設定上對於目標被發現存在高估的問題，因此 Double-DQN 提出了解決這個高估問題的方法。\",\"原始 DQN 目標\",\"Double-DQN 目標\",\"r+γmaxb∈A​Q(y,b;θ−)\",\"r+γQ(y,maxb∈A​Q(y,b;θ);θ−)\",\"高估的狀況如底下的綠線。紫線是目標函數，橘線是綠線與紫線的誤差，不難發現到確實都存在高估的狀況。\",\"然而使用了 Double-DQN 之後的誤差(藍線)就小到幾乎不存在了。\",\"Image from Hado van Hasselt, Arthur Guez, David Silver (2015)\",\"Tips\",\"Double-DQN 帶來的幾個好處\",\"讓 DQN 高估的問題消失，有更好的效果\"]},\"171\":{\"h\":\"Dueling DQN\",\"t\":[\"Dueling DQN 的概念仍然是透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"與 DQN 不同的地方在於他並不是直接去學習 Q∗，而是另外定義了一個 Advantage functionA。\",\"A(s,a)=Q(s,a)−V(s)\",\"V(s) 就像是 baseline，表示著在當前這個 state s 底下你預期可以拿到多好的 return，所以 A(s,a) 意義上就是在看每個 action 有多好多壞。\",\"透過 V 和 A 的總和就能夠得到 Q。上圖就是在最後分開成兩個輸出結果 V 和 A，最後合併成 Q。\",\"Q(s,a;θ)=V(s;θV​)+A(s,a;θA​)\",\"Info\",\"在實務上為了避免像是 V(s) 都是 0，實際上跟 DQN 沒有差異的問題，因此細節上是還會對 A(s,a) 加上總和為 0 的限制。\",\"Q(s,a,θ,α,β)=V(a,θ,α)+​A(s,a,θ,β)−∣A∣1​a′∈∣A∣∑​A(s,a′,θ,β)​\",\"α,β 只是調整 V 和 A 兩部分影響程度的參數。\",\"把 Dueling DQN 搭配 Double DQN 之後可以得到底下的 Loss。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"Tips\",\"Dueling DQN 帶來了幾個好處\",\"使用 Advantage function 增加模型的更新與 exploration。\"]},\"172\":{\"h\":\"A3C\",\"t\":[\"在 DQN 當中使用了 Experience Reply 去避免訓練資料上的強關聯性，然而存在幾個缺點\",\"需要額外的 memory 去儲存 replay buffer\",\"需要 off-policy alogorithm，對於 online RL 來說可能導致收斂不穩定以及緩慢等問題\",\"A3C 的概念就如同火影忍者的影分身之術，讓每個分身在各自的環境當中訓練，訓練成效也就翻倍。\",\"Image from Arthur Juliani@Medium\",\"A3C 是使用 advantage actor-critic 的方式，會直接去學 policy 以及 value function。因此在參數上也就包含了兩項 θπ​ 以及 θv​ 分別表示 policy 以及 value function 的參數。考慮在時間 t 往後看 k 步的更新，參照 A3C 的論文，兩個參數的 Loss 計算分別如下。\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"A：Advantage function 也就是 A3C 當中的 R−V\",\"H：Entropy function 根據 A3C 的論文，加上這一項能夠促使模型更好 exploration\",\"β：調整 A 和 H 的影響程度\",\"Qi​：在時間 i 時於 state st+i​ 執行 policy π 的 return\",\"Note\",\"紅色的部分也就是前面提及的 entropy regularization\",\"最後整體的 Loss 也就如下\",\"L(θ)=Lπ(θπ​)+λLV(θv​)\",\"λ 用來調整兩個 Loss 的影響力\",\"Info\",\"Note：原始 A3C 論文中更新一步，並且沒有使用 entropy 的算式\",\"θ′θv′​​:dθ←dθ+∇θ′​logπ(ai​∣si​;θ′)(R−V(si​;θv′​)):dθv​←dθv​+∂(R−V(si​;θv′​))2/∂θv′​​\",\"Tips\",\"A3C 帶來了幾個好處\",\"降低訓練資料之間的關聯性 畢竟每個 agent 訓練的環境都不同，得到的資料也就不同\",\"能夠使用 on-policy 或是 off-policy，增加通用性\",\"可以平行化加速訓練\",\"穩定地訓練\"]},\"173\":{\"h\":\"Methodology\"},\"174\":{\"h\":\"基本想法\",\"t\":[\"Noisy-Net 的想法跟 Parameter Space Noise for Exploration 的想法基本上是相同方向，都是要對 parameter space 去加上 noise。\",\"作法上，對於每個可訓練的參數拆解成 ζ=(μ,σ)，然後再透過 zero-mean 的 ϵ 增加 noise。也就是說對於一個參數 θ 我們會寫成：\",\"θ=defμ+σ⊙ϵ\",\"所以對於一個 Linear Layer 來說\",\"y=wx+b⇒y=(μw+σw⊙ϵw)x+(μb+σb⊙ϵb)\",\"就只是這樣而已，不要想太多！\",\"Tips\",\"刻意挑 zero-mean 的 noise 是為了採用底下的特性方便後續 Loss 的計算。\",\"Lˉ(ζ)=E[L(θ)]\",\"因此\",\"∇Lˉ(ζ)=∇E[L(θ)]=E[∇μ,Σ​L(μ+Σ⊙ϵ)]\",\"Σ 包含了所有 σ\",\"加上了 Monte-Carlo approximation 之後，可以用單一的 sample ξ 去近似\",\"∇Lˉ(ζ)≈∇μ,Σ​L(μ+Σ⊙ξ)\",\"跟 OpenAI 提出的方法略為不同的地方在於他並不是直接對 network 的參數加上 Gaussian Noise，而是給了參數 ϵw,ϵb 去決定要加怎樣的 noise。\",\"在每一個 episode 開始之前先把參數加上 noise，接下來這一整個 episode 就都是用這個 network 去訓練，意即在過程中不會對 noise 做調整。\"]},\"175\":{\"h\":\"減少產 random number 時間\",\"t\":[\"這樣的做法下每一個 episode 都需要 random noise 在 weight 和 bias 上。假如 w∈Rq×p,b∈Rq，那麼 ϵw∈Rq×p,ϵb∈Rq，也就意味著需要 random 出 pq+q 個數值。\",\"上面基本的做法作者稱他為 Independent Gaussian noise，而接下來作者給出一個 Factorised Gaussian noise 的做法。\",\"基本上就是將 random number 拆分\",\"ϵi,jw​ϵjb​​=f(ϵi​)f(ϵj​)=f(ϵj​)​\",\"其中 f(x)=sgn(x)∣x∣​,ϵi​∈Rq,ϵj​∈Rp。\",\"如此一來，只需要產出 p+q 個 random number 也可以達到類似的效果。\"]},\"176\":{\"h\":\"DQN & Dueling DQN\",\"t\":[\"由於 DQN 和 Dueling DQN 是在 single-thread 上訓練，因此上述的 Random Overhead 會比較大，在這裡採用 Factorised Gaussian noise。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 ϵ-greedy 了。\",\"原本的 DQN 對 Loss 的定義如下。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γb∈Amax​Q(y,b,ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\",\"最外層的期望值是對 ϵ 和 ϵ′\",\"同樣地也可以對 Dueling DQN 做修改。原本的定義為\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γQ(y,argb∈Amax​Q(y,b,ϵ′′;ζ),ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\"]},\"177\":{\"h\":\"Distributed A3C\",\"t\":[\"由於 A3C 是在 multi-thread 上訓練，因此不太需要考慮上述的 Random Overhead，在這裡採用 Independent Gaussian noise 即可。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 Entropy function 了。\",\"原本的 A3C\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"NoisyNet-A3C\",\"∇ζπ​​Lπ(ζπ​)LV(ζv​)​=−E[Eπ[i=0∑k​∇ζπ​​log(π(at+i​∣st+i​;ζπ​,ϵ))A(st+i​,at+i​;ζv​,ϵ)]]=E[i=0∑k​Eπ[(Qi​−V(st+i​;ζv​,ϵ))∣st+i​]]2​\",\"Noise initialize details\",\"Independent Gaussian noise\",\"μi,j​∼U[−p3​​,+p3​​]\",\"p 是 input 的數量\",\"σi,j​=0.017\",\"Factorised Gaussian noise\",\"μi,j​∼U[−p1​​,+p1​​]\",\"p 是 input 的數量\",\"σi,j​=p​0.5​\"]},\"178\":{\"h\":\"Results\"},\"179\":{\"h\":\"Experiments\",\"t\":[\"實驗是做在 57 Atari games 上。每 1M 個 frames 評估一次，episode 每 108K frames 會 truncate 一次。將沒有做任何修正的 DQN、Dueling DQN、A3C 作為 Baseline。\",\"首先把 Baseline 以及加上 NoisyNet 的模型都跟 Human 比較，底下是用來評估優劣的評分方式。\",\"100×ScoreHuman​−ScoreRandom​Scoreagent​−ScoreRandom​​\",\"Note\",\"最後得出的結果為 0：跟 Random 一樣糟 最後得出的結果為 100：跟 Human 一樣好\",\"可以從分數上明顯看出來加上了 NoisyNet 後對於 Mean 以及 Median 都有正面的影響。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"接下來評估加上 NoisyNet 帶來的影響力，評分方式會也跟 Baseline 比較。\",\"100×max(ScoreHuman​,ScoreBaseline​)−ScoreRandom​Scoreagent​−ScoreBaseline​​\",\"可以看到在大多數的遊戲加上了 NoisyNet 之後的結果都有些進步。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"不過進步主要在 DQN 以及 Dueling 上較為顯著。A3C 的部分在退步也是有幾項退步蠻多，也並不是每次加上 NoisyNet 都會帶來 improvement。\",\"從訓練中的曲線也可以明顯看到 NoisyNet 可以帶來很不錯的 improvement。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"180\":{\"h\":\"Analysis\",\"t\":[\"為了進一步去釐清這樣的做法為什麼是可行、合理的，作者進一步去研究。\",\"回顧一下我們加上 Noise 的方法，是把一個可訓練參數拆成 ζ=(μ,σ)，再額外多一個 Noise ϵ。\",\"θ=defμ+σ⊙ϵ\",\"理想上，我們最後的 Loss 應該要能夠好好收斂，也就是說最後的 solution 應該要是 deterministic。那麼這裡加上的 ϵ 就應該隨著訓練慢慢被忽視，作用只在於訓練的前中期提供 exploration。因此，我們也就會期待 σ 這個參數會漸漸趨近於 0 了！\",\"定義底下的平均\",\"Σˉ=Nweights​1​i∑​∣σiw​∣\",\"作者發現在每一個遊戲當中最後一個 Layer 的 Σˉ 都是會逐漸趨近於 0 的，然而若觀察倒數第二個 Layer 卻並不一定了，有些甚至是遞增的。也就是說，其實 NoisyNet 並不會都得出 deterministic solution。\",\"此外，透過 Σˉ 的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同，也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"181\":{\"h\":\"Contribution\",\"t\":[\"提供一個簡單又有效的 Exploration 方式\",\"能夠在 on-policy 以及 off-policy 上適用\",\"能夠輕易地套用在所有的 RL 算法當中\"]},\"182\":{\"h\":\"值得一看的文章們\",\"t\":[\"强化学习中on-policy 与off-policy有什么区别？\",\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\",\"Asynchronous Methods for Deep Reinforcement Learning\",\"Deep Exploration via Randomized Value Functions\",\"Kalman Temporal Differences\",\"VIME: Variational Information Maximizing Exploration\",\"Parameter Space Noise for Exploration\",\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"Better exploration with parameter noise\",\"强化学习中的探索与利用（count-based)\"]},\"183\":{\"c\":[\"Note\"]},\"184\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"]},\"185\":{\"h\":\"Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\"},\"186\":{\"h\":\"Basic Information\",\"t\":[\"Pan Zhang1, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen @ University of Science and Technology of China, Microsoft Research Asia\",\"2021 CVPR\"]},\"187\":{\"h\":\"問題描述\",\"t\":[\"如同前面看過的 DACS，這一篇 paper 也是想要解決 semantic segmentation 當中 UDA(Unsupervised Domain Adaption) 的問題。\",\"近年來流行 self-training 方法，透過 Pseudo Labelling 的方式來處理。也就是說會在訓練的過程當中透過當下的預測給這些 training data 一個假的 label，然後再拿去訓練。雖然這種做法開始能夠讓 source domain 適應 target domain 了，但比起 supervised learning 與 semi-supervised learning 的 performance 還是相差許多。\",\"作者認為目前的做法存在兩個問題\",\"只選擇信度高於某個閥值的預測作為 pseudo label，但結果不一定正確，會使模型被誤導 例如下圖，圈起來的 - 就被錯誤分類。\",\"Image modified from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"由於 Domain Gap 很大，Network 傾向在 target domain 上產生比較雜亂的特徵 就像是看到有殼的動物就當成是昆蟲，把四隻腳站立的動物都當成草食類動物一樣。認知跟實際充滿巨大的 Gap 導致對結果的特徵很雜亂。\",\"針對上述兩個問題，作者分別使用 Online pseudo labels denoising 以及 learning a compact target structure 來解決。\"]},\"188\":{\"h\":\"Related Works\",\"t\":[\"UDA\",\"Unsupervised representation learning\",\"Learning from noisy labels\",\"Self-training\"]},\"189\":{\"h\":\"Methodology\"},\"190\":{\"h\":\"Preliminary\",\"t\":[\"這裡先定義一下接下來會用到的基本 Notation。\",\"ns​,nt​ 分別表示 source 和 target dataset 的大小\",\"Xs​={xs​}j=1ns​​,Xt​={xt​}j=1nt​​ 分別表示 source 和 target dataset\",\"Ys​={ys​}j=1ns​​,Yt​={yt​}j=1nt​​ 分別表示 source 和 target dataset 對應的 segmentation labels\",\"y^​t​ 表示 pseudo label\",\"Ys​ 和 Yt​ 都有 K 個共通的 classes\"]},\"191\":{\"h\":\"Target\",\"t\":[\"semantic segmentation 當中的 UDA 問題，目標在於給定 Xs​,Xt​,Ys​，不知道 Yt​ 的前提下，去預測 target dataset 的 semantic segmentation。\",\"其中一種方法是採用 Pseudo Labels，透過如 Cross Entropy Loss 來調整模型的機率分布。\",\"lcet​=−i=1∑H×W​k=1∑K​y^​t(i,k)​log(pt(i,k)​)\",\"其中 pt(i,k)​ 是一個 softmax probability 表示 pixel xt(i)​ 是 class k 的機率。 至於 y^​t(i,k)​ 則會直接表示屬於哪一個 class，也就是 hard labels。也額外定義 ξ 來轉換 soft 與 hard labels。\",\"y^​t(i,k)​={1,0,​if k=argmaxk′​pt(i,k′)​otherwise​,yt​^​=ξ(pt​)\",\"而一個 network h 也可以拆成 feature extractor f 以及 classifier g 兩個部分，用 h=f∘g 來表示。\"]},\"192\":{\"h\":\"Prototypical pseudo label denoising\",\"t\":[\"作者認為每經過一個 training stage 才去更新 pseudo label 會太慢，在一個 training stage 當中 network 可能已經 overfit 在那些充滿噪點的 labels，被錯誤的資訊誤導了。\",\"很直覺地，會想要讓 network 的參數更新、pseudo label 的更新 兩個可以同時處理。\",\"然而，若直接同時更新的話，network 會很容易忽略了細部的特徵，進而傾向 overfit 在 source domain，只在 source domain 獲得高的分數。\",\"因此作者提出的方法是將 soft pseudo labels 固定住，對於每個 class k 選擇一個 prototype η(k) 以及一個對應的 weight wt(i,k)​。訓練過程中根據與 prototype 之間的距離去調整 weight，進而影響預測的 pseudo label。\",\"y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​)\",\"wt(i,k)​ 就是上述的 weight\",\"pt,0(i,k)​ 與過去的 soft pseudo label pt,(i,k)​ 稍有不同，整個訓練過程中都會固定住\",\":::success 跟 Clustering 頗類似，每個 cluster 的中心點就如同這裡的 prototype，距離 cluster A 中心點越近，模型就越相信他是屬於 cluster A。\",\"我們會隨著訓練過程慢慢調整 prototype，讓他越來越貼合真實的狀況。\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"當然，這裡的距離是投射到高維空間之後 feature 之間的距離！ :::\"]},\"193\":{\"h\":\"權重計算\",\"t\":[\"權重的計算方式如下\",\"wt(i,k)​=∑k′​exp(−∥f~​(xt​)(i)−η(k′)∥/τ)exp(−∥f~​(xt​)(i)−η(k)∥/τ)​\",\"f(xt​)(i) 表示第 i 筆 target data 的 feature\",\"f~​(xt​)(i) 是 momentum encoder，可以看成是更新較慢的 encoder\",\"η(k) 表示 class k 的特徵中心點\",\"τ 表示 softmax temperature，這裡設為 1\",\"權重的計算方式本質上就是 softmax function。計算的是 feature 跟每個 class 的中心點 η(k) 距離遠近。\",\"當距離很大時，產出的權重就會很小\",\"當距離很小時，產出的權重就會很大\"]},\"194\":{\"h\":\"prototype 計算\",\"t\":[\"而 prototype 的計算方式如下\",\"η(k)=∑xt​∈Xt​​∑i​1(y^​t(i,k)​==1)∑xt​∈Xt​​∑i​f(xt​)(i)∗1(y^​t(i,k)​==1)​\",\"Prototype 的計算本質上就是找中心點。把所有對應到 class k 的 feature 加總後平均。\",\"然而這種做法每次要更新 prototype 就需要看過整個 target dataset 的所有 features，計算上負擔過大。因此作者改用一個 mini-batch 當中中心點的 moving average 來估計 (Exponential Moving Average, EMA)。\",\"η(k)←λη(k)+(1−λ)η′(k)\",\"η′(k) 表示當前 mini-batch 當中 class k 的 feature 平均\",\"λ=0.9999\"]},\"195\":{\"h\":\"Loss 計算\",\"t\":[\"至此我們有了新的方法取得 pseudo labels (也就是 y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​))，最後就剩下更新的 Loss 如何計算。\",\"與傳統的 Cross Entropy(CE) 不同，這裡作者採用 Symmetric Cross Entropy(SCE) 試圖增加對 lebel noise 的容忍程度。\",\"lscet​=αlce​(pt​,y^​t​)+βlce​(y^​t​,pt​)\",\"α=0.1\",\"β=1\",\":::success 改成透過 prototype 去調整 pseudo label 能夠帶來許多的好處\",\"對 outlier 比較不敏感\",\"每個 class 都是平等的，較不會因為 class 的不平衡導致預測錯誤 \",\"在 semantic segmentation 中這一點尤其重要，因為 class 的分布往往分散\",\"實際上對於 hard class 的預測有改善\",\"對於一開始預測錯誤的 pseudo labels 能夠漸漸改正 :::\"]},\"196\":{\"h\":\"Structure learning by enforcing consistency\",\"t\":[\"理想上，只要我們的 feature extractor 能好好表示出 feature，即便在 target domain 上也能好好地區分不同的 class，那麼 pseudo label 也就可以更好地減輕 noise 的影響。\",\"然而因為 Domain Adaption 尤其 UDA 對於 target domain 的認識嚴重缺乏，encode 出來的 features 往往會很分散。\",\"作者透過對所擁有的 target domain 知識增強來改善，並且分成了弱增強 T(xt​) 以及 強增強 T′(xt​)。實際上，弱增強只是給原圖，強增強只是加上 data augumentation。\",\"這裡使用的 Data Augumentation 包含了旋轉、明暗調整、彩度調整等\",\"我們的目標是要讓 T(xt​) 與 T′(xt​) 對應的 feature 可以比較接近。作者分別去計算兩者的 weight zT​ 與 zT′​ (稱為 Soft prototypical assignment)，試圖讓他們產出的分布要越接近越好。因此透過 KL divergence 去計算 loss lklt​。\",\"lklt​=KL(zT​∥zT′​)\",\"其中\",\"zT(i,k)​=∑k′​exp(−∥f~​(T(xt​))(i)−η(k′)∥/τ)exp(−∥f~​(T(xt​))(i)−η(k)∥/τ)​,zT′(i,k)​=∑k′​exp(−∥f(T(xt​))(i)−η(k′)∥/τ)exp(−∥f(T(xt​))(i)−η(k)∥/τ)​\",\"兩者差異只在於使用的 encoder 分別是 f 和 f~​。因為 zT​ 是由弱增強得到，受到的干擾較少，因此適合用來教 encoder 經過強增強的 prototype assignment 應該與經過弱增強的相同。\",\"ProDA\",\":::success 如此一來，就能夠迫使模型對於這些略有不同的 feature 具有相同的 pseudo label，使得 target domain features 更加密集。 :::\",\"最後，為了避免所謂的 degeneration issue，也就是有些 cluster 是空的狀態，作者進一步設計一個 loss lregt​ 鼓勵類別盡量地平均。\",\"lregt​=−i=1∑H×W​j=1∑K​logpt(i,k)​\",\"將上述的種種 loss 結合，合併成底下的 loss ltotal​。\",\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",\"γ1​=10\",\"γ2​=0.1\"]},\"197\":{\"h\":\"Distillation to self-supervised model\",\"t\":[\"最後，作者進一步加上了 Knowledge Distillation，使最終的結果進一步提昇。\",\"透過前面的步驟得到的模型稱為 h，會做為 KD 當中的 Teacher Model。\",\"要訓練的 Student Model 稱為 h†。會採用 SimCLRv2 的 pretrain weights 開始訓練。\",\"為了避免 student model 忘記 source domain 的知識，所以也會把 source domain 的資料拿進來使用。整體的 Loss lKD​ 計算方式如下。\",\"lKD​=lces​(ps​,py​)+lcet​(pt†​,ξ(pt​))+βKL(pt​∥pt†​)\"]},\"198\":{\"h\":\"整體流程\",\"t\":[\"整體流程被分成三個階段。\",\"第一階段包含了 Prototypical Pseudo Label Denoising 以及 Target Structure Learning。 目的是要先訓練出一個 Teacher Model。意即讓 ltotoal​ 收斂。\",\"ProDA_Losses\",\"第二與第三階段都是 Knowledge Distillation。 目的是要訓練出一個 Student Model。意即讓 lKD​ 收斂。\",\"ProDA_Losses2\",\"Caution\",\"根據 Appendix 附上的 algorithm，實際上他所謂的 避免模型忘記 source domain 只不過是把 source data 扔進 \\\"teacher model\\\"，取得 lces​(ps​,py​)，然後拿去 tune \\\"student model\\\"。\",\"但是從頭到尾都沒丟給 student model，為甚麼可以直接拿去 tune，並且預期能夠讓 student model \\\"學會 source domain\\\" 的資料?\"]},\"199\":{\"h\":\"Results\"},\"200\":{\"h\":\"實驗設定\",\"t\":[\"Segmentation 模型採用 DeepLabv2 搭配 ResNet-101 Backbone。\",\"訓練前首先透過 AdaptSegNet 搭配對抗式學習對 segmentation 模型 warmup。\",\"Knowledge Distillation 的部分採用了 pretrained SimCLRv2 搭配 ResNet-101 backbone。\",\"Dataset 的部分一如既往採用 GTA5、SYNTHIA、Cityscapes 這三個 datasets。之前在 DACS 也有介紹過同樣的 datasets。\",\"接下來直接看 ProDA 在兩個 benchmarks 上面的表現。\"]},\"201\":{\"h\":\"GTA5 → \\\\rightarrow → Cityscapes\",\"t\":[\"在 GTA5 → Cityscapes 的部分明顯可以看到最後的 mIOU 比起過去的 SOTA models 好許多。在絕大多數的類別當中也是比起過去的做法還要強。\",\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升。因為我們現在對每一個 classes 都是平等對待所導致。\",\"image\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"上半部分是用 domain alignment 的解決方案，下面則是 self-training。\"]},\"202\":{\"h\":\"SYNTHIA → \\\\rightarrow → Cityscapes\",\"t\":[\"在 SYNTHIA → Cityscapes 的部分如下，同樣可以看到 mIoU 比起過去的 SOTA 有不少的提升。\",\"image\"]},\"203\":{\"h\":\"Contribution\",\"t\":[\"提出一個可以即時修正 psuedo label 的方法(prototypes)\",\"展示知識蒸餾(Knowledge Distillation)在 UDA 上同樣可以獲得更多改善\",\"提出一個新的 UDA for semantic segmentation 的 SOTA model ProDA\"]},\"204\":{\"h\":\"值得一看的文章們\",\"t\":[\"［論文筆記］Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\",\"UDA 論文 HackMD 筆記\",\"剖析深度學習 (2)：你知道Cross Entropy和KL Divergence代表什麼意義嗎？談機器學習裡的資訊理論\",\"【读】领域自适应语义分割 - ProDA\",\"論文筆記 — Momentum Contrast for Unsupervised Visual Representation Learning(MOCO)\",\"Momentum Contrast for Unsupervised Visual Representation Learning\",\"Symmetric Cross Entropy\"]},\"205\":{\"c\":[\"Note\"]},\"206\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"CVPR\"]},\"207\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"领域自适应语义分割\",{\"1\":{\"204\":1}}],[\"读\",{\"1\":{\"204\":1}}],[\"談機器學習裡的資訊理論\",{\"1\":{\"204\":1}}],[\"剖析深度學習\",{\"1\":{\"204\":1}}],[\"展示知識蒸餾\",{\"1\":{\"203\":1}}],[\"扔進\",{\"1\":{\"198\":1}}],[\"附上的\",{\"1\":{\"198\":1}}],[\"附近\",{\"1\":{\"136\":1}}],[\"第二與第三階段都是\",{\"1\":{\"198\":1}}],[\"第一階段包含了\",{\"1\":{\"198\":1}}],[\"第一個能夠在所有\",{\"1\":{\"27\":1}}],[\"收斂\",{\"1\":{\"198\":2}}],[\"收斂性相關研究\",{\"0\":{\"112\":1},\"1\":{\"109\":1}}],[\"鼓勵類別盡量地平均\",{\"1\":{\"196\":1}}],[\"鼓勵去探索那些在\",{\"1\":{\"6\":1}}],[\"受到的干擾較少\",{\"1\":{\"196\":1}}],[\"稱為\",{\"1\":{\"196\":1,\"197\":1}}],[\"彩度調整等\",{\"1\":{\"196\":1}}],[\"明暗調整\",{\"1\":{\"196\":1}}],[\"弱增強只是給原圖\",{\"1\":{\"196\":1}}],[\"強增強只是加上\",{\"1\":{\"196\":1}}],[\"強增強\",{\"1\":{\"196\":1}}],[\"知識增強來改善\",{\"1\":{\"196\":1}}],[\"較不會因為\",{\"1\":{\"195\":1}}],[\"較不具有彈性\",{\"1\":{\"22\":1}}],[\"試圖讓他們產出的分布要越接近越好\",{\"1\":{\"196\":1}}],[\"試圖增加對\",{\"1\":{\"195\":1}}],[\"試著用那一個人的做法走過一次\",{\"1\":{\"168\":1}}],[\"←λη\",{\"1\":{\"194\":1}}],[\"∗1\",{\"1\":{\"194\":1}}],[\"∗y\",{\"1\":{\"129\":1}}],[\"∑xt​∈xt​​∑i​f\",{\"1\":{\"194\":1}}],[\"∑k−1​rk​\",{\"1\":{\"15\":1}}],[\"∑k−1​1\",{\"1\":{\"15\":1}}],[\"產出的權重就會很大\",{\"1\":{\"193\":1}}],[\"產出的權重就會很小\",{\"1\":{\"193\":1}}],[\"產生一個新的\",{\"1\":{\"68\":1}}],[\"產生出選擇\",{\"1\":{\"13\":1}}],[\"∥\",{\"1\":{\"193\":2,\"196\":4}}],[\"∥2​\",{\"1\":{\"93\":1}}],[\"權重的計算方式本質上就是\",{\"1\":{\"193\":1}}],[\"權重的計算方式如下\",{\"1\":{\"193\":1}}],[\"權重計算\",{\"0\":{\"193\":1}}],[\"距離遠近\",{\"1\":{\"193\":1}}],[\"距離\",{\"1\":{\"192\":1}}],[\"頗類似\",{\"1\":{\"192\":1}}],[\"頗偏\",{\"1\":{\"79\":1}}],[\"稍有不同\",{\"1\":{\"192\":1}}],[\"稍微修改了這個做法\",{\"1\":{\"114\":1}}],[\"稍微翻了一下\",{\"1\":{\"3\":1}}],[\"η\",{\"1\":{\"192\":1,\"193\":2,\"194\":4}}],[\"ηt​=ηbase​⋅t\",{\"1\":{\"94\":1}}],[\"獲得高的分數\",{\"1\":{\"192\":1}}],[\"獲得的\",{\"1\":{\"117\":1}}],[\"若直接同時更新的話\",{\"1\":{\"192\":1}}],[\"若已經又經過\",{\"1\":{\"19\":1}}],[\"認知跟實際充滿巨大的\",{\"1\":{\"187\":1}}],[\"認識與理解\",{\"1\":{\"140\":1}}],[\"圈起來的\",{\"1\":{\"187\":1}}],[\"適應\",{\"1\":{\"187\":1}}],[\"適應不同的環境\",{\"1\":{\"24\":1}}],[\"强化学习中的探索与利用\",{\"1\":{\"182\":1}}],[\"强化学习中on\",{\"1\":{\"182\":1}}],[\"与off\",{\"1\":{\"182\":1}}],[\"方式\",{\"1\":{\"181\":1}}],[\"方法通常會將輸入的圖片經過一個\",{\"1\":{\"148\":1}}],[\"方法處理的大架構如下圖所示\",{\"1\":{\"148\":1}}],[\"方法甚至大幅超越了過去他們提出的\",{\"1\":{\"145\":1}}],[\"方法如\",{\"1\":{\"130\":1}}],[\"方法\",{\"1\":{\"68\":1,\"91\":1,\"187\":1}}],[\"方法也是屬於\",{\"1\":{\"68\":1}}],[\"回顧一下我們加上\",{\"1\":{\"180\":1}}],[\"合併成底下的\",{\"1\":{\"196\":1}}],[\"合理的\",{\"1\":{\"180\":1}}],[\"合成出\",{\"1\":{\"68\":2}}],[\"評分方式會也跟\",{\"1\":{\"179\":1}}],[\"評估一次\",{\"1\":{\"179\":1}}],[\"評估方式\",{\"0\":{\"117\":1}}],[\"現在替換成\",{\"1\":{\"176\":2}}],[\"現在\",{\"1\":{\"176\":1,\"177\":1}}],[\"現在即便丟到未知的環境當中\",{\"1\":{\"136\":1}}],[\"拆分\",{\"1\":{\"175\":1}}],[\"拆開來解決\",{\"1\":{\"22\":1}}],[\"拆開來\",{\"1\":{\"12\":1}}],[\"假如\",{\"1\":{\"175\":1}}],[\"減少產\",{\"0\":{\"175\":1}}],[\"減少overfitting的狀況\",{\"1\":{\"56\":1}}],[\"意即讓\",{\"1\":{\"198\":2}}],[\"意即在過程中不會對\",{\"1\":{\"174\":1}}],[\"意義上就是在看每個\",{\"1\":{\"171\":1}}],[\"≈∇μ\",{\"1\":{\"174\":1}}],[\"ξ\",{\"1\":{\"174\":1,\"191\":1,\"197\":1}}],[\"∇ζπ​​lπ\",{\"1\":{\"177\":1}}],[\"∇μ\",{\"1\":{\"174\":1}}],[\"∇lˉ\",{\"1\":{\"174\":2}}],[\"∇θπ​​lπ\",{\"1\":{\"172\":1,\"177\":1}}],[\"刻意挑\",{\"1\":{\"174\":1}}],[\"穩定地訓練\",{\"1\":{\"172\":1}}],[\"∂θv\",{\"1\":{\"172\":1}}],[\"∂w∂l​​wi​​\",{\"1\":{\"50\":1}}],[\"紅色的部分也就是前面提及的\",{\"1\":{\"172\":1}}],[\"紅綠燈\",{\"1\":{\"155\":1}}],[\"執行\",{\"1\":{\"172\":1}}],[\"參照\",{\"1\":{\"172\":1}}],[\"參數越多就有越強大的表達能力\",{\"1\":{\"124\":1}}],[\"步的更新\",{\"1\":{\"172\":1}}],[\"往後看\",{\"1\":{\"172\":1}}],[\"往往會很分散\",{\"1\":{\"196\":1}}],[\"往往會有許多我們沒有的\",{\"1\":{\"63\":1}}],[\"往往假設資料的分布會維持住\",{\"1\":{\"108\":1}}],[\"往往在邊界上會有許多誤判的\",{\"1\":{\"70\":1}}],[\"往往先從貼標籤開始\",{\"1\":{\"67\":1}}],[\"往往就會\",{\"1\":{\"64\":1}}],[\"沒有差異的問題\",{\"1\":{\"171\":1}}],[\"藍線\",{\"1\":{\"170\":1}}],[\"藍色線\",{\"1\":{\"99\":1}}],[\"橘線是綠線與紫線的誤差\",{\"1\":{\"170\":1}}],[\"橘色線\",{\"1\":{\"99\":1}}],[\"紫線是目標函數\",{\"1\":{\"170\":1}}],[\"紫色圓點表示\",{\"1\":{\"22\":1}}],[\"原本的定義為\",{\"1\":{\"176\":1}}],[\"原本的\",{\"1\":{\"176\":1,\"177\":1}}],[\"原始\",{\"1\":{\"170\":1,\"172\":1}}],[\"原先\",{\"1\":{\"17\":1}}],[\"維度過大訓練困難的問題\",{\"1\":{\"169\":1}}],[\"替代\",{\"1\":{\"169\":1}}],[\"帶來了幾個好處\",{\"1\":{\"169\":1,\"171\":1,\"172\":1}}],[\"帶來的幾個好處\",{\"1\":{\"170\":1}}],[\"帶來的影響力\",{\"1\":{\"179\":1}}],[\"帶來的影響後發現\",{\"1\":{\"156\":1}}],[\"帶來的影響耶\",{\"1\":{\"133\":1}}],[\"帶來的好處\",{\"1\":{\"133\":1}}],[\"帶來的效益\",{\"1\":{\"25\":1}}],[\"∼d​\",{\"1\":{\"169\":1,\"171\":1,\"176\":4}}],[\"∼ε​\",{\"1\":{\"110\":1}}],[\"底下是用來評估優劣的評分方式\",{\"1\":{\"179\":1}}],[\"底下你預期可以拿到多好的\",{\"1\":{\"171\":1}}],[\"底下的內容只是單純的\",{\"1\":{\"168\":1}}],[\"底下使用不同架構時會再特別提醒\",{\"1\":{\"154\":1}}],[\"核心的概念很簡單\",{\"1\":{\"168\":1}}],[\"達到鼓勵\",{\"1\":{\"168\":1}}],[\"年由\",{\"1\":{\"168\":1}}],[\"效率與品質的問題\",{\"1\":{\"166\":1}}],[\"論文\",{\"1\":{\"204\":1}}],[\"論文中更新一步\",{\"1\":{\"172\":1}}],[\"論文中有提及在\",{\"1\":{\"166\":1}}],[\"論文筆記\",{\"1\":{\"91\":1,\"103\":1,\"161\":1,\"204\":2}}],[\"研究了\",{\"1\":{\"160\":1}}],[\"研究不同的\",{\"1\":{\"102\":1}}],[\"縮小\",{\"1\":{\"158\":1}}],[\"註\",{\"1\":{\"158\":1}}],[\"比起過去的\",{\"1\":{\"201\":1,\"202\":1}}],[\"比起單純的\",{\"1\":{\"158\":1}}],[\"比對\",{\"1\":{\"156\":2}}],[\"比較不敏感\",{\"1\":{\"195\":1}}],[\"比較\",{\"1\":{\"179\":2}}],[\"比較好\",{\"1\":{\"168\":1}}],[\"比較之後可以發現到\",{\"1\":{\"158\":1}}],[\"比較的結果如下圖所示\",{\"1\":{\"155\":1}}],[\"比較的部份為了公平有時會將\",{\"1\":{\"154\":1}}],[\"比較基準\",{\"0\":{\"118\":1}}],[\"比較常被batch\",{\"1\":{\"43\":1}}],[\"比較差的結果\",{\"1\":{\"21\":1}}],[\"比較傾向去試試看那些不熟的\",{\"1\":{\"6\":1}}],[\"比較大的時候\",{\"1\":{\"6\":1}}],[\"倍\",{\"1\":{\"156\":1}}],[\"調低\",{\"1\":{\"156\":1}}],[\"調整\",{\"1\":{\"71\":1,\"172\":1}}],[\"巴士\",{\"1\":{\"155\":1}}],[\"騎士\",{\"1\":{\"155\":1}}],[\"→\",{\"0\":{\"201\":2,\"202\":2},\"1\":{\"155\":2,\"156\":1,\"201\":1,\"202\":1}}],[\"換成\",{\"1\":{\"154\":1}}],[\"換句話說\",{\"1\":{\"67\":1}}],[\"重疊的部分以平均作為結果\",{\"1\":{\"152\":1}}],[\"重複\",{\"1\":{\"19\":1}}],[\"亦然\",{\"1\":{\"151\":1}}],[\"亦即\",{\"1\":{\"13\":1,\"25\":1}}],[\"額外考慮\",{\"1\":{\"151\":1}}],[\"額外的\",{\"1\":{\"145\":1}}],[\"別忘了\",{\"1\":{\"151\":1}}],[\"⊙y^​d\",{\"1\":{\"151\":1}}],[\"⊙y^​c​\",{\"1\":{\"151\":1}}],[\"外面補\",{\"1\":{\"151\":1}}],[\"寫成數學式也許比較難理解\",{\"1\":{\"151\":1}}],[\"設為\",{\"1\":{\"151\":1}}],[\"設定請詳閱\",{\"1\":{\"96\":1}}],[\"設定如下\",{\"1\":{\"6\":1}}],[\"越大\",{\"1\":{\"156\":2,\"157\":2}}],[\"越大變得越小\",{\"1\":{\"22\":1}}],[\"越靠近\",{\"1\":{\"151\":2}}],[\"σˉ\",{\"1\":{\"180\":2}}],[\"σˉ=nweights​1​i∑​∣σiw​∣\",{\"1\":{\"180\":1}}],[\"σi\",{\"1\":{\"177\":2}}],[\"σ​l\",{\"1\":{\"174\":2}}],[\"σ2i\",{\"1\":{\"168\":1}}],[\"σ\",{\"1\":{\"151\":1,\"174\":3,\"180\":2}}],[\"又分別要給多少的注意力\",{\"1\":{\"151\":1}}],[\"又有不少的\",{\"1\":{\"3\":1}}],[\"機制去判斷現在應該要注重\",{\"1\":{\"151\":1}}],[\"機器\",{\"1\":{\"58\":1}}],[\"保證選出來的座標可以被\",{\"1\":{\"150\":1}}],[\"變成跟\",{\"1\":{\"151\":1}}],[\"變成最後的預測\",{\"1\":{\"149\":1}}],[\"變化\",{\"1\":{\"100\":1}}],[\"混合\",{\"1\":{\"149\":1,\"159\":1}}],[\"混合成新的圖片\",{\"1\":{\"68\":1}}],[\"整體流程被分成三個階段\",{\"1\":{\"198\":1}}],[\"整體流程\",{\"0\":{\"198\":1}}],[\"整體的\",{\"1\":{\"197\":1}}],[\"整體的架構如下圖\",{\"1\":{\"149\":1}}],[\"整除\",{\"1\":{\"150\":1}}],[\"整個訓練過程中都會固定住\",{\"1\":{\"192\":1}}],[\"整個\",{\"1\":{\"128\":1}}],[\"整個問題\",{\"1\":{\"110\":1}}],[\"記憶體過量\",{\"1\":{\"149\":1}}],[\"記錄在\",{\"1\":{\"114\":1}}],[\"關於\",{\"1\":{\"148\":1}}],[\"關注在\",{\"1\":{\"145\":1}}],[\"∈∣a∣∑​a\",{\"1\":{\"171\":1}}],[\"∈\",{\"1\":{\"151\":1}}],[\"∈rohd​​×owd​​×c\",{\"1\":{\"150\":1}}],[\"∈rohc​​×owc​​×c\",{\"1\":{\"150\":1}}],[\"∈rst​ht​​×st​wt​​×3\",{\"1\":{\"148\":1}}],[\"∈cthings​\",{\"1\":{\"93\":1}}],[\"ζv​\",{\"1\":{\"177\":3}}],[\"ζπ​\",{\"1\":{\"177\":2}}],[\"ζ−\",{\"1\":{\"176\":2}}],[\"ζ=\",{\"1\":{\"174\":1,\"180\":1}}],[\"ζ\",{\"1\":{\"148\":1,\"150\":1,\"174\":3,\"176\":5}}],[\"則會直接表示屬於哪一個\",{\"1\":{\"191\":1}}],[\"則越相信\",{\"1\":{\"151\":2}}],[\"則透過\",{\"1\":{\"148\":1}}],[\"則是把輸出乘上機率\",{\"1\":{\"129\":1}}],[\"則是現實世界當中的影像\",{\"1\":{\"74\":1}}],[\"則是overlapping\",{\"1\":{\"44\":1}}],[\"則是傳統的non\",{\"1\":{\"44\":1}}],[\"創造出在兩個狀況下都能夠順利辨認的方法\",{\"1\":{\"145\":1}}],[\"資訊\",{\"1\":{\"145\":1,\"152\":2}}],[\"資料具有偏差\",{\"1\":{\"124\":1}}],[\"資料不足\",{\"1\":{\"124\":1}}],[\"架構\",{\"1\":{\"145\":1,\"152\":1,\"167\":1}}],[\"架構對於\",{\"1\":{\"102\":1}}],[\"探討\",{\"1\":{\"139\":1}}],[\"探索機率更高\",{\"1\":{\"14\":1}}],[\"探索機率高\",{\"1\":{\"14\":1}}],[\"擷取出的特徵圖如上\",{\"1\":{\"136\":1}}],[\"反而彼此會難以配合\",{\"1\":{\"136\":1}}],[\"反之在\",{\"1\":{\"168\":1}}],[\"反之大的解析度會不好預測大物件\",{\"1\":{\"156\":1}}],[\"反之會走出最短路\",{\"1\":{\"22\":1}}],[\"反之則越相信\",{\"1\":{\"151\":2}}],[\"反之則相遠\",{\"1\":{\"63\":1}}],[\"反之則是\",{\"1\":{\"22\":1}}],[\"反之則會去走那些比較熟悉的\",{\"1\":{\"6\":1}}],[\"修正\",{\"1\":{\"136\":1}}],[\"錯誤\",{\"1\":{\"136\":1}}],[\"篇\",{\"1\":{\"135\":1}}],[\"種大主題\",{\"1\":{\"134\":1}}],[\"種不同的類別\",{\"1\":{\"133\":1}}],[\"種不同模型一樣\",{\"1\":{\"128\":1}}],[\"要訓練的\",{\"1\":{\"197\":1}}],[\"要先乘\",{\"1\":{\"130\":1}}],[\"要傾向\",{\"1\":{\"14\":2}}],[\"梯度會是\",{\"1\":{\"130\":1}}],[\"梯度最佳解相關算法\",{\"1\":{\"58\":1}}],[\"組成的模型就像是有\",{\"1\":{\"128\":1}}],[\"測試過程中讓每個\",{\"1\":{\"128\":1}}],[\"藉此確保後續的運作正常\",{\"1\":{\"150\":1}}],[\"藉此讓模型能夠學習到更多的特徵\",{\"1\":{\"126\":1}}],[\"藉此避免neuron間co\",{\"1\":{\"37\":1}}],[\"理想上\",{\"1\":{\"180\":1,\"196\":1}}],[\"理想上將輸入經過\",{\"1\":{\"126\":1}}],[\"理應因此得到較好的結果\",{\"1\":{\"93\":1}}],[\"很直覺地\",{\"1\":{\"192\":1}}],[\"很大\",{\"1\":{\"187\":1}}],[\"很顯然地\",{\"1\":{\"124\":1}}],[\"很多時候我們並不會直接去蒐集真實的資料\",{\"1\":{\"63\":1}}],[\"本身\",{\"1\":{\"124\":1}}],[\"本研究使用ilsvrc\",{\"1\":{\"39\":1}}],[\"本研究訓練的deep\",{\"1\":{\"38\":1}}],[\"證明了\",{\"1\":{\"118\":1}}],[\"事先處理好了\",{\"1\":{\"118\":1}}],[\"事先標記好\",{\"1\":{\"118\":1}}],[\"類似\",{\"1\":{\"118\":1}}],[\"人工提取\",{\"1\":{\"118\":1}}],[\"人類\",{\"1\":{\"111\":1}}],[\"人類的\",{\"1\":{\"3\":2}}],[\"即可\",{\"1\":{\"117\":1,\"177\":1}}],[\"即便自己知道當下用哪一個\",{\"1\":{\"168\":1}}],[\"即便只用了\",{\"1\":{\"158\":1}}],[\"即便在\",{\"1\":{\"91\":1,\"196\":1}}],[\"即便在參數比較異常的狀況下仍然能有很不錯的學習成果\",{\"1\":{\"24\":1}}],[\"即便這些\",{\"1\":{\"91\":1}}],[\"即便這個環境設定是相當簡單的\",{\"1\":{\"22\":1}}],[\"即便\",{\"1\":{\"24\":1,\"66\":1,\"145\":1}}],[\"才去更新\",{\"1\":{\"192\":1}}],[\"才會去擷取畫面\",{\"1\":{\"116\":1}}],[\"才能支撐更大\",{\"1\":{\"55\":1}}],[\"順利避免了最初提及的兩個問題\",{\"1\":{\"114\":1}}],[\"ϕ\",{\"1\":{\"114\":1}}],[\"ϕt+1​←αϕt​+\",{\"1\":{\"90\":1}}],[\"筆記\",{\"1\":{\"204\":1}}],[\"筆\",{\"1\":{\"114\":1,\"193\":1}}],[\"固定住\",{\"1\":{\"192\":1}}],[\"固定使用的\",{\"1\":{\"157\":1}}],[\"固定為\",{\"1\":{\"116\":1}}],[\"固定\",{\"1\":{\"112\":1,\"137\":2}}],[\"舒緩\",{\"1\":{\"112\":1}}],[\"發表在\",{\"1\":{\"168\":1}}],[\"發散\",{\"1\":{\"112\":1}}],[\"發現到\",{\"1\":{\"101\":1}}],[\"算法當中\",{\"1\":{\"181\":1}}],[\"算法\",{\"1\":{\"111\":1}}],[\"既然我們想要得到全部\",{\"1\":{\"152\":1}}],[\"既然\",{\"1\":{\"110\":1}}],[\"既然有\",{\"1\":{\"8\":1}}],[\"應用在過去各種\",{\"1\":{\"155\":1}}],[\"應用在\",{\"1\":{\"108\":1}}],[\"應該與經過弱增強的相同\",{\"1\":{\"196\":1}}],[\"應該要是\",{\"1\":{\"180\":1}}],[\"應該要能夠好好收斂\",{\"1\":{\"180\":1}}],[\"應該要慢慢變大\",{\"1\":{\"117\":1}}],[\"應該要給\",{\"1\":{\"3\":1}}],[\"應該可以不用做這件事情\",{\"1\":{\"42\":1}}],[\"好許多\",{\"1\":{\"201\":1}}],[\"好像不太能直接看出\",{\"1\":{\"133\":1}}],[\"好\",{\"1\":{\"108\":1}}],[\"能好好表示出\",{\"1\":{\"196\":1}}],[\"能辨識細節的優點\",{\"1\":{\"145\":1,\"150\":1}}],[\"能辨識大範圍特徵的優點\",{\"1\":{\"145\":1,\"150\":1}}],[\"能不能把\",{\"1\":{\"108\":1}}],[\"能夠漸漸改正\",{\"1\":{\"195\":1}}],[\"能夠帶來許多的好處\",{\"1\":{\"195\":1}}],[\"能夠輕易地套用在所有的\",{\"1\":{\"181\":1}}],[\"能夠使用\",{\"1\":{\"172\":1}}],[\"能夠使模型更好去學習各自的\",{\"1\":{\"12\":1}}],[\"能夠透過在訓練當中忽略其他的\",{\"1\":{\"136\":1}}],[\"能夠在\",{\"1\":{\"181\":1}}],[\"能夠在所有的\",{\"1\":{\"25\":1}}],[\"能夠在上述\",{\"1\":{\"5\":1}}],[\"能夠上\",{\"1\":{\"22\":1}}],[\"能夠調整自己的\",{\"1\":{\"16\":1}}],[\"能夠盡可能去正確探索環境\",{\"1\":{\"3\":1}}],[\"語音資料等\",{\"1\":{\"108\":1}}],[\"視覺影像\",{\"1\":{\"108\":1}}],[\"阅读笔记\",{\"1\":{\"103\":1}}],[\"仍然有更好的\",{\"1\":{\"101\":1}}],[\"儘管\",{\"1\":{\"101\":1}}],[\"另外也考慮到\",{\"1\":{\"100\":1}}],[\"另一方面\",{\"1\":{\"13\":1,\"22\":1,\"24\":1}}],[\"太強\",{\"1\":{\"100\":1}}],[\"後對於\",{\"1\":{\"179\":1}}],[\"後的\",{\"1\":{\"136\":1}}],[\"後都可以獲得很棒的結果\",{\"1\":{\"135\":1}}],[\"後續還有一些類似的實驗用來驗證在各個領域採用\",{\"1\":{\"135\":1}}],[\"後續的研究中則發現到\",{\"1\":{\"112\":1}}],[\"後從\",{\"1\":{\"135\":1}}],[\"後\",{\"1\":{\"99\":1}}],[\"後可以得到更好的成效\",{\"1\":{\"24\":1}}],[\"預測\",{\"1\":{\"152\":1}}],[\"預測的機率\",{\"1\":{\"148\":1}}],[\"預測的\",{\"1\":{\"100\":1}}],[\"預測的結果並不是每一個點都會考慮到\",{\"1\":{\"152\":1}}],[\"預測的結果\",{\"1\":{\"99\":1}}],[\"預設是不會知道的\",{\"1\":{\"90\":1}}],[\"已經有比\",{\"1\":{\"158\":1}}],[\"已經放不進\",{\"1\":{\"158\":1}}],[\"已經能夠在這種資料上去擷取特徵\",{\"1\":{\"108\":1}}],[\"已經\",{\"1\":{\"96\":1}}],[\"已知\",{\"1\":{\"14\":1}}],[\"幾乎都是訓練在\",{\"1\":{\"93\":1}}],[\"忘記\",{\"1\":{\"93\":1,\"197\":1}}],[\"希望他們在\",{\"1\":{\"93\":1}}],[\"希望改善這兩個對\",{\"1\":{\"3\":1}}],[\"搞壞所導致\",{\"1\":{\"93\":1}}],[\"被錯誤的資訊誤導了\",{\"1\":{\"192\":1}}],[\"被\",{\"1\":{\"92\":1,\"99\":1}}],[\"被預測成\",{\"1\":{\"70\":2}}],[\"出\",{\"1\":{\"175\":1}}],[\"出現在預測機率最高的前\",{\"1\":{\"133\":1}}],[\"出現的頻率\",{\"1\":{\"92\":1}}],[\"出來的結果會比沒有調整還要降低約\",{\"1\":{\"156\":1}}],[\"出來的\",{\"1\":{\"12\":1,\"196\":1}}],[\"干擾形成\",{\"1\":{\"92\":1}}],[\"數量\",{\"1\":{\"129\":1}}],[\"數量以及大小\",{\"1\":{\"91\":1}}],[\"數量相同\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"給出更穩健的\",{\"1\":{\"152\":1}}],[\"給出的幫助開始出現\",{\"1\":{\"138\":1}}],[\"給出的輸出取平均\",{\"1\":{\"124\":1}}],[\"給出不同\",{\"1\":{\"91\":1}}],[\"給你學習\",{\"1\":{\"13\":1}}],[\"根據\",{\"1\":{\"172\":1,\"198\":1}}],[\"根據實驗的結果我們選擇使用\",{\"1\":{\"91\":1}}],[\"根據過去的研究\",{\"1\":{\"90\":1}}],[\"再額外多一個\",{\"1\":{\"180\":1}}],[\"再多搭配另一個\",{\"1\":{\"157\":1}}],[\"再多一點調整後可以再提升約\",{\"1\":{\"97\":1}}],[\"再使用\",{\"1\":{\"152\":1}}],[\"再使用不同的\",{\"1\":{\"91\":1}}],[\"再結合\",{\"1\":{\"151\":1}}],[\"再進一步裁切得到\",{\"1\":{\"150\":1}}],[\"再依據得到的affinity\",{\"1\":{\"91\":1}}],[\"再次說明了\",{\"1\":{\"25\":1}}],[\"階段能夠動態地依據當下的輸入資料的相似性來產生對應的affinity\",{\"1\":{\"91\":1}}],[\"圖中圈起來的是各種交通工具\",{\"1\":{\"91\":1}}],[\"圖片中以\",{\"1\":{\"24\":1}}],[\"進一步提升效能\",{\"1\":{\"159\":1}}],[\"進一步提高\",{\"1\":{\"100\":1}}],[\"進一步產出合併結果\",{\"1\":{\"159\":1}}],[\"進一步去看每個\",{\"1\":{\"156\":1}}],[\"進一步去研究不同大小的\",{\"1\":{\"91\":1}}],[\"進而影響預測的\",{\"1\":{\"192\":1}}],[\"進而影響到結果\",{\"1\":{\"22\":1}}],[\"進而傾向\",{\"1\":{\"192\":1}}],[\"進而避免\",{\"1\":{\"136\":1}}],[\"進而去完成許多複雜的任務\",{\"1\":{\"108\":1}}],[\"進而產生這樣的結果\",{\"1\":{\"100\":1}}],[\"進而提出\",{\"1\":{\"99\":1}}],[\"進而解決這個問題\",{\"1\":{\"68\":1}}],[\"進而使得整體訓練採用的\",{\"1\":{\"13\":1}}],[\"否則給\",{\"1\":{\"90\":1,\"148\":1}}],[\"至此我們有了新的方法取得\",{\"1\":{\"195\":1}}],[\"至於\",{\"1\":{\"191\":1}}],[\"至於那些被忽略的\",{\"1\":{\"116\":1}}],[\"至於詳細的\",{\"1\":{\"96\":1}}],[\"至少信心度要超過\",{\"1\":{\"90\":1}}],[\"至今平均的\",{\"1\":{\"14\":1}}],[\"至今被嘗試的次數\",{\"1\":{\"14\":1}}],[\"信心度的標準\",{\"1\":{\"90\":1}}],[\"期待預測的\",{\"1\":{\"90\":1}}],[\"期許未來算力的大幅進步能夠支撐用來處理影片序列\",{\"1\":{\"55\":1}}],[\"等\",{\"1\":{\"166\":1}}],[\"等架構\",{\"1\":{\"87\":1}}],[\"等不同的做法\",{\"1\":{\"66\":1}}],[\"領域當中把一些\",{\"1\":{\"108\":1}}],[\"領域當中做出了劃時代的貢獻\",{\"1\":{\"86\":1}}],[\"領域\",{\"1\":{\"102\":1}}],[\"領域都已經是過時的產物\",{\"1\":{\"87\":1}}],[\"領域的開拓性論文\",{\"1\":{\"33\":1}}],[\"物件偵測的領域自適應\",{\"1\":{\"82\":1}}],[\"挑選\",{\"1\":{\"80\":1}}],[\"針對上述兩個問題\",{\"1\":{\"187\":1}}],[\"針對這幾個部分同樣去研究對應的影響\",{\"1\":{\"159\":1}}],[\"針對\",{\"1\":{\"80\":1,\"91\":1,\"126\":1,\"157\":1}}],[\"針對不同的\",{\"1\":{\"7\":1,\"132\":1}}],[\"判斷要不要\",{\"1\":{\"80\":1}}],[\"真的很糟\",{\"1\":{\"78\":1}}],[\"建構的\",{\"1\":{\"77\":1}}],[\"照片是在\",{\"1\":{\"76\":1,\"77\":1}}],[\"照片是在城市當中開車拍下的各種照片\",{\"1\":{\"75\":1}}],[\"照著這樣的想法\",{\"1\":{\"13\":1}}],[\"作用只在於訓練的前中期提供\",{\"1\":{\"180\":1}}],[\"作法上\",{\"1\":{\"174\":1}}],[\"作為輸入會太過於耗費記憶體\",{\"1\":{\"158\":1}}],[\"作為輸出\",{\"1\":{\"128\":1}}],[\"作為一種\",{\"1\":{\"126\":1}}],[\"作為\",{\"1\":{\"73\":1,\"91\":1,\"108\":1,\"158\":1,\"179\":1}}],[\"作者進一步加上了\",{\"1\":{\"197\":1}}],[\"作者進一步設計一個\",{\"1\":{\"196\":1}}],[\"作者進一步去研究\",{\"1\":{\"180\":1}}],[\"作者進一步去比較自己改良的\",{\"1\":{\"101\":1}}],[\"作者進一步去忽略畫面上方\",{\"1\":{\"100\":1}}],[\"作者進一步去分析究竟是哪一個部分使最後得到好的結果\",{\"1\":{\"91\":1}}],[\"作者分別去計算兩者的\",{\"1\":{\"196\":1}}],[\"作者分別使用\",{\"1\":{\"187\":1}}],[\"作者透過對所擁有的\",{\"1\":{\"196\":1}}],[\"作者透過以下幾種方法來避免overfitting\",{\"1\":{\"45\":1}}],[\"作者發現在每一個遊戲當中最後一個\",{\"1\":{\"180\":1}}],[\"作者除了分開兩個部分去產出\",{\"1\":{\"151\":1}}],[\"作者在\",{\"1\":{\"149\":1}}],[\"作者在實驗的過程當中發現到\",{\"1\":{\"92\":1}}],[\"作者在實驗當中發現如果採用\",{\"1\":{\"17\":1}}],[\"作者比較了同樣架構下不同\",{\"1\":{\"138\":1}}],[\"作者比較兩個模型在不同\",{\"1\":{\"22\":1}}],[\"作者設計了兩種狀況來看\",{\"1\":{\"137\":1}}],[\"作者使用一個簡單的\",{\"1\":{\"136\":1}}],[\"作者去挑了在該領域的\",{\"1\":{\"132\":1}}],[\"作者選了許多不同領域的資料集如下\",{\"1\":{\"132\":1}}],[\"作者從啟發當中發想提出了\",{\"1\":{\"124\":1}}],[\"作者從\",{\"1\":{\"124\":1}}],[\"作者推測是因為權重即便只有小的變化也會對\",{\"1\":{\"117\":1}}],[\"作者把這些\",{\"1\":{\"93\":1}}],[\"作者認為每經過一個\",{\"1\":{\"192\":1}}],[\"作者認為目前的做法存在兩個問題\",{\"1\":{\"187\":1}}],[\"作者認為\",{\"1\":{\"136\":1}}],[\"作者認為過去大模型之所以會\",{\"1\":{\"136\":1}}],[\"作者認為這是因為\",{\"1\":{\"100\":1}}],[\"作者認為這是因為若這些\",{\"1\":{\"92\":1}}],[\"作者認為這是好的\",{\"1\":{\"93\":1}}],[\"作者認為在這兩者都有一個共通點\",{\"1\":{\"23\":1}}],[\"作者懷疑會不會其實我們應該要試著採用更好的\",{\"1\":{\"87\":1}}],[\"作者觀點\",{\"0\":{\"55\":1}}],[\"作者們也使用了以下的normalization技巧\",{\"1\":{\"43\":1}}],[\"作者們使用了一些優化\",{\"1\":{\"38\":1}}],[\"作者們train了一個deep\",{\"1\":{\"33\":1}}],[\"作者將\",{\"1\":{\"23\":1,\"24\":1}}],[\"作者也注意到\",{\"1\":{\"100\":1}}],[\"作者也給出每個\",{\"1\":{\"97\":1}}],[\"作者也將alexnet的主要features分為四點\",{\"1\":{\"40\":1}}],[\"作者也觀察了在幾款遊戲訓練過程中當中\",{\"1\":{\"24\":1}}],[\"作者也發現到當使用\",{\"1\":{\"136\":1}}],[\"作者也發現到\",{\"1\":{\"22\":1}}],[\"作者也發現如果把\",{\"1\":{\"22\":1}}],[\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子\",{\"1\":{\"12\":1}}],[\"作者建構一個簡單的\",{\"1\":{\"22\":1}}],[\"部分的影響程度\",{\"1\":{\"71\":1}}],[\"λ=0\",{\"1\":{\"194\":1}}],[\"λfd​\",{\"1\":{\"93\":1}}],[\"λ\",{\"1\":{\"71\":1,\"172\":1}}],[\"得出來的效果都比起直接使用高解析度圖片還要差\",{\"1\":{\"157\":1}}],[\"得出\",{\"1\":{\"149\":1}}],[\"得出的\",{\"1\":{\"91\":1}}],[\"得到不同的經驗\",{\"1\":{\"168\":1}}],[\"得到比\",{\"1\":{\"160\":1}}],[\"得到的資料也就不同\",{\"1\":{\"172\":1}}],[\"得到的結果會更好\",{\"1\":{\"157\":2}}],[\"得到的最大\",{\"1\":{\"110\":1}}],[\"得到細節的資訊\",{\"1\":{\"151\":1}}],[\"得到大範圍的資訊\",{\"1\":{\"151\":1}}],[\"得到\",{\"1\":{\"71\":1,\"150\":1,\"151\":1}}],[\"得以有不同程度的影響\",{\"1\":{\"27\":1}}],[\"詳細的步驟具體來說\",{\"1\":{\"71\":1}}],[\"覆蓋\",{\"1\":{\"70\":1}}],[\"許多的\",{\"1\":{\"70\":1}}],[\"成功結合\",{\"1\":{\"119\":1}}],[\"成功在雙陸棋上面\",{\"1\":{\"111\":1}}],[\"成功在幾乎所有的\",{\"1\":{\"101\":1}}],[\"成功將\",{\"1\":{\"102\":1}}],[\"成功避免了預測結果變差的狀況\",{\"1\":{\"100\":1}}],[\"成新的\",{\"1\":{\"70\":1}}],[\"成為大學ml\",{\"1\":{\"57\":1}}],[\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清\",{\"1\":{\"68\":1}}],[\"先轉成\",{\"1\":{\"68\":1}}],[\"先在\",{\"1\":{\"67\":1}}],[\"拿出來做一些\",{\"1\":{\"114\":1}}],[\"拿出來\",{\"1\":{\"93\":1}}],[\"拿出兩張\",{\"1\":{\"68\":1}}],[\"拿去訓練\",{\"1\":{\"70\":1,\"71\":1}}],[\"拿來比較加上\",{\"1\":{\"24\":1}}],[\"看\",{\"1\":{\"67\":1}}],[\"缺乏\",{\"1\":{\"67\":1}}],[\"畢竟每個\",{\"1\":{\"172\":1}}],[\"畢竟都還在熟悉新的環境\",{\"1\":{\"136\":1}}],[\"畢竟在\",{\"1\":{\"80\":1}}],[\"畢竟\",{\"1\":{\"67\":1}}],[\"了\",{\"1\":{\"67\":1,\"110\":1,\"176\":1,\"177\":1,\"180\":1,\"187\":1}}],[\"半監督式學習\",{\"1\":{\"67\":1,\"82\":2}}],[\"天空之類的就通常會像是在半空中\",{\"1\":{\"66\":1}}],[\"天生就有一些自己的偏好\",{\"1\":{\"13\":1}}],[\"號誌\",{\"1\":{\"66\":1}}],[\"行動之前需要先看運氣抽接下來使用的武器\",{\"1\":{\"168\":1}}],[\"行人都還是會跟地板黏在一起\",{\"1\":{\"66\":1}}],[\"行為模式改變\",{\"1\":{\"15\":1}}],[\"汽車\",{\"1\":{\"66\":1,\"155\":1}}],[\"同樣可以看到\",{\"1\":{\"202\":1}}],[\"同樣可以觀察到\",{\"1\":{\"133\":1}}],[\"同樣可以觀察到在加上\",{\"1\":{\"133\":1}}],[\"同樣地也可以對\",{\"1\":{\"176\":1}}],[\"同樣的\",{\"1\":{\"157\":1}}],[\"同樣也包含了\",{\"1\":{\"126\":1}}],[\"同樣以自駕車的例子來說\",{\"1\":{\"66\":1}}],[\"同時將\",{\"1\":{\"160\":1}}],[\"同時\",{\"1\":{\"156\":1}}],[\"同時避免\",{\"1\":{\"91\":1}}],[\"同時也能在\",{\"1\":{\"25\":1}}],[\"同時也可以看到\",{\"1\":{\"25\":1}}],[\"通常在\",{\"1\":{\"93\":1}}],[\"通常會對模型的結果有不小的影響\",{\"1\":{\"138\":1}}],[\"通常會有個通病是在\",{\"1\":{\"91\":1}}],[\"通常會寫成以下形式\",{\"1\":{\"35\":1}}],[\"通常並不會差太多\",{\"1\":{\"66\":1}}],[\"依照\",{\"1\":{\"66\":1,\"148\":1}}],[\"依照過去\",{\"1\":{\"6\":1}}],[\"做修改\",{\"1\":{\"176\":1}}],[\"做調整\",{\"1\":{\"174\":1}}],[\"做了詳細的比較如下表\",{\"1\":{\"157\":1}}],[\"做了兩個小修正\",{\"1\":{\"16\":1}}],[\"做出來的\",{\"1\":{\"152\":1}}],[\"做出一個\",{\"1\":{\"68\":1}}],[\"做的事情簡單來說就是\",{\"1\":{\"149\":1}}],[\"做在\",{\"1\":{\"136\":1}}],[\"做為新的\",{\"1\":{\"91\":1}}],[\"做得很棒不能直接表達在整體會表達很棒\",{\"1\":{\"80\":1}}],[\"做\",{\"1\":{\"67\":1}}],[\"做對抗式學習\",{\"1\":{\"66\":1}}],[\"結合\",{\"1\":{\"119\":1,\"196\":1}}],[\"結合起來\",{\"1\":{\"114\":1}}],[\"結果相當糟糕\",{\"1\":{\"64\":1}}],[\"結束\",{\"1\":{\"19\":1}}],[\"擅自用自己的思維解讀\",{\"1\":{\"64\":1}}],[\"轉變到\",{\"1\":{\"64\":1}}],[\"影像分割\",{\"1\":{\"64\":1}}],[\"近年來流行\",{\"1\":{\"187\":1}}],[\"近年來透過\",{\"1\":{\"64\":1}}],[\"近似\",{\"1\":{\"12\":2}}],[\"讓他越來越貼合真實的狀況\",{\"1\":{\"192\":1}}],[\"讓選擇更多樣\",{\"1\":{\"168\":1}}],[\"讓\",{\"1\":{\"116\":1,\"169\":1,\"170\":1}}],[\"讓我們得以用較低的成本在虛擬環境中訓練模型\",{\"1\":{\"63\":1}}],[\"讓每個分身在各自的環境當中訓練\",{\"1\":{\"172\":1}}],[\"讓每個\",{\"1\":{\"13\":1}}],[\"降低訓練資料之間的關聯性\",{\"1\":{\"172\":1}}],[\"降低了資料之間的相關性\",{\"1\":{\"169\":1}}],[\"降低到了\",{\"1\":{\"135\":1}}],[\"降低\",{\"1\":{\"63\":1,\"71\":1}}],[\"想解決的就是盡可能地將\",{\"1\":{\"63\":1}}],[\"到高解析度\",{\"1\":{\"157\":1}}],[\"到的時間有所相關\",{\"1\":{\"99\":1}}],[\"到的機率\",{\"1\":{\"92\":1}}],[\"到\",{\"1\":{\"63\":1,\"92\":1,\"100\":1}}],[\"環境與虛擬世界有差距\",{\"1\":{\"63\":1}}],[\"環境中取得\",{\"1\":{\"19\":1}}],[\"虛擬世界\",{\"1\":{\"63\":1}}],[\"像是\",{\"1\":{\"68\":1,\"70\":1,\"78\":1,\"145\":1}}],[\"像是直接有一台車會去蒐集真實街景資料\",{\"1\":{\"63\":1}}],[\"像一樣很常用的relu函數則沒有被擠壓的上界\",{\"1\":{\"36\":1}}],[\"投射到同一個平面上\",{\"1\":{\"63\":1}}],[\"共同發表\",{\"1\":{\"62\":1}}],[\"查爾摩斯理工大學\",{\"1\":{\"62\":1}}],[\"三\",{\"1\":{\"58\":1}}],[\"基礎數學\",{\"1\":{\"58\":1}}],[\"基本想法\",{\"0\":{\"174\":1}}],[\"基本概念\",{\"1\":{\"28\":1}}],[\"基本上就是將\",{\"1\":{\"175\":1}}],[\"基本上就是從\",{\"1\":{\"68\":1}}],[\"基本上都相當接近\",{\"1\":{\"22\":1}}],[\"基本上使用了\",{\"1\":{\"9\":1}}],[\"基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界\",{\"1\":{\"14\":1}}],[\"直覺上會覺得\",{\"1\":{\"152\":1}}],[\"直接從\",{\"1\":{\"119\":1}}],[\"直接使用\",{\"1\":{\"91\":1}}],[\"直接把訓練在虛擬環境的模型應用在真實環境\",{\"1\":{\"64\":1}}],[\"直觀理解並應用主成分分析\",{\"1\":{\"58\":1}}],[\"直到\",{\"1\":{\"19\":1}}],[\"世上最生動的\",{\"1\":{\"58\":1}}],[\"還是相差許多\",{\"1\":{\"187\":1}}],[\"還是存在差異的\",{\"1\":{\"67\":1}}],[\"還是\",{\"1\":{\"66\":1,\"151\":1}}],[\"還真的有一種站在巨人肩膀上的感覺\",{\"1\":{\"57\":1}}],[\"還要強\",{\"1\":{\"24\":1}}],[\"除了越準確越好\",{\"1\":{\"90\":1}}],[\"除了\",{\"1\":{\"78\":1,\"79\":1,\"116\":1}}],[\"除了感受到這個領域的快速發展外\",{\"1\":{\"57\":1}}],[\"除了展現驚人的成果以外\",{\"1\":{\"25\":1}}],[\"超酷xd\",{\"1\":{\"57\":1}}],[\"甚至那些\",{\"1\":{\"97\":1}}],[\"甚至對\",{\"1\":{\"79\":1}}],[\"甚至有些東西是本篇論文的作者在不久前提出的\",{\"1\":{\"57\":1}}],[\"甚至後來brock等人提出說在殘差神經網路\",{\"1\":{\"43\":1}}],[\"考慮在時間\",{\"1\":{\"172\":1}}],[\"考慮到\",{\"1\":{\"79\":1}}],[\"考慮有限的\",{\"1\":{\"8\":1}}],[\"考究到這篇論文的時間是在2012\",{\"1\":{\"57\":1}}],[\"心得\",{\"0\":{\"57\":1}}],[\"促使了後來越來越深的model與算力發展\",{\"1\":{\"56\":1}}],[\"促使neurons去學習更加robust的features\",{\"1\":{\"49\":1}}],[\"深度很重要\",{\"1\":{\"56\":1}}],[\"深度學習\",{\"1\":{\"33\":1,\"58\":1}}],[\"避免了訓練目標經常地變動造成訓練效果差\",{\"1\":{\"169\":1}}],[\"避免了\",{\"1\":{\"126\":1,\"169\":1}}],[\"避免了vanishing\",{\"1\":{\"56\":1}}],[\"避免模型忘記\",{\"1\":{\"198\":1}}],[\"避免模型\",{\"1\":{\"93\":1}}],[\"避免\",{\"1\":{\"87\":1}}],[\"避免太大或是太小\",{\"1\":{\"6\":1}}],[\"總共有\",{\"1\":{\"133\":1}}],[\"總和最大化\",{\"1\":{\"110\":1}}],[\"總結來說\",{\"1\":{\"56\":1}}],[\"總之\",{\"1\":{\"19\":1}}],[\"包括後來越來越深的模型\",{\"1\":{\"56\":1}}],[\"包含5層的convolutional\",{\"1\":{\"40\":1}}],[\"包含大約1000個類別的1000個圖像\",{\"1\":{\"39\":1}}],[\"包含了旋轉\",{\"1\":{\"196\":1}}],[\"包含了所有\",{\"1\":{\"174\":1}}],[\"包含了許多的高解析度彩色圖片\",{\"1\":{\"133\":1}}],[\"包含了許多\",{\"1\":{\"133\":3}}],[\"包含了\",{\"1\":{\"90\":1,\"114\":1,\"154\":1}}],[\"包含了超過1500萬張有標注的high\",{\"1\":{\"39\":1}}],[\"包含了幾個重要的部分\",{\"1\":{\"5\":1}}],[\"非常多的圖片and時間資訊\",{\"1\":{\"55\":1}}],[\"學會\",{\"1\":{\"198\":1}}],[\"學到的大多都是跟顏色無關的特性\",{\"1\":{\"53\":1}}],[\"學習\",{\"1\":{\"19\":1}}],[\"訓練前首先透過\",{\"1\":{\"200\":1}}],[\"訓練過程中根據與\",{\"1\":{\"192\":1}}],[\"訓練過程中讓每個\",{\"1\":{\"128\":1}}],[\"訓練的環境都不同\",{\"1\":{\"172\":1}}],[\"訓練的過程包含了一點專家系統的概念\",{\"1\":{\"118\":2}}],[\"訓練成效也就翻倍\",{\"1\":{\"172\":1}}],[\"訓練避免偏差\",{\"1\":{\"169\":1}}],[\"訓練時就會使用\",{\"1\":{\"151\":1}}],[\"訓練\",{\"1\":{\"91\":1,\"102\":1,\"113\":1}}],[\"訓練集為imagenet中的120萬張image\",{\"1\":{\"50\":1}}],[\"訓練了約5~6天\",{\"1\":{\"50\":1}}],[\"初始值都為0\",{\"1\":{\"50\":1}}],[\"初始權重的部份\",{\"1\":{\"50\":1}}],[\"造成每次iterate\",{\"1\":{\"49\":1}}],[\"加總後平均\",{\"1\":{\"194\":1}}],[\"加\",{\"1\":{\"168\":1}}],[\"加起來共10個\",{\"1\":{\"47\":1}}],[\"加上這一項能夠促使模型更好\",{\"1\":{\"172\":1}}],[\"加上總和為\",{\"1\":{\"171\":1}}],[\"加上後會再進一步提升效果\",{\"1\":{\"130\":1}}],[\"加上了\",{\"1\":{\"22\":1,\"129\":1,\"174\":1}}],[\"加上了一個\",{\"1\":{\"15\":1}}],[\"加上\",{\"1\":{\"22\":1,\"48\":1,\"91\":1,\"97\":4,\"126\":3,\"132\":1,\"133\":1,\"135\":1,\"159\":2,\"168\":1}}],[\"加上不同的偏好\",{\"1\":{\"13\":1}}],[\"四個角落和中間\",{\"1\":{\"47\":1}}],[\"抽出其中5個patches\",{\"1\":{\"47\":1}}],[\"容易導致嚴重的overfitting\",{\"1\":{\"45\":1}}],[\"且包含了\",{\"1\":{\"151\":1}}],[\"且提出技巧避免\",{\"1\":{\"149\":1}}],[\"且當validation\",{\"1\":{\"50\":1}}],[\"且作者們也發現\",{\"1\":{\"44\":1}}],[\"且所有元素總和為1\",{\"1\":{\"35\":1}}],[\"傳統上的general\",{\"1\":{\"44\":1}}],[\"取出第一層\",{\"1\":{\"136\":1}}],[\"取出圖片\",{\"1\":{\"71\":1}}],[\"取出圖片與\",{\"1\":{\"71\":1}}],[\"取代\",{\"1\":{\"43\":1,\"58\":1}}],[\"取得的\",{\"1\":{\"110\":1}}],[\"取得\",{\"1\":{\"19\":1,\"71\":1,\"198\":1}}],[\"取得不同的\",{\"1\":{\"9\":1}}],[\"經過強增強的\",{\"1\":{\"196\":1}}],[\"經過實驗後發現無論是採用\",{\"1\":{\"98\":1}}],[\"經過\",{\"1\":{\"71\":1,\"97\":1,\"148\":1}}],[\"經過相同的\",{\"1\":{\"66\":1}}],[\"經過了約90個cycles\",{\"1\":{\"50\":1}}],[\"經過調整後為n=5\",{\"1\":{\"43\":1}}],[\"經過relu\",{\"1\":{\"43\":1}}],[\"α=0\",{\"1\":{\"195\":1}}],[\"α=10−4\",{\"1\":{\"43\":1}}],[\"αi​則為隨機變量\",{\"1\":{\"48\":1}}],[\"α\",{\"1\":{\"43\":1,\"100\":1,\"171\":3}}],[\"αt​\",{\"1\":{\"6\":1}}],[\"雖說我們現在會覺得alexnet其實也沒有很深\",{\"1\":{\"56\":1}}],[\"雖說ilsvrc\",{\"1\":{\"52\":1}}],[\"雖說ilsvrc的圖像有1000個類別\",{\"1\":{\"45\":1}}],[\"雖說dropout大致會將iteration的數量加倍\",{\"1\":{\"49\":1}}],[\"雖說以現在gpu的發展\",{\"1\":{\"42\":1}}],[\"雖然這種做法開始能夠讓\",{\"1\":{\"187\":1}}],[\"雖然就結果而言他們的\",{\"1\":{\"136\":1}}],[\"雖然已經有\",{\"1\":{\"67\":1}}],[\"雖然在\",{\"1\":{\"25\":1}}],[\"雖然兩個模型都會把\",{\"1\":{\"12\":1}}],[\"精準調控gpu之間的通訊量\",{\"1\":{\"42\":1}}],[\"單純\",{\"1\":{\"159\":1}}],[\"單純在\",{\"1\":{\"133\":1}}],[\"單純加上\",{\"1\":{\"133\":1}}],[\"單純用\",{\"1\":{\"78\":1,\"79\":1}}],[\"單純的\",{\"1\":{\"15\":1,\"70\":1}}],[\"單一個gtx\",{\"1\":{\"42\":1}}],[\">r\",{\"1\":{\"93\":1}}],[\">τ\",{\"1\":{\"90\":1}}],[\">\",{\"0\":{\"78\":1,\"79\":1},\"1\":{\"41\":1,\"74\":2}}],[\"及f\",{\"1\":{\"41\":1}}],[\"所影響如下\",{\"1\":{\"129\":1}}],[\"所有的算法\",{\"1\":{\"118\":1}}],[\"所示\",{\"1\":{\"100\":1}}],[\"所使用的\",{\"1\":{\"93\":1}}],[\"所謂的半監督式學習也就是說\",{\"1\":{\"67\":1}}],[\"所謂的\",{\"1\":{\"63\":1,\"67\":1}}],[\"所以也會把\",{\"1\":{\"197\":1}}],[\"所以對於一個\",{\"1\":{\"174\":1}}],[\"所以要讓大小都相等\",{\"1\":{\"151\":1}}],[\"所以沒有\",{\"1\":{\"151\":1}}],[\"所以漸漸地每個人都會有一定的能力水平\",{\"1\":{\"136\":1}}],[\"所以作為一個團隊來說\",{\"1\":{\"136\":1}}],[\"所以作者們將network橫跨到兩個gpu上\",{\"1\":{\"42\":1}}],[\"所以就不再贅述\",{\"1\":{\"135\":1}}],[\"所以設定\",{\"1\":{\"116\":1}}],[\"所以\",{\"1\":{\"108\":1,\"171\":1}}],[\"所以圖片底下的部分實際上並不是跟街景相關\",{\"1\":{\"100\":1}}],[\"所以採用這個方法\",{\"1\":{\"90\":1}}],[\"所以他們認為這樣不太公平\",{\"1\":{\"80\":1}}],[\"所以在數據上\",{\"1\":{\"79\":1}}],[\"所以表上的error\",{\"1\":{\"52\":1}}],[\"所以研究者們透過簡單的rescale與裁切的方式\",{\"1\":{\"39\":1}}],[\"所在的位置\",{\"1\":{\"22\":1}}],[\"代表的是在第i的batch\",{\"1\":{\"50\":1}}],[\"代表每個neuron每次有21​的機率被dropout\",{\"1\":{\"49\":1}}],[\"代表每個neuron有多大的機率會被\",{\"1\":{\"37\":1}}],[\"代表無論input是多少\",{\"1\":{\"37\":1}}],[\"他的輸出會被\",{\"1\":{\"129\":1}}],[\"他要去近似的是\",{\"1\":{\"110\":1}}],[\"他可以很好地把不同的\",{\"1\":{\"91\":1}}],[\"他也不會變成貓\",{\"1\":{\"48\":1}}],[\"他就是一個saturating的activation\",{\"1\":{\"36\":1}}],[\"他們比較了\",{\"1\":{\"156\":1}}],[\"他們提出的\",{\"1\":{\"145\":1}}],[\"他們會先透過\",{\"1\":{\"113\":1}}],[\"他們透過實驗發現這很不錯\",{\"1\":{\"94\":1}}],[\"他們後來發現\",{\"1\":{\"91\":1}}],[\"他們也試著用相同的手段訓練模型\",{\"1\":{\"80\":1}}],[\"他們選擇用\",{\"1\":{\"80\":1}}],[\"他們選擇其中\",{\"1\":{\"78\":1}}],[\"他們認為在其他的\",{\"1\":{\"80\":1}}],[\"他們並不是採用\",{\"1\":{\"21\":1}}],[\"他們將\",{\"1\":{\"6\":1}}],[\"常用的sigmoid函數會將值擠壓到區間\",{\"1\":{\"36\":1}}],[\"舉例而言\",{\"1\":{\"168\":1}}],[\"舉例來說\",{\"1\":{\"36\":1,\"67\":1}}],[\"舉一個在\",{\"1\":{\"63\":1}}],[\"舉一個例子來說\",{\"1\":{\"13\":1}}],[\"正式的定義可以寫成這樣\",{\"1\":{\"36\":1}}],[\"正確\",{\"1\":{\"3\":1}}],[\"簡單來說\",{\"1\":{\"48\":1}}],[\"簡單來說就是將一個k維的向量壓縮成一個k維的機率向量\",{\"1\":{\"35\":1}}],[\"簡單設計成一個\",{\"1\":{\"14\":1}}],[\"運用在computer\",{\"1\":{\"33\":1}}],[\"卷積神經網路\",{\"1\":{\"33\":1}}],[\"<>\",{\"1\":{\"28\":1}}],[\"值得一看的文章們\",{\"0\":{\"28\":1,\"82\":1,\"103\":1,\"140\":1,\"161\":1,\"182\":1,\"204\":1}}],[\"值得一提的是\",{\"1\":{\"22\":1}}],[\"使最終的結果進一步提昇\",{\"1\":{\"197\":1}}],[\"使得\",{\"1\":{\"196\":1}}],[\"使得子代能夠適應整個環境\",{\"1\":{\"124\":1}}],[\"使得所有\",{\"1\":{\"116\":1}}],[\"使得同類型的資料會相近\",{\"1\":{\"63\":1}}],[\"使這條weight無法被更新\",{\"1\":{\"37\":1}}],[\"使模型具有更好的普遍性\",{\"1\":{\"27\":1}}],[\"使用了\",{\"1\":{\"133\":1}}],[\"使用的有無造成的結果\",{\"1\":{\"138\":1}}],[\"使用的有無有相當大的不同\",{\"1\":{\"133\":1}}],[\"使用的一些\",{\"1\":{\"129\":1}}],[\"使用線性的\",{\"1\":{\"112\":1}}],[\"使用非線性的\",{\"1\":{\"112\":1}}],[\"使用標準差是0\",{\"1\":{\"50\":1}}],[\"使用overlapping\",{\"1\":{\"44\":1}}],[\"使用relu會比使用tanh的neurons快上6倍\",{\"1\":{\"41\":1}}],[\"使用這種nonlinearity的neurons也叫做relus\",{\"1\":{\"41\":1}}],[\"使用softmax\",{\"1\":{\"35\":1}}],[\"使用\",{\"1\":{\"9\":1,\"79\":1,\"91\":2,\"130\":1,\"150\":1,\"159\":1,\"169\":1,\"171\":1}}],[\"使用不同的\",{\"1\":{\"9\":1,\"157\":1}}],[\"自從這篇論文以來\",{\"1\":{\"57\":1}}],[\"自適應不同環境\",{\"1\":{\"27\":1}}],[\"自己調整\",{\"1\":{\"24\":1}}],[\"解決過去\",{\"1\":{\"119\":1}}],[\"解決\",{\"1\":{\"91\":1}}],[\"解決訓練不穩定\",{\"1\":{\"27\":1}}],[\"解決了前面提及的第三個問題\",{\"1\":{\"110\":1}}],[\"解決了\",{\"1\":{\"12\":1,\"16\":1,\"17\":1,\"25\":1}}],[\"解決了一些\",{\"1\":{\"10\":1}}],[\"顯示了\",{\"1\":{\"25\":1}}],[\"卻並不一定了\",{\"1\":{\"180\":1}}],[\"卻會受到\",{\"1\":{\"168\":1}}],[\"卻又不會有過高的計算量\",{\"1\":{\"128\":1}}],[\"卻反而往往得到很糟糕的結果\",{\"1\":{\"93\":1}}],[\"卻是最差的\",{\"1\":{\"25\":1}}],[\"卻在一些簡單的問題做得很差\",{\"1\":{\"10\":1}}],[\"提供一個簡單又有效的\",{\"1\":{\"181\":1}}],[\"提供了更大的普遍性\",{\"1\":{\"24\":1}}],[\"提昇training的效度\",{\"1\":{\"43\":1}}],[\"提出的方法略為不同的地方在於他並不是直接對\",{\"1\":{\"174\":1}}],[\"提出了解決這個高估問題的方法\",{\"1\":{\"170\":1}}],[\"提出了三個方法避免\",{\"1\":{\"102\":1}}],[\"提出新的\",{\"1\":{\"160\":1}}],[\"提出一個新的\",{\"1\":{\"203\":1}}],[\"提出一個可以即時修正\",{\"1\":{\"203\":1}}],[\"提出一個能夠搭配許多\",{\"1\":{\"160\":1}}],[\"提出一個簡單避免\",{\"1\":{\"139\":1}}],[\"提出如\",{\"1\":{\"67\":1}}],[\"提出\",{\"1\":{\"27\":1}}],[\"提出透過\",{\"1\":{\"27\":1}}],[\"提升模型的預測能力\",{\"1\":{\"149\":1}}],[\"提升了約\",{\"1\":{\"97\":4}}],[\"提升\",{\"1\":{\"22\":1}}],[\"75​\",{\"1\":{\"158\":2}}],[\"75\",{\"1\":{\"43\":1}}],[\"7\",{\"1\":{\"42\":1,\"80\":2,\"97\":1,\"116\":1,\"137\":1}}],[\"79\",{\"1\":{\"24\":1}}],[\"76\",{\"1\":{\"24\":1,\"80\":1}}],[\"70\",{\"1\":{\"24\":2}}],[\"62\",{\"1\":{\"135\":1}}],[\"680\",{\"1\":{\"134\":1}}],[\"68\",{\"1\":{\"80\":1}}],[\"6\",{\"1\":{\"24\":1,\"96\":1,\"97\":2,\"137\":1}}],[\"69571\",{\"1\":{\"24\":1}}],[\"6463\",{\"1\":{\"24\":1}}],[\"601\",{\"1\":{\"24\":1}}],[\"60\",{\"1\":{\"24\":2,\"133\":1,\"158\":1}}],[\"4​​otherwise​\",{\"1\":{\"151\":2}}],[\"4​=bd\",{\"1\":{\"150\":1}}],[\"4​=bc\",{\"1\":{\"150\":1}}],[\"4​\",{\"1\":{\"150\":2}}],[\"4≤p≤0\",{\"1\":{\"137\":2}}],[\"49\",{\"1\":{\"80\":1}}],[\"4\",{\"1\":{\"42\":1,\"43\":1,\"44\":1,\"50\":1,\"67\":1,\"97\":3,\"114\":1,\"155\":1,\"156\":1,\"159\":1}}],[\"4336\",{\"1\":{\"24\":1}}],[\"4596\",{\"1\":{\"24\":1}}],[\"4753\",{\"1\":{\"24\":1}}],[\"400\",{\"1\":{\"19\":1}}],[\"40\",{\"1\":{\"17\":1}}],[\"8\",{\"1\":{\"97\":2,\"118\":1,\"134\":1,\"137\":1,\"158\":1,\"182\":1}}],[\"83\",{\"1\":{\"24\":1,\"80\":1}}],[\"89\",{\"1\":{\"24\":2}}],[\"84\",{\"1\":{\"24\":1,\"80\":1}}],[\"800000\",{\"1\":{\"135\":1}}],[\"80\",{\"1\":{\"17\":2}}],[\"3​​≤j<obd\",{\"1\":{\"151\":1}}],[\"3​​≤j<s⋅obd\",{\"1\":{\"151\":1}}],[\"3​​\",{\"1\":{\"151\":1}}],[\"3​+wd​​\",{\"1\":{\"150\":1}}],[\"3​+swc​​\",{\"1\":{\"150\":1}}],[\"3​∼u\",{\"1\":{\"150\":2}}],[\"3​\",{\"1\":{\"150\":2}}],[\"35\",{\"1\":{\"80\":1,\"133\":1}}],[\"3gb\",{\"1\":{\"50\":1}}],[\"3\",{\"1\":{\"44\":1,\"97\":1,\"133\":2,\"156\":1,\"158\":2,\"159\":2}}],[\"31\",{\"1\":{\"24\":1,\"135\":1}}],[\"30\",{\"1\":{\"24\":2,\"156\":1}}],[\"32×32\",{\"1\":{\"133\":2}}],[\"32\",{\"1\":{\"24\":1}}],[\"38\",{\"1\":{\"24\":1}}],[\"349971\",{\"1\":{\"24\":1}}],[\"±\",{\"1\":{\"24\":10}}],[\"95\",{\"1\":{\"133\":1}}],[\"98\",{\"1\":{\"80\":1}}],[\"9400\",{\"1\":{\"77\":1}}],[\"9\",{\"1\":{\"50\":1,\"155\":1,\"159\":1}}],[\"90\",{\"1\":{\"24\":1}}],[\"96\",{\"1\":{\"24\":1}}],[\"999\",{\"1\":{\"100\":1}}],[\"9999\",{\"1\":{\"21\":1,\"24\":1,\"194\":1}}],[\"99\",{\"1\":{\"21\":1,\"24\":1}}],[\"款比較困難的遊戲當中有些甚至是能夠比\",{\"1\":{\"24\":1}}],[\"異常地大\",{\"1\":{\"24\":1}}],[\"尤其\",{\"1\":{\"196\":1}}],[\"尤其預測的過程當中並不包含\",{\"1\":{\"152\":1}}],[\"尤其是從\",{\"1\":{\"64\":1}}],[\"尤其在\",{\"1\":{\"23\":1,\"67\":1}}],[\"尤其當\",{\"1\":{\"10\":1}}],[\"可能已經\",{\"1\":{\"192\":1}}],[\"可對應到\",{\"1\":{\"76\":1}}],[\"可當作是這個neuron並不存在\",{\"1\":{\"37\":1}}],[\"可見\",{\"1\":{\"22\":1}}],[\"可以比較接近\",{\"1\":{\"196\":1}}],[\"可以看成是更新較慢的\",{\"1\":{\"193\":1}}],[\"可以看到在大多數的遊戲加上了\",{\"1\":{\"179\":1}}],[\"可以看到在綠色線的部分\",{\"1\":{\"100\":1}}],[\"可以看到左邊的部分\",{\"1\":{\"156\":1}}],[\"可以看到\",{\"1\":{\"133\":1}}],[\"可以看到所有\",{\"1\":{\"97\":1}}],[\"可以看到比較明顯的結果\",{\"1\":{\"23\":1}}],[\"可以帶來很不錯的\",{\"1\":{\"179\":1}}],[\"可以從分數上明顯看出來加上了\",{\"1\":{\"179\":1}}],[\"可以用單一的\",{\"1\":{\"174\":1}}],[\"可以平行化加速訓練\",{\"1\":{\"172\":1}}],[\"可以進一步提升\",{\"1\":{\"159\":2}}],[\"可以有\",{\"1\":{\"158\":1}}],[\"可以有更好的\",{\"1\":{\"91\":1}}],[\"可以讓模型學習到更好的高解析度特徵\",{\"1\":{\"151\":1}}],[\"可以告訴我們要考慮多少的\",{\"1\":{\"151\":1}}],[\"可以得到\",{\"1\":{\"156\":2}}],[\"可以得到每個\",{\"1\":{\"148\":1}}],[\"可以得到更好的結果\",{\"1\":{\"87\":1,\"133\":2}}],[\"可以參考過去的文章\",{\"1\":{\"148\":1}}],[\"可以預測出\",{\"1\":{\"148\":1}}],[\"可以設定\",{\"1\":{\"137\":1}}],[\"可以描述其輸入與輸出如下\",{\"1\":{\"129\":1}}],[\"可以單純透過觀察模型在\",{\"1\":{\"117\":1}}],[\"可以直接把資料之間的關聯建構起來\",{\"1\":{\"108\":1}}],[\"可以觀察到\",{\"1\":{\"97\":1,\"100\":1,\"156\":1}}],[\"可以分成\",{\"1\":{\"66\":1}}],[\"可以發現搭配了\",{\"1\":{\"99\":1}}],[\"可以發現\",{\"1\":{\"78\":1,\"79\":1}}],[\"可以發現單純用\",{\"1\":{\"64\":1}}],[\"可以發現到在資料量小的狀況下使用\",{\"1\":{\"138\":1}}],[\"可以發現到在左邊的圖中選擇\",{\"1\":{\"137\":1}}],[\"可以發現到對應的\",{\"1\":{\"133\":1}}],[\"可以發現到通常越大的模型能夠提供更好的效益\",{\"1\":{\"91\":1}}],[\"可以發現到當\",{\"1\":{\"91\":1}}],[\"可以發現到\",{\"1\":{\"25\":1,\"91\":2,\"155\":1,\"157\":1}}],[\"可以發現到不同的遊戲會有不同的偏好\",{\"1\":{\"24\":1}}],[\"可以發現到加上\",{\"1\":{\"24\":1}}],[\"可以注意到在isvrc\",{\"1\":{\"52\":1}}],[\"可以將saturate理解成有沒有將值擠壓到一個特定的區間\",{\"1\":{\"36\":1}}],[\"可以認為\",{\"1\":{\"24\":1}}],[\"可以移除\",{\"1\":{\"16\":1}}],[\"可以選擇\",{\"1\":{\"14\":1}}],[\"可以拿到多少\",{\"1\":{\"8\":1}}],[\"可以幫助我們去評估如果我們\",{\"1\":{\"8\":1}}],[\"移除\",{\"1\":{\"22\":1}}],[\"確實已經透過前面的各個實驗證實了他的效度是相當不錯\",{\"1\":{\"136\":1}}],[\"確實能夠得到好的結果\",{\"1\":{\"101\":1}}],[\"確實能夠帶來相當好的效益\",{\"1\":{\"22\":1}}],[\"確實它會傾向讓\",{\"1\":{\"14\":1}}],[\"增加通用性\",{\"1\":{\"172\":1}}],[\"增加模型的更新與\",{\"1\":{\"171\":1}}],[\"增加\",{\"1\":{\"22\":1,\"166\":1,\"174\":1}}],[\"導致對結果的特徵很雜亂\",{\"1\":{\"187\":1}}],[\"導致我們對於不同的設計誤以為是不同類型的物件\",{\"1\":{\"145\":1}}],[\"導致複雜的模型學習過於有偏差\",{\"1\":{\"124\":1}}],[\"導致過度複雜的模型直接學習到\",{\"1\":{\"124\":1}}],[\"導致接下來會經過的\",{\"1\":{\"117\":1}}],[\"導致卡車時常被預測成汽車\",{\"1\":{\"67\":1}}],[\"導致預測失準\",{\"1\":{\"64\":1}}],[\"導致\",{\"1\":{\"22\":1,\"100\":1,\"145\":1}}],[\"與過去的資源消耗相較減輕甚多\",{\"1\":{\"102\":1}}],[\"與過去的\",{\"1\":{\"101\":2,\"155\":1,\"192\":1}}],[\"與過去的方法相比\",{\"1\":{\"86\":1}}],[\"與傳統的\",{\"1\":{\"91\":1,\"195\":1}}],[\"與test\",{\"1\":{\"52\":1}}],[\"與17\",{\"1\":{\"52\":1}}],[\"與1\",{\"1\":{\"43\":1}}],[\"與convolutional\",{\"1\":{\"33\":1}}],[\"與\",{\"1\":{\"22\":1,\"25\":1,\"43\":1,\"62\":1,\"90\":1,\"96\":1,\"108\":1,\"113\":1,\"126\":2,\"145\":1,\"149\":3,\"151\":5,\"155\":2,\"156\":3,\"157\":2,\"159\":3,\"160\":1,\"170\":1,\"171\":1,\"187\":1,\"191\":1,\"196\":2}}],[\"由此可見\",{\"1\":{\"22\":1}}],[\"由於過去的研究當中發現\",{\"1\":{\"148\":1}}],[\"由於標記\",{\"1\":{\"87\":1}}],[\"由於\",{\"1\":{\"22\":2,\"91\":1,\"130\":1,\"151\":1,\"176\":1,\"177\":1,\"187\":1}}],[\"綠色圓點表示\",{\"1\":{\"22\":1}}],[\"注意並不是\",{\"1\":{\"22\":1}}],[\"軸表示\",{\"1\":{\"22\":2}}],[\"各有其好處\",{\"1\":{\"151\":1}}],[\"各有一個\",{\"1\":{\"13\":1}}],[\"各自的\",{\"1\":{\"64\":1}}],[\"各自最傾向\",{\"1\":{\"22\":1}}],[\"接著透過\",{\"1\":{\"150\":1}}],[\"接著作者比較\",{\"1\":{\"22\":1}}],[\"接下來直接看\",{\"1\":{\"200\":1}}],[\"接下來評估加上\",{\"1\":{\"179\":1}}],[\"接下來這一整個\",{\"1\":{\"174\":1}}],[\"接下來\",{\"1\":{\"151\":1}}],[\"接下來透過\",{\"1\":{\"150\":1}}],[\"接下來依照你的需求不同\",{\"1\":{\"13\":1}}],[\"接下來用\",{\"1\":{\"7\":1}}],[\"走到\",{\"1\":{\"22\":1}}],[\"最外層的期望值是對\",{\"1\":{\"176\":1}}],[\"最一開始提到的問題就是使用\",{\"1\":{\"158\":1}}],[\"最一開始的假設是認為小的解析度會不好預測小物體\",{\"1\":{\"156\":1}}],[\"最相近的一個研究\",{\"1\":{\"113\":1}}],[\"最小化\",{\"1\":{\"110\":1}}],[\"最大化\",{\"1\":{\"110\":1}}],[\"最明顯\",{\"1\":{\"99\":1}}],[\"最主要的貢獻\",{\"1\":{\"91\":1}}],[\"最好的\",{\"1\":{\"78\":1}}],[\"最\",{\"1\":{\"70\":1,\"90\":1}}],[\"最初被用於把\",{\"1\":{\"68\":1}}],[\"最初是為了解決\",{\"1\":{\"67\":1}}],[\"最終也可以看到\",{\"1\":{\"101\":1}}],[\"最終得到更好的結果\",{\"1\":{\"100\":1}}],[\"最終則是\",{\"1\":{\"87\":1}}],[\"最終在所有的\",{\"1\":{\"25\":1}}],[\"最終\",{\"1\":{\"22\":1}}],[\"最傾向\",{\"1\":{\"22\":2}}],[\"最多\",{\"1\":{\"22\":1}}],[\"最後就剩下更新的\",{\"1\":{\"195\":1}}],[\"最後整體的\",{\"1\":{\"172\":1}}],[\"最後合併成\",{\"1\":{\"171\":1}}],[\"最後的\",{\"1\":{\"155\":1}}],[\"最後要把結果\",{\"1\":{\"151\":1}}],[\"最後輸出會乘上\",{\"1\":{\"130\":1}}],[\"最後得出的結果為\",{\"1\":{\"179\":2}}],[\"最後得出來的結果\",{\"1\":{\"118\":1}}],[\"最後得到的\",{\"1\":{\"99\":1}}],[\"最後將\",{\"1\":{\"118\":1}}],[\"最後比較\",{\"1\":{\"25\":1}}],[\"最後是針對\",{\"1\":{\"24\":1}}],[\"最後\",{\"1\":{\"16\":1,\"24\":1,\"25\":1,\"145\":1,\"196\":1,\"197\":1}}],[\"最後你一樣可以透過這些\",{\"1\":{\"13\":1}}],[\"最後只需要設定\",{\"1\":{\"9\":1}}],[\"最後分數越多越好\",{\"1\":{\"3\":1}}],[\"右移動\",{\"1\":{\"22\":1}}],[\"右邊是\",{\"1\":{\"7\":1}}],[\"左\",{\"1\":{\"22\":1}}],[\"左邊是\",{\"1\":{\"7\":1}}],[\"下面則是\",{\"1\":{\"201\":1}}],[\"下有多強\",{\"1\":{\"91\":1}}],[\"下拍攝\",{\"1\":{\"77\":1}}],[\"下拍攝的\",{\"1\":{\"76\":1}}],[\"下\",{\"1\":{\"22\":1,\"82\":1,\"157\":1}}],[\"隨機地放在地圖上的任意格子\",{\"1\":{\"22\":1}}],[\"我們的目標是要讓\",{\"1\":{\"196\":1}}],[\"我們最後的\",{\"1\":{\"180\":1}}],[\"我們知道\",{\"1\":{\"151\":1}}],[\"我們想要從中切出一塊\",{\"1\":{\"150\":1}}],[\"我們將\",{\"1\":{\"133\":1}}],[\"我們也就會期待\",{\"1\":{\"180\":1}}],[\"我們也可以再加上其他的技巧去做更多的處理\",{\"1\":{\"133\":1}}],[\"我們也可以去定義當前\",{\"1\":{\"90\":1}}],[\"我們也發現到模型越來越會傾向於\",{\"1\":{\"124\":1}}],[\"我們是從\",{\"1\":{\"114\":1}}],[\"我們理想上會預期那些\",{\"1\":{\"93\":1}}],[\"我們會隨著訓練過程慢慢調整\",{\"1\":{\"192\":1}}],[\"我們會寫成\",{\"1\":{\"174\":1}}],[\"我們會從其中取最大的當成是最終的\",{\"1\":{\"148\":1}}],[\"我們會盡可能讓出現頻率越低的\",{\"1\":{\"92\":1}}],[\"我們會選其中最大的當成是最後的答案\",{\"1\":{\"90\":1}}],[\"我們就希望讓他在訓練過程當中出現的頻率可以更高\",{\"1\":{\"92\":1}}],[\"我們對於\",{\"1\":{\"67\":1}}],[\"我們可以想成現在\",{\"1\":{\"66\":1}}],[\"我們透過\",{\"1\":{\"22\":1}}],[\"我的觀點\",{\"0\":{\"56\":1}}],[\"我個人覺得已經是現在進行式了\",{\"1\":{\"55\":1}}],[\"我還沒有理解這一段做了什麼\",{\"1\":{\"8\":1}}],[\"範圍變成\",{\"1\":{\"21\":1}}],[\"其方法與這一篇可說是大同小異\",{\"1\":{\"168\":1}}],[\"其餘則為\",{\"1\":{\"116\":1}}],[\"其實\",{\"1\":{\"180\":1}}],[\"其實在訓練初期其實是能夠辨別這些\",{\"1\":{\"93\":1}}],[\"其實我也不知道為什麼\",{\"1\":{\"50\":1}}],[\"其他遊戲則都是\",{\"1\":{\"116\":1}}],[\"其他的\",{\"1\":{\"78\":1}}],[\"其他絕大多都是\",{\"1\":{\"67\":1}}],[\"其他像是路燈\",{\"1\":{\"66\":1}}],[\"其他\",{\"1\":{\"21\":1,\"101\":1}}],[\"其他部分基本上都跟\",{\"1\":{\"19\":1}}],[\"其中一種方法是採用\",{\"1\":{\"191\":1}}],[\"其中一半的\",{\"1\":{\"68\":1}}],[\"其中i是指第幾次iteration\",{\"1\":{\"50\":1}}],[\"其中pi​與λi​分別為rgb\",{\"1\":{\"48\":1}}],[\"其中ax\",{\"1\":{\"43\":1}}],[\"其中的\",{\"1\":{\"13\":1}}],[\"其中\",{\"1\":{\"8\":2,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"110\":1,\"150\":1,\"151\":1,\"175\":1,\"191\":1,\"196\":1}}],[\"交給\",{\"1\":{\"19\":1}}],[\"將上述的種種\",{\"1\":{\"196\":1}}],[\"將沒有做任何修正的\",{\"1\":{\"179\":1}}],[\"將兩者結合後\",{\"1\":{\"156\":1}}],[\"將裁切後的圖片\",{\"1\":{\"150\":1}}],[\"將圖片切成\",{\"1\":{\"149\":1}}],[\"將圖像down\",{\"1\":{\"39\":1}}],[\"將產出的\",{\"1\":{\"66\":1}}],[\"將各半的neurons放在各個gpu上\",{\"1\":{\"42\":1}}],[\"將分為dataset\",{\"1\":{\"38\":1}}],[\"將\",{\"1\":{\"19\":1,\"70\":1,\"71\":1,\"114\":1,\"150\":1}}],[\"⋅∣st+i​\",{\"1\":{\"172\":1,\"177\":1}}],[\"⋅∣s\",{\"1\":{\"169\":1}}],[\"⋅k\",{\"1\":{\"150\":4}}],[\"⋅mthings\",{\"1\":{\"93\":1}}],[\"⋅\",{\"1\":{\"19\":1,\"90\":1,\"110\":1,\"148\":1}}],[\"估計當前\",{\"1\":{\"19\":1}}],[\"開始訓練\",{\"1\":{\"197\":1}}],[\"開始前\",{\"1\":{\"19\":1}}],[\"開始之前先把參數加上\",{\"1\":{\"174\":1}}],[\"開始之前他們把一個\",{\"1\":{\"22\":1}}],[\"開始之前\",{\"1\":{\"13\":1}}],[\"搭配對抗式學習對\",{\"1\":{\"200\":1}}],[\"搭配\",{\"1\":{\"17\":2,\"73\":1,\"87\":1,\"101\":1,\"112\":1,\"160\":1,\"171\":1,\"200\":2}}],[\"yong\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"youtube\",{\"1\":{\"148\":1,\"149\":2,\"152\":1}}],[\"you\",{\"1\":{\"82\":1}}],[\"y=wx+b⇒y=\",{\"1\":{\"174\":1}}],[\"y∼p\",{\"1\":{\"169\":1}}],[\"yuille\",{\"1\":{\"161\":1}}],[\"yukai\",{\"1\":{\"151\":1}}],[\"yc\",{\"1\":{\"151\":1}}],[\"yds​\",{\"1\":{\"151\":2}}],[\"ylrs​\",{\"1\":{\"148\":1}}],[\"y^​t\",{\"1\":{\"191\":2,\"192\":1,\"194\":2,\"195\":1}}],[\"y^​t​\",{\"1\":{\"190\":1,\"195\":2}}],[\"y^​c\",{\"1\":{\"151\":3}}],[\"y^​c​=fs\",{\"1\":{\"150\":1}}],[\"y^​c​\",{\"1\":{\"150\":1}}],[\"y^​dt​\",{\"1\":{\"151\":1}}],[\"y^​ds​\",{\"1\":{\"151\":2}}],[\"y^​d\",{\"1\":{\"151\":1}}],[\"y^​d​=fs\",{\"1\":{\"150\":1}}],[\"y^​d​\",{\"1\":{\"150\":1,\"151\":1}}],[\"y^​lrt​\",{\"1\":{\"148\":1}}],[\"y^​lrs​\",{\"1\":{\"148\":1}}],[\"y^​lrs​=fθ​\",{\"1\":{\"148\":1}}],[\"y^​\",{\"1\":{\"148\":3}}],[\"yhrs\",{\"1\":{\"148\":2}}],[\"yt\",{\"1\":{\"90\":1}}],[\"yt​\",{\"1\":{\"90\":1,\"190\":2,\"191\":1}}],[\"yt​=\",{\"1\":{\"90\":1,\"190\":1}}],[\"yt​^​=ξ\",{\"1\":{\"191\":1}}],[\"yt​^​=t^q\",{\"1\":{\"8\":1}}],[\"yt​^​\",{\"1\":{\"8\":1,\"71\":2}}],[\"ys=\",{\"1\":{\"148\":1}}],[\"ysc​\",{\"1\":{\"93\":1}}],[\"ys\",{\"1\":{\"90\":1,\"92\":1,\"93\":2}}],[\"ys​=\",{\"1\":{\"90\":1,\"190\":1}}],[\"ys​\",{\"1\":{\"71\":5,\"90\":1,\"190\":2,\"191\":1}}],[\"ym​\",{\"1\":{\"71\":4}}],[\"ya​\",{\"1\":{\"68\":1}}],[\"yang\",{\"1\":{\"67\":1,\"161\":1}}],[\"yi\",{\"1\":{\"66\":2,\"73\":1,\"161\":1}}],[\"yiheng\",{\"1\":{\"64\":1}}],[\"yi​−q\",{\"1\":{\"110\":1}}],[\"yi​​=es\",{\"1\":{\"110\":1}}],[\"yi​則是做完normalization的值\",{\"1\":{\"43\":1}}],[\"yi​為經過位置在\",{\"1\":{\"43\":1}}],[\"y\",{\"1\":{\"22\":1,\"43\":1,\"129\":2,\"148\":4,\"169\":2,\"170\":3,\"171\":3,\"176\":10}}],[\"yk​\",{\"1\":{\"16\":1}}],[\"之前在\",{\"1\":{\"200\":1}}],[\"之類的\",{\"1\":{\"70\":1}}],[\"之間的距離\",{\"1\":{\"192\":1}}],[\"之間的距離去調整\",{\"1\":{\"192\":1}}],[\"之間的距離可以拉近\",{\"1\":{\"93\":1}}],[\"之間\",{\"1\":{\"151\":1}}],[\"之間會是比較好的選擇\",{\"1\":{\"137\":1}}],[\"之間會比較平坦\",{\"1\":{\"137\":1}}],[\"之間都會具有相當高的相關性\",{\"1\":{\"108\":1}}],[\"之間有一些重疊的\",{\"1\":{\"24\":1}}],[\"之間均勻分布的隨機\",{\"1\":{\"16\":1}}],[\"之間均勻分布的隨機值\",{\"1\":{\"16\":1}}],[\"之後可以得到底下的\",{\"1\":{\"171\":1}}],[\"之後可以來到\",{\"1\":{\"133\":1}}],[\"之後的結果都有些進步\",{\"1\":{\"179\":1}}],[\"之後的誤差\",{\"1\":{\"170\":1}}],[\"之後的預測結果\",{\"1\":{\"151\":1}}],[\"之後就可以降低到\",{\"1\":{\"133\":1}}],[\"之後能夠還原出輸入的原貌\",{\"1\":{\"126\":1}}],[\"之後作為實際上儲存進\",{\"1\":{\"114\":1}}],[\"之後得到的成效在\",{\"1\":{\"24\":1}}],[\"之後\",{\"1\":{\"24\":1,\"97\":4,\"100\":1,\"114\":1,\"129\":1,\"136\":1,\"174\":1}}],[\"之後仍然不會停止的話就不會出現\",{\"1\":{\"22\":1}}],[\"之後交給\",{\"1\":{\"19\":1}}],[\"之所以說\",{\"1\":{\"3\":1}}],[\"多加上\",{\"1\":{\"16\":1}}],[\"多多探索\",{\"1\":{\"6\":1}}],[\"對所有\",{\"1\":{\"156\":1}}],[\"對應的\",{\"1\":{\"68\":2,\"90\":2,\"148\":1,\"190\":1,\"196\":1}}],[\"對陌生人的認識\",{\"1\":{\"67\":1}}],[\"對抗式學習\",{\"1\":{\"66\":1}}],[\"對於一開始預測錯誤的\",{\"1\":{\"195\":1}}],[\"對於效能提升大約\",{\"1\":{\"159\":1}}],[\"對於原本\",{\"1\":{\"150\":1}}],[\"對於模型的訓練沒有什麼幫助\",{\"1\":{\"138\":1}}],[\"對於這些\",{\"1\":{\"92\":1}}],[\"對於特定image的每個pixel來說\",{\"1\":{\"48\":1}}],[\"對於每個\",{\"1\":{\"192\":1}}],[\"對於每個可訓練的參數拆解成\",{\"1\":{\"174\":1}}],[\"對於每個rgb\",{\"1\":{\"48\":1}}],[\"對於每個實驗的\",{\"1\":{\"21\":1}}],[\"對於\",{\"1\":{\"17\":1,\"67\":1,\"156\":1,\"160\":1,\"172\":1,\"196\":1}}],[\"對於結果並不會有影響\",{\"1\":{\"16\":1}}],[\"對於學習是並沒有幫助的\",{\"1\":{\"10\":1}}],[\"對\",{\"1\":{\"16\":1,\"133\":1,\"151\":1,\"176\":1,\"195\":1}}],[\"剩餘的都是相同的\",{\"1\":{\"15\":1}}],[\"剩下的bias都初始化為0\",{\"1\":{\"50\":1}}],[\"剩下的四款遊戲則是因為環境太大\",{\"1\":{\"3\":1}}],[\"剩下這些遊戲有怎樣的共通點呢\",{\"1\":{\"3\":1}}],[\"僅僅是加上\",{\"1\":{\"15\":1}}],[\"來估計\",{\"1\":{\"194\":1}}],[\"來表示\",{\"1\":{\"191\":1}}],[\"來表示一個點要偏向用\",{\"1\":{\"151\":1}}],[\"來轉換\",{\"1\":{\"191\":1}}],[\"來調整模型的機率分布\",{\"1\":{\"191\":1}}],[\"來調整整體\",{\"1\":{\"22\":1}}],[\"來達成\",{\"1\":{\"168\":1}}],[\"來分別處理這兩個部分\",{\"1\":{\"150\":1}}],[\"來近似\",{\"1\":{\"110\":1}}],[\"來說可能導致收斂不穩定以及緩慢等問題\",{\"1\":{\"172\":1}}],[\"來說它造成了怎樣的影響呢\",{\"1\":{\"136\":1}}],[\"來說\",{\"1\":{\"110\":1,\"129\":1,\"174\":1}}],[\"來說由於缺乏對於\",{\"1\":{\"67\":1}}],[\"來說是相當大的問題\",{\"1\":{\"67\":1}}],[\"來尋找參數\",{\"1\":{\"50\":1}}],[\"來讓每個\",{\"1\":{\"27\":1}}],[\"來比較\",{\"1\":{\"23\":1}}],[\"來限制要考慮多久之前的經驗\",{\"1\":{\"15\":1}}],[\"來解決它\",{\"1\":{\"14\":1}}],[\"來解決\",{\"1\":{\"13\":1,\"187\":1}}],[\"採取\",{\"1\":{\"110\":1}}],[\"採取的次數以及得到的\",{\"1\":{\"15\":1}}],[\"採用的\",{\"1\":{\"151\":1}}],[\"採用\",{\"1\":{\"96\":1,\"159\":2,\"160\":1}}],[\"採用了\",{\"1\":{\"93\":1,\"148\":1}}],[\"採用不同的\",{\"1\":{\"13\":1}}],[\"採用分散式學習\",{\"1\":{\"9\":1}}],[\"而接下來作者給出一個\",{\"1\":{\"175\":1}}],[\"而言都獲得更好的結果\",{\"1\":{\"155\":1}}],[\"而言就算他答對\",{\"1\":{\"133\":1}}],[\"而且可以發現到幾乎所有的\",{\"1\":{\"136\":1}}],[\"而且也往往會經過一段時間的延遲才取得\",{\"1\":{\"108\":1}}],[\"而在\",{\"1\":{\"168\":2}}],[\"而在右邊的圖當中可以看到如果是固定\",{\"1\":{\"137\":1}}],[\"而在所有的\",{\"1\":{\"133\":1}}],[\"而在這些許多不同的架構下\",{\"1\":{\"133\":1}}],[\"而在測試階段\",{\"1\":{\"129\":1}}],[\"而在測試時\",{\"1\":{\"47\":1}}],[\"而對於這一層\",{\"1\":{\"129\":1}}],[\"而最一開始的結果其實是還不錯的\",{\"1\":{\"100\":1}}],[\"而最重要的關鍵其實在於深度\",{\"1\":{\"55\":1}}],[\"而一個\",{\"1\":{\"92\":1,\"191\":1}}],[\"而\",{\"1\":{\"67\":1,\"73\":1,\"74\":1,\"110\":1,\"113\":1,\"133\":1,\"148\":2,\"194\":1}}],[\"而半監督式學習困難的點在於雖然對於\",{\"1\":{\"67\":1}}],[\"而被提出的\",{\"1\":{\"67\":1}}],[\"而alexnet也從此被視為在computer\",{\"1\":{\"57\":1}}],[\"而alexnet的出現也是受惠於gpu發展的產物\",{\"1\":{\"56\":1}}],[\"而alexnet使用f\",{\"1\":{\"41\":1}}],[\"而這裡則選擇在\",{\"1\":{\"168\":1}}],[\"而這種探索的困難度甚至是指數性地成長\",{\"1\":{\"166\":1}}],[\"而這張圖可以看到說\",{\"1\":{\"53\":1}}],[\"而這樣的深度在處理大量圖像分類是必須的\",{\"1\":{\"38\":1}}],[\"而gpu2中的kernel學到的大多都是跟顏色相關的\",{\"1\":{\"53\":1}}],[\"而每一層的learning\",{\"1\":{\"50\":1}}],[\"而n\",{\"1\":{\"43\":1}}],[\"而n則為與之相鄰的kernel數\",{\"1\":{\"43\":1}}],[\"而是給了參數\",{\"1\":{\"174\":1}}],[\"而是另外定義了一個\",{\"1\":{\"171\":1}}],[\"而是自駕車車體\",{\"1\":{\"100\":1}}],[\"而是包含了機率的概念\",{\"1\":{\"90\":1}}],[\"而是將\",{\"1\":{\"71\":1}}],[\"而是\",{\"1\":{\"21\":1}}],[\"而已\",{\"1\":{\"15\":1,\"151\":1}}],[\"而隨著\",{\"1\":{\"15\":1}}],[\"而整體的\",{\"1\":{\"6\":1}}],[\"而整體\",{\"1\":{\"6\":1}}],[\"並預測出結果\",{\"1\":{\"149\":1}}],[\"並無法繼續擴充到其他的領域\",{\"1\":{\"111\":1}}],[\"並無法好好只透過一個\",{\"1\":{\"22\":1}}],[\"並\",{\"1\":{\"110\":1}}],[\"並沒有得到比\",{\"1\":{\"91\":1}}],[\"並沒有\",{\"1\":{\"80\":1}}],[\"並不會都得出\",{\"1\":{\"180\":1}}],[\"並不會有更好的\",{\"1\":{\"159\":1}}],[\"並不會是一個好的選項\",{\"1\":{\"15\":1}}],[\"並不一定要是\",{\"1\":{\"78\":1}}],[\"並不是所有的\",{\"1\":{\"67\":1}}],[\"並將第2\",{\"1\":{\"50\":1}}],[\"並將softmax\",{\"1\":{\"47\":1}}],[\"並直接在這些patches上訓練\",{\"1\":{\"47\":1}}],[\"並讓gpu只在特定的layers進行溝通\",{\"1\":{\"42\":1}}],[\"並參加了ilsvrc\",{\"1\":{\"39\":1}}],[\"並以人工標記\",{\"1\":{\"39\":1}}],[\"並得到了相較前人方法顯著優異許多的表現\",{\"1\":{\"33\":1}}],[\"並且預期能夠讓\",{\"1\":{\"198\":1}}],[\"並且分成了弱增強\",{\"1\":{\"196\":1}}],[\"並且沒有使用\",{\"1\":{\"172\":1}}],[\"並且達到了很棒的效果\",{\"1\":{\"168\":1}}],[\"並且如果進一步把\",{\"1\":{\"158\":1}}],[\"並且如果將\",{\"1\":{\"155\":1}}],[\"並且對於大的物件也同樣保有更好的結果\",{\"1\":{\"155\":1}}],[\"並且選擇了一個\",{\"1\":{\"150\":1}}],[\"並且進一步透過其他的技巧可以再做得更好\",{\"1\":{\"133\":1}}],[\"並且發現這樣的做法實際上對於訓練有相當好的幫助\",{\"1\":{\"126\":1}}],[\"並且通常會有更好的表現\",{\"1\":{\"124\":1}}],[\"並且做出相對應的\",{\"1\":{\"116\":1}}],[\"並且會使用\",{\"1\":{\"116\":1}}],[\"並且\",{\"1\":{\"114\":1,\"155\":1,\"158\":1}}],[\"並且證明了底下兩個狀況是可以確保收斂\",{\"1\":{\"112\":1}}],[\"並且最終的\",{\"1\":{\"101\":1}}],[\"並且普遍最後的\",{\"1\":{\"99\":1}}],[\"並且也可以觀察到那些比較早開始有所提升的\",{\"1\":{\"99\":1}}],[\"並且也是\",{\"1\":{\"12\":1}}],[\"並且透過觀察訓練過程作者發現到\",{\"1\":{\"93\":1}}],[\"並且有趣的是\",{\"1\":{\"91\":1}}],[\"並且不同\",{\"1\":{\"27\":1}}],[\"並且每個\",{\"1\":{\"22\":1}}],[\"然後拿去\",{\"1\":{\"198\":1}}],[\"然後拿到了\",{\"1\":{\"80\":1}}],[\"然後會有一個\",{\"1\":{\"66\":1}}],[\"然後應用在真實的環境當中\",{\"1\":{\"63\":1}}],[\"然後再拿去訓練\",{\"1\":{\"187\":1}}],[\"然後再透過\",{\"1\":{\"174\":1}}],[\"然後再把這些特徵丟去給\",{\"1\":{\"113\":1}}],[\"然後再應用在真實的世界當中\",{\"1\":{\"63\":1}}],[\"然後再讓\",{\"1\":{\"9\":1}}],[\"然後結束這個\",{\"1\":{\"22\":1}}],[\"然後繼續跟環境互動\",{\"1\":{\"19\":1}}],[\"然而因為\",{\"1\":{\"196\":1}}],[\"然而若觀察倒數第二個\",{\"1\":{\"180\":1}}],[\"然而若直接使用高解析度的圖片\",{\"1\":{\"145\":1}}],[\"然而存在幾個缺點\",{\"1\":{\"172\":1}}],[\"然而使用了\",{\"1\":{\"170\":1}}],[\"然而在現實狀況下往往並不會如此簡單\",{\"1\":{\"166\":1}}],[\"然而由於\",{\"1\":{\"152\":1}}],[\"然而我們會希望每一個地方都能夠精確地考慮\",{\"1\":{\"152\":1}}],[\"然而我們也看到近幾年\",{\"1\":{\"108\":1}}],[\"然而這種做法每次要更新\",{\"1\":{\"194\":1}}],[\"然而這種方法的\",{\"1\":{\"90\":1}}],[\"然而這些研究都並未能夠給出用非線性去學\",{\"1\":{\"112\":1}}],[\"然而\",{\"1\":{\"15\":1,\"93\":1,\"124\":1,\"136\":1,\"170\":1,\"192\":1}}],[\"從訓練中的曲線也可以明顯看到\",{\"1\":{\"179\":1}}],[\"從圖片當中可以再次觀察到\",{\"1\":{\"159\":1}}],[\"從圖片上來觀察\",{\"1\":{\"155\":1}}],[\"從上表可以觀察到\",{\"1\":{\"159\":1}}],[\"從上面的圖片中也可以觀察到這樣的\",{\"1\":{\"24\":1}}],[\"從生物學的角度來看\",{\"1\":{\"124\":1}}],[\"從\",{\"1\":{\"71\":2,\"151\":2}}],[\"從這裡也可以了解到實際上讓每個\",{\"1\":{\"24\":1}}],[\"從這些觀察當中可以得到兩個待改善的地方\",{\"1\":{\"3\":1}}],[\"從結果可以發現到\",{\"1\":{\"22\":1}}],[\"從式子當中也可以觀察到\",{\"1\":{\"14\":1}}],[\"嘗試機率高\",{\"1\":{\"14\":1}}],[\"嘗試機率低\",{\"1\":{\"14\":1}}],[\"嘗試次數少\",{\"1\":{\"14\":1}}],[\"嘗試次數少的選擇\",{\"1\":{\"14\":1}}],[\"嘗試次數多的選擇\",{\"1\":{\"14\":1}}],[\"更加密集\",{\"1\":{\"196\":1}}],[\"更細節來說\",{\"1\":{\"150\":1}}],[\"更早看到它\",{\"1\":{\"92\":1}}],[\"更多關於模型選擇的實驗\",{\"1\":{\"91\":1}}],[\"更點出了\",{\"1\":{\"56\":1}}],[\"更深的模型\",{\"1\":{\"55\":1}}],[\"更好的\",{\"1\":{\"158\":1}}],[\"更好的結果\",{\"1\":{\"155\":1,\"160\":1}}],[\"更好的表現\",{\"1\":{\"91\":1}}],[\"更好的成效\",{\"1\":{\"27\":1}}],[\"更好學習\",{\"1\":{\"8\":1}}],[\"更新參數是從\",{\"1\":{\"169\":1}}],[\"更新模型參數\",{\"1\":{\"19\":1}}],[\"更新\",{\"1\":{\"15\":1}}],[\"更高\",{\"1\":{\"14\":1}}],[\"高估的問題消失\",{\"1\":{\"170\":1}}],[\"高估的狀況如底下的綠線\",{\"1\":{\"170\":1}}],[\"高\",{\"1\":{\"14\":5}}],[\"➡️\",{\"1\":{\"14\":8}}],[\"低\",{\"1\":{\"14\":3}}],[\"平均去評估\",{\"1\":{\"117\":1}}],[\"平均是0的高斯分佈來初始化每一層的權重\",{\"1\":{\"50\":1}}],[\"平均\",{\"1\":{\"14\":5,\"15\":1,\"194\":1}}],[\"未知\",{\"1\":{\"14\":1}}],[\"目標在於給定\",{\"1\":{\"191\":1}}],[\"目標\",{\"1\":{\"170\":2}}],[\"目標同樣是要分類\",{\"1\":{\"135\":1}}],[\"目標同樣是將每個圖片分類到正確的類別當中\",{\"1\":{\"133\":1}}],[\"目標是要去把每個演講分類到正確的主題當中\",{\"1\":{\"134\":1}}],[\"目標是要辨識出門牌號碼\",{\"1\":{\"133\":1}}],[\"目標是在整個\",{\"1\":{\"14\":1}}],[\"目的是要訓練出一個\",{\"1\":{\"198\":1}}],[\"目的是要先訓練出一個\",{\"1\":{\"198\":1}}],[\"目的是要分類每張圖片至正確的類別\",{\"1\":{\"133\":1}}],[\"目的是要辨認出每個圖片是對應到哪個數字\",{\"1\":{\"133\":1}}],[\"目的也是希望能夠讓\",{\"1\":{\"5\":1}}],[\"你知道cross\",{\"1\":{\"204\":1}}],[\"你們整體看起來的表現會很不錯\",{\"1\":{\"136\":1}}],[\"你的失誤\",{\"1\":{\"136\":1}}],[\"你選擇\",{\"1\":{\"14\":1}}],[\"你分別把這幾個\",{\"1\":{\"13\":1}}],[\"kd\",{\"1\":{\"197\":1}}],[\"kl\",{\"1\":{\"196\":1}}],[\"koray\",{\"1\":{\"107\":1,\"114\":1,\"117\":1,\"118\":1}}],[\"knowledge\",{\"1\":{\"90\":1,\"145\":1,\"148\":1,\"197\":1,\"198\":1,\"200\":1,\"203\":1}}],[\"know\",{\"1\":{\"82\":1}}],[\"k=argmaxk\",{\"1\":{\"191\":1}}],[\"k=s⋅o\",{\"1\":{\"150\":1}}],[\"k=4\",{\"1\":{\"116\":1}}],[\"k=3\",{\"1\":{\"116\":1}}],[\"k=2\",{\"1\":{\"43\":1}}],[\"k=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"krizhevsky\",{\"1\":{\"32\":1,\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"k−τ\",{\"1\":{\"15\":2}}],[\"k−1\",{\"1\":{\"14\":1,\"15\":1}}],[\"kalman\",{\"1\":{\"182\":1}}],[\"kavukcuoglu\",{\"1\":{\"107\":1,\"114\":1,\"117\":1,\"118\":1}}],[\"kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"kapturowski\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"k\",{\"1\":{\"14\":2,\"15\":1,\"43\":1,\"116\":1,\"150\":5,\"172\":1,\"190\":1,\"191\":7,\"192\":9,\"193\":6,\"194\":9,\"195\":3,\"196\":7}}],[\"0​ifobd\",{\"1\":{\"151\":1}}],[\"0​ifs⋅obd\",{\"1\":{\"151\":1}}],[\"05\",{\"1\":{\"135\":1}}],[\"0~9\",{\"1\":{\"133\":1}}],[\"02\",{\"1\":{\"80\":1,\"133\":1}}],[\"017\",{\"1\":{\"177\":1}}],[\"01\",{\"1\":{\"50\":2}}],[\"04\",{\"1\":{\"24\":1}}],[\"000\",{\"1\":{\"133\":1}}],[\"00\",{\"1\":{\"24\":6}}],[\"06\",{\"1\":{\"24\":1}}],[\"0\",{\"1\":{\"14\":1,\"15\":2,\"16\":2,\"21\":3,\"22\":3,\"24\":5,\"36\":1,\"41\":1,\"52\":1,\"80\":1,\"90\":1,\"97\":1,\"100\":1,\"116\":1,\"129\":1,\"130\":1,\"136\":1,\"137\":3,\"148\":2,\"150\":4,\"151\":8,\"159\":2,\"168\":1,\"171\":2,\"179\":1,\"180\":2,\"191\":1,\"192\":2,\"195\":1}}],[\"把所有對應到\",{\"1\":{\"194\":1}}],[\"把四隻腳站立的動物都當成草食類動物一樣\",{\"1\":{\"187\":1}}],[\"把兩個圖片\",{\"1\":{\"68\":1}}],[\"把這個上界當成是它預期的\",{\"1\":{\"14\":1}}],[\"把\",{\"1\":{\"14\":1,\"68\":3,\"70\":1,\"71\":1,\"91\":1,\"171\":1}}],[\"ϵj​∈rp\",{\"1\":{\"175\":1}}],[\"ϵj​\",{\"1\":{\"175\":2}}],[\"ϵi​∈rq\",{\"1\":{\"175\":1}}],[\"ϵi​\",{\"1\":{\"175\":1}}],[\"ϵi\",{\"1\":{\"175\":1}}],[\"ϵb∈rq\",{\"1\":{\"175\":1}}],[\"ϵb\",{\"1\":{\"174\":1}}],[\"ϵw∈rq×p\",{\"1\":{\"175\":1}}],[\"ϵw\",{\"1\":{\"174\":1}}],[\"ϵucb​\",{\"1\":{\"16\":1}}],[\"ϵ\",{\"1\":{\"13\":1,\"16\":1,\"114\":1,\"166\":1,\"167\":1,\"168\":3,\"169\":1,\"171\":1,\"174\":1,\"176\":8,\"177\":3,\"180\":2}}],[\"ϵl​=ϵ1+αl−11​\",{\"1\":{\"19\":1}}],[\"ϵl​\",{\"1\":{\"13\":1,\"19\":2}}],[\"wt\",{\"1\":{\"192\":3,\"193\":1,\"195\":1}}],[\"wtest\",{\"1\":{\"129\":1}}],[\"w∈rq×p\",{\"1\":{\"175\":1}}],[\"w−swc​\",{\"1\":{\"150\":1}}],[\"wc​=wd​\",{\"1\":{\"150\":1}}],[\"wen\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"wei\",{\"1\":{\"161\":1}}],[\"weight\",{\"1\":{\"130\":1,\"175\":1,\"192\":3,\"196\":1}}],[\"weights\",{\"1\":{\"129\":1,\"197\":1}}],[\"weak\",{\"1\":{\"87\":1}}],[\"wf​\",{\"1\":{\"93\":1}}],[\"w\",{\"1\":{\"90\":1,\"93\":1,\"129\":1}}],[\"work\",{\"1\":{\"64\":1}}],[\"works\",{\"0\":{\"4\":1,\"34\":1,\"65\":1,\"88\":1,\"109\":1,\"125\":1,\"146\":1,\"167\":1,\"188\":1}}],[\"wang\",{\"1\":{\"161\":1,\"169\":1,\"171\":1,\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"way\",{\"0\":{\"122\":1}}],[\"warmup\",{\"0\":{\"94\":1,\"98\":1},\"1\":{\"91\":1,\"94\":1,\"97\":2,\"98\":1,\"148\":1,\"200\":1}}],[\"warning\",{\"1\":{\"13\":1,\"168\":1}}],[\"wall\",{\"1\":{\"77\":1}}],[\"wacv\",{\"1\":{\"62\":1},\"2\":{\"84\":1}}],[\"what\",{\"0\":{\"63\":1},\"1\":{\"58\":1}}],[\"wikipedia\",{\"1\":{\"126\":1}}],[\"wilhelm\",{\"1\":{\"70\":2,\"71\":2,\"78\":2,\"79\":1}}],[\"winter\",{\"1\":{\"62\":1}}],[\"window\",{\"0\":{\"15\":1,\"16\":1,\"17\":1,\"23\":1,\"152\":1},\"1\":{\"15\":2,\"16\":1,\"152\":1}}],[\"with\",{\"0\":{\"31\":1,\"106\":1,\"152\":1},\"1\":{\"58\":1,\"81\":1,\"92\":1,\"109\":1,\"168\":2,\"182\":2}}],[\"中這一點尤其重要\",{\"1\":{\"195\":1}}],[\"中心點越近\",{\"1\":{\"192\":1}}],[\"中隨意挑一筆\",{\"1\":{\"169\":1}}],[\"中調整\",{\"1\":{\"156\":1}}],[\"中提出的\",{\"1\":{\"154\":1}}],[\"中間的部分才會有\",{\"1\":{\"151\":1}}],[\"中的\",{\"1\":{\"114\":1}}],[\"中文敘述參考\",{\"1\":{\"91\":1}}],[\"中各取圖片\",{\"1\":{\"66\":1}}],[\"中可以不做normalization\",{\"1\":{\"43\":1}}],[\"中\",{\"1\":{\"13\":1}}],[\"此外\",{\"1\":{\"13\":1,\"21\":1,\"22\":1,\"24\":1,\"90\":2,\"91\":1,\"100\":2,\"157\":1,\"180\":1}}],[\"前面都加上一組\",{\"1\":{\"13\":1}}],[\"具有相同的\",{\"1\":{\"196\":1}}],[\"具有更高的靈活性\",{\"1\":{\"13\":1}}],[\"具體而言有三個部分\",{\"1\":{\"91\":1}}],[\"具體來說就是他們試圖在\",{\"1\":{\"168\":1}}],[\"具體來說如下圖\",{\"1\":{\"21\":1}}],[\"具體來說\",{\"1\":{\"5\":1,\"68\":1}}],[\"蒐集的\",{\"1\":{\"13\":1}}],[\"蒐集一些\",{\"1\":{\"13\":1}}],[\"什麼時候該\",{\"1\":{\"13\":1}}],[\"透過前面的步驟得到的模型稱為\",{\"1\":{\"197\":1}}],[\"透過如\",{\"1\":{\"191\":1}}],[\"透過儲存\",{\"1\":{\"169\":1}}],[\"透過同時考慮\",{\"1\":{\"149\":1}}],[\"透過一個\",{\"1\":{\"111\":1}}],[\"透過一些方式混在一起\",{\"1\":{\"68\":1}}],[\"透過固定訓練的目標\",{\"1\":{\"110\":1}}],[\"透過產生假想的\",{\"1\":{\"90\":1}}],[\"透過這個模型我們就有辦法給\",{\"1\":{\"67\":1}}],[\"透過相鄰的neurons間的相互抑制來減少雜訊\",{\"1\":{\"43\":1}}],[\"透過給定一個特定的機率\",{\"1\":{\"37\":1}}],[\"透過上一個\",{\"1\":{\"19\":1}}],[\"透過各自的\",{\"1\":{\"19\":1}}],[\"透過\",{\"1\":{\"16\":1,\"17\":1,\"19\":1,\"25\":1,\"66\":1,\"71\":1,\"114\":1,\"119\":1,\"148\":2,\"149\":2,\"151\":1,\"169\":1,\"171\":1,\"180\":1,\"187\":1}}],[\"透過它決定接下來要使用的\",{\"1\":{\"13\":1}}],[\"透過加上\",{\"1\":{\"13\":1,\"100\":1}}],[\"透過拆開訓練\",{\"1\":{\"12\":1}}],[\"無論是對\",{\"1\":{\"156\":1}}],[\"無論是單純加上\",{\"1\":{\"134\":1}}],[\"無論是否有使用\",{\"1\":{\"12\":1}}],[\"無法收斂的問題可以透過\",{\"1\":{\"112\":1}}],[\"無法收斂\",{\"1\":{\"112\":1}}],[\"無法好好處理\",{\"1\":{\"10\":1,\"17\":1}}],[\"輸入進去\",{\"1\":{\"12\":1}}],[\"於是他們定義了底下的\",{\"1\":{\"169\":1}}],[\"於是把每個點跟\",{\"1\":{\"151\":1}}],[\"於是我們再加上一個\",{\"1\":{\"151\":1}}],[\"於是對於\",{\"1\":{\"129\":1}}],[\"於是乎最後的整體\",{\"1\":{\"93\":1}}],[\"於是作者嘗試修改\",{\"1\":{\"91\":1}}],[\"於是\",{\"1\":{\"12\":1,\"90\":1,\"91\":1,\"92\":1}}],[\"為甚麼可以直接拿去\",{\"1\":{\"198\":1}}],[\"為了避免\",{\"1\":{\"197\":1}}],[\"為了避免所謂的\",{\"1\":{\"196\":1}}],[\"為了進一步去釐清這樣的做法為什麼是可行\",{\"1\":{\"180\":1}}],[\"為了同時保有\",{\"1\":{\"150\":1}}],[\"為了測試\",{\"1\":{\"132\":1}}],[\"為了降低成本與訓練時間\",{\"1\":{\"38\":1}}],[\"為當前\",{\"1\":{\"12\":1}}],[\"為目標\",{\"1\":{\"12\":1}}],[\"為\",{\"1\":{\"12\":3,\"110\":1,\"116\":1,\"151\":1}}],[\"表示當前\",{\"1\":{\"194\":1}}],[\"表示第\",{\"1\":{\"193\":1}}],[\"表示著在當前這個\",{\"1\":{\"171\":1}}],[\"表示要訓練的模型\",{\"1\":{\"148\":1}}],[\"表示每個\",{\"1\":{\"129\":1}}],[\"表示不影響\",{\"1\":{\"116\":1}}],[\"表示不同的\",{\"1\":{\"13\":1}}],[\"表示在時間\",{\"1\":{\"110\":1}}],[\"表示\",{\"1\":{\"12\":1,\"24\":1,\"90\":8,\"129\":6,\"148\":5,\"150\":1,\"190\":1,\"191\":1,\"193\":2}}],[\"表示從\",{\"1\":{\"12\":1}}],[\"表示使用的是哪一個\",{\"1\":{\"12\":1}}],[\"因為我們現在對每一個\",{\"1\":{\"201\":1}}],[\"因為無法好好擷取特徵\",{\"1\":{\"145\":1}}],[\"因為\",{\"1\":{\"133\":1,\"195\":1,\"196\":1}}],[\"因為遊戲當中的雷射會跑很快\",{\"1\":{\"116\":1}}],[\"因為缺乏對他人的理解\",{\"1\":{\"64\":1}}],[\"因為dropout減少了neurons間的co\",{\"1\":{\"49\":1}}],[\"因為過去的經驗即便在現實狀況改變仍然有大影響力\",{\"1\":{\"15\":1}}],[\"因為是一次更新\",{\"1\":{\"12\":1}}],[\"因此適合用來教\",{\"1\":{\"196\":1}}],[\"因此透過\",{\"1\":{\"196\":1}}],[\"因此不太需要考慮上述的\",{\"1\":{\"177\":1}}],[\"因此上述的\",{\"1\":{\"176\":1}}],[\"因此在參數上也就包含了兩項\",{\"1\":{\"172\":1}}],[\"因此在評分上我們可以考慮\",{\"1\":{\"133\":1}}],[\"因此細節上是還會對\",{\"1\":{\"171\":1}}],[\"因此仍然沒有解決問題\",{\"1\":{\"166\":1}}],[\"因此驗證了假設是正確的\",{\"1\":{\"156\":1}}],[\"因此當訓練時與預測時採用的解析度大小相同會是最理想的\",{\"1\":{\"152\":1}}],[\"因此會希望能夠取得所有點的\",{\"1\":{\"152\":1}}],[\"因此會比較能夠好好評估\",{\"1\":{\"21\":1}}],[\"因此我們會訓練出一個\",{\"1\":{\"148\":1}}],[\"因此對於上一層的\",{\"1\":{\"129\":1}}],[\"因此作者改用一個\",{\"1\":{\"194\":1}}],[\"因此作者改成\",{\"1\":{\"117\":1}}],[\"因此作者提出的方法是將\",{\"1\":{\"192\":1}}],[\"因此作者採用相同解析度大小\",{\"1\":{\"152\":1}}],[\"因此作者認為選擇\",{\"1\":{\"137\":1}}],[\"因此作者認為這是跟圖片被\",{\"1\":{\"99\":1}}],[\"因此作者認為\",{\"1\":{\"24\":1}}],[\"因此評估一個\",{\"1\":{\"117\":1}}],[\"因此這一部分會針對記憶體用量的部分去做討論\",{\"1\":{\"158\":1}}],[\"因此這一部分就是要驗這這個假設\",{\"1\":{\"156\":1}}],[\"因此這一篇論文提出一個方法試圖去消除\",{\"1\":{\"166\":1}}],[\"因此這一篇\",{\"1\":{\"87\":1}}],[\"因此這裡的拉近只會針對\",{\"1\":{\"93\":1}}],[\"因此這時候\",{\"1\":{\"68\":1}}],[\"因此算力是非常重要的\",{\"1\":{\"55\":1}}],[\"因此就是一個non\",{\"1\":{\"36\":1}}],[\"因此\",{\"1\":{\"12\":1,\"15\":1,\"36\":1,\"37\":1,\"42\":1,\"45\":1,\"63\":1,\"93\":1,\"100\":1,\"114\":1,\"130\":1,\"170\":1,\"174\":1,\"180\":1}}],[\"個數值\",{\"1\":{\"175\":1}}],[\"個主題\",{\"1\":{\"135\":1}}],[\"個講者的演講資料\",{\"1\":{\"134\":1}}],[\"個當中\",{\"1\":{\"133\":1}}],[\"個類別\",{\"1\":{\"133\":1}}],[\"個小時\",{\"1\":{\"102\":1}}],[\"個共通的\",{\"1\":{\"90\":1,\"190\":1}}],[\"個的平均\",{\"1\":{\"79\":1}}],[\"個平均跟\",{\"1\":{\"79\":1}}],[\"個比較困難的遊戲當中\",{\"1\":{\"24\":1}}],[\"個比較難的遊戲當中測試的結果\",{\"1\":{\"23\":1}}],[\"個\",{\"1\":{\"12\":1,\"14\":1,\"19\":1,\"22\":1,\"77\":1,\"79\":2,\"93\":1,\"114\":1,\"116\":2,\"128\":1,\"175\":1,\"179\":1}}],[\"個遊戲場景\",{\"1\":{\"3\":1}}],[\"個遊戲是\",{\"1\":{\"3\":1}}],[\"個遊戲當中有\",{\"1\":{\"3\":1}}],[\"細節上\",{\"1\":{\"12\":1,\"13\":1,\"22\":1}}],[\"時於\",{\"1\":{\"172\":1}}],[\"時的\",{\"1\":{\"145\":1}}],[\"時訓練不佳的問題\",{\"1\":{\"119\":1}}],[\"時常我們會訓練在合成資料上\",{\"1\":{\"63\":1}}],[\"時間\",{\"0\":{\"175\":1},\"1\":{\"12\":2}}],[\"時\",{\"1\":{\"12\":1}}],[\"跟每個\",{\"1\":{\"193\":1}}],[\"跟對\",{\"1\":{\"129\":1}}],[\"跟幾個\",{\"1\":{\"118\":1}}],[\"跟環境的互動過程當中的\",{\"1\":{\"114\":1}}],[\"跟這一篇\",{\"1\":{\"113\":1}}],[\"跟其他架構相比\",{\"1\":{\"101\":1}}],[\"跟目標相同\",{\"1\":{\"90\":1}}],[\"跟他們的horizontal\",{\"1\":{\"47\":1}}],[\"跟0\",{\"1\":{\"44\":1}}],[\"跟\",{\"1\":{\"12\":1,\"24\":1,\"68\":2,\"71\":1,\"73\":1,\"118\":1,\"174\":1,\"179\":2,\"192\":1}}],[\"相比更加重要\",{\"1\":{\"156\":1}}],[\"相比多了\",{\"1\":{\"155\":2}}],[\"相較之下有更大的影響\",{\"1\":{\"159\":1}}],[\"相較之下\",{\"1\":{\"101\":1,\"108\":1,\"168\":1}}],[\"相較於s=2\",{\"1\":{\"44\":1}}],[\"相較於單個gpu\",{\"1\":{\"42\":1}}],[\"相鄰而導致的誤判被稱為\",{\"1\":{\"70\":1}}],[\"相差過大\",{\"1\":{\"63\":1}}],[\"相對的\",{\"1\":{\"22\":1}}],[\"相同\",{\"1\":{\"19\":1,\"90\":1,\"151\":1}}],[\"相同的\",{\"1\":{\"12\":1,\"133\":1}}],[\"相當重要的問題\",{\"1\":{\"3\":1}}],[\"兩者差異只在於使用的\",{\"1\":{\"196\":1}}],[\"兩者去作出判斷\",{\"1\":{\"151\":1}}],[\"兩部分影響程度的參數\",{\"1\":{\"171\":1}}],[\"兩種方法\",{\"1\":{\"133\":1}}],[\"兩個可以同時處理\",{\"1\":{\"192\":1}}],[\"兩個參數的\",{\"1\":{\"172\":1}}],[\"兩個部分\",{\"1\":{\"91\":1,\"149\":1,\"191\":1}}],[\"兩個模型都是使用\",{\"1\":{\"12\":1}}],[\"兩個\",{\"1\":{\"12\":1,\"66\":1}}],[\"兩篇\",{\"1\":{\"3\":1}}],[\"j=1nt​​\",{\"1\":{\"190\":2}}],[\"j=1ns​​\",{\"1\":{\"190\":2}}],[\"j​=p​0\",{\"1\":{\"177\":1}}],[\"j​=0\",{\"1\":{\"177\":1}}],[\"j​∼u\",{\"1\":{\"177\":2}}],[\"jw​ϵjb​​=f\",{\"1\":{\"175\":1}}],[\"juliani\",{\"1\":{\"172\":1}}],[\"jiang\",{\"1\":{\"161\":1}}],[\"jitter\",{\"1\":{\"90\":1}}],[\"j−obi\",{\"1\":{\"151\":1}}],[\"jmlr\",{\"1\":{\"123\":1},\"2\":{\"142\":1}}],[\"john\",{\"1\":{\"19\":1}}],[\"j\",{\"1\":{\"12\":5,\"13\":1,\"19\":1,\"90\":8,\"92\":1,\"93\":13,\"129\":1,\"151\":3}}],[\"處理預測階段的\",{\"1\":{\"159\":1}}],[\"處理成相同\",{\"1\":{\"91\":1}}],[\"處理\",{\"1\":{\"12\":1,\"64\":1,\"93\":1}}],[\"用\",{\"1\":{\"191\":1}}],[\"用量普遍很高\",{\"1\":{\"145\":1}}],[\"用來調整兩個\",{\"1\":{\"172\":1}}],[\"用來調整兩種\",{\"1\":{\"6\":1}}],[\"用來加上\",{\"1\":{\"167\":1}}],[\"用來描述裁切的\",{\"1\":{\"150\":1}}],[\"用來比較\",{\"1\":{\"91\":1}}],[\"用來表示一個\",{\"1\":{\"14\":2}}],[\"用兩個\",{\"1\":{\"12\":1}}],[\"首先把\",{\"1\":{\"179\":1}}],[\"首先將圖片經過裁切出一部分\",{\"1\":{\"150\":1}}],[\"首先說明這篇\",{\"1\":{\"129\":1}}],[\"首先看到上面的表格\",{\"1\":{\"97\":1}}],[\"首先\",{\"1\":{\"91\":1}}],[\"首先從256×256的圖像中隨機提取224×224的patches\",{\"1\":{\"47\":1}}],[\"首先針對\",{\"1\":{\"12\":1}}],[\"首先定義從\",{\"1\":{\"8\":1}}],[\"問題的解法是把所有可能的\",{\"1\":{\"124\":1}}],[\"問題在於不同的\",{\"1\":{\"64\":1}}],[\"問題\",{\"1\":{\"10\":1,\"14\":1,\"191\":1}}],[\"問題描述\",{\"0\":{\"3\":1,\"33\":1,\"64\":1,\"87\":1,\"108\":1,\"124\":1,\"145\":1,\"166\":1,\"187\":1},\"1\":{\"87\":1}}],[\"但結果不一定正確\",{\"1\":{\"187\":1}}],[\"但比起\",{\"1\":{\"187\":1}}],[\"但卻還是有\",{\"1\":{\"158\":1}}],[\"但卻隨著訓練過程慢慢地變糟\",{\"1\":{\"93\":1}}],[\"但他只是希望把多的地方設為\",{\"1\":{\"151\":1}}],[\"但他也大幅降低了overfitting的發生\",{\"1\":{\"49\":1}}],[\"但我們都會認為那就是人行道\",{\"1\":{\"145\":1}}],[\"但又會逐漸趨緩\",{\"1\":{\"138\":1}}],[\"但隨著資料量越來越大\",{\"1\":{\"138\":1}}],[\"但因為結論都是可以看到\",{\"1\":{\"135\":1}}],[\"但有包含了部分的學習過程\",{\"1\":{\"118\":1}}],[\"但數值範圍往往很\",{\"1\":{\"108\":1}}],[\"但數學有點太難\",{\"1\":{\"8\":1}}],[\"但透過剪貼則可以造成不同環境的突兀感\",{\"1\":{\"68\":1}}],[\"但在\",{\"1\":{\"108\":1}}],[\"但在邊界上往往還是難以有好的結果\",{\"1\":{\"67\":1}}],[\"但在alexnet中使用overlapping\",{\"1\":{\"44\":1}}],[\"但主要的問題來自於\",{\"1\":{\"67\":1}}],[\"但以當時來說\",{\"1\":{\"56\":1}}],[\"但還是可以看出alexnet遠比第二名有著更好的準確率\",{\"1\":{\"52\":1}}],[\"但作者說是heuristic\",{\"1\":{\"50\":1}}],[\"但alexnet具有6000萬個parameters\",{\"1\":{\"45\":1}}],[\"但如果是為了平行運算的效能優化就不一定了xd\",{\"1\":{\"42\":1}}],[\"但並不\",{\"1\":{\"25\":1}}],[\"但最後能取得更好的\",{\"1\":{\"23\":1}}],[\"但整體來說兩者都能在最後趨近於\",{\"1\":{\"22\":1}}],[\"但是從頭到尾都沒丟給\",{\"1\":{\"198\":1}}],[\"但是從擷取出的特徵當中可以很明顯的看出搭配\",{\"1\":{\"136\":1}}],[\"但是實作時卻使用\",{\"1\":{\"145\":1}}],[\"但是那些事情還是要處理\",{\"1\":{\"136\":1}}],[\"但是這些架構在\",{\"1\":{\"87\":1}}],[\"但是這種做法實際上效果很糟糕\",{\"1\":{\"70\":1}}],[\"但是並不全面\",{\"1\":{\"67\":1}}],[\"但是對於真實世界\",{\"1\":{\"63\":1}}],[\"但是在訓練上的效果看起來還不賴\",{\"1\":{\"136\":1}}],[\"但是在\",{\"1\":{\"25\":1,\"101\":1,\"117\":1}}],[\"但是\",{\"1\":{\"13\":1}}],[\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的\",{\"1\":{\"13\":1}}],[\"但是卻跟其他人有同樣的影響力\",{\"1\":{\"10\":1}}],[\"但\",{\"1\":{\"12\":1,\"78\":1}}],[\"每\",{\"1\":{\"179\":2}}],[\"每年舉辦的ilsvrc會使用imagenet一個subset\",{\"1\":{\"39\":1}}],[\"每一次的iteration都會重新隨機選擇一部份的neuron來dropout\",{\"1\":{\"37\":1}}],[\"每一個\",{\"1\":{\"13\":1}}],[\"每個人也都還是能夠有一定的能力去解決\",{\"1\":{\"136\":1}}],[\"每個遊戲的\",{\"1\":{\"116\":1}}],[\"每個s\",{\"1\":{\"44\":1}}],[\"每個\",{\"1\":{\"12\":1,\"13\":2,\"16\":1,\"19\":1,\"128\":1,\"129\":1,\"192\":1,\"195\":1}}],[\"每種\",{\"1\":{\"10\":1,\"13\":1,\"16\":1}}],[\"每忽略一個\",{\"1\":{\"3\":1}}],[\"難以收斂的問題\",{\"1\":{\"12\":1,\"27\":1}}],[\"難以收斂\",{\"1\":{\"10\":1}}],[\"實驗是做在\",{\"1\":{\"179\":1}}],[\"實驗上調整了\",{\"1\":{\"116\":1}}],[\"實驗做在\",{\"1\":{\"116\":1}}],[\"實驗設定\",{\"0\":{\"73\":1,\"96\":1,\"116\":1,\"154\":1,\"200\":1}}],[\"實現了一種側抑制的形式\",{\"1\":{\"43\":1}}],[\"實作上採用了常見的\",{\"1\":{\"96\":1}}],[\"實作上\",{\"1\":{\"10\":1,\"114\":1}}],[\"實際上他所謂的\",{\"1\":{\"198\":1}}],[\"實際上\",{\"1\":{\"196\":1}}],[\"實際上對於\",{\"1\":{\"195\":1}}],[\"實際上跟\",{\"1\":{\"171\":1}}],[\"實際上包含了\",{\"1\":{\"91\":1}}],[\"實際上是有幫助的\",{\"1\":{\"24\":1}}],[\"實際上還會為了讓\",{\"1\":{\"8\":1}}],[\"實際上訓練的\",{\"1\":{\"8\":1}}],[\"丟在\",{\"1\":{\"9\":1}}],[\"有不少的提升\",{\"1\":{\"202\":1}}],[\"有不少人最後給的結果之所以那麼好看是因為\",{\"1\":{\"80\":1}}],[\"有多好多壞\",{\"1\":{\"171\":1}}],[\"有多少影響呢\",{\"1\":{\"23\":1}}],[\"有更好的效果\",{\"1\":{\"170\":1}}],[\"有更好的處理\",{\"1\":{\"155\":1}}],[\"有更多的\",{\"1\":{\"168\":1}}],[\"有提及一個使用\",{\"1\":{\"166\":1}}],[\"有正向的幫助\",{\"1\":{\"148\":1}}],[\"有機率\",{\"1\":{\"128\":1}}],[\"有效地避免\",{\"1\":{\"124\":1}}],[\"有性生殖能夠讓子代有部分雙親的優勢\",{\"1\":{\"124\":1}}],[\"有大的影響\",{\"1\":{\"117\":1}}],[\"有所提升\",{\"1\":{\"98\":1}}],[\"有較高的機會被\",{\"1\":{\"92\":1}}],[\"有較大的影響\",{\"1\":{\"22\":1}}],[\"有另一個有趣的好處是\",{\"1\":{\"91\":1}}],[\"有許多相像的地方\",{\"1\":{\"91\":1}}],[\"有許多新的架構可以得到更高的\",{\"1\":{\"87\":1}}],[\"有許多的\",{\"1\":{\"9\":1}}],[\"有兩列分別表示\",{\"1\":{\"79\":1}}],[\"有點偏以及\",{\"1\":{\"78\":1}}],[\"有還不錯的成效\",{\"1\":{\"114\":1}}],[\"有還不錯的\",{\"1\":{\"78\":1}}],[\"有一些常見的\",{\"1\":{\"74\":1}}],[\"有可能就被誤判成人行道\",{\"1\":{\"67\":1}}],[\"有部分的認知\",{\"1\":{\"67\":1}}],[\"有相當大的差異\",{\"1\":{\"66\":1}}],[\"有最大的\",{\"1\":{\"25\":1}}],[\"有最好的結果\",{\"1\":{\"25\":1}}],[\"有了評斷信心水平的標準\",{\"1\":{\"90\":1}}],[\"有了\",{\"1\":{\"24\":1}}],[\"有了目標\",{\"1\":{\"8\":1}}],[\"有人天生愛保險\",{\"1\":{\"13\":1}}],[\"有人天生愛探險\",{\"1\":{\"13\":1}}],[\"有些甚至是遞增的\",{\"1\":{\"180\":1}}],[\"有些是\",{\"1\":{\"79\":1}}],[\"有些傾向\",{\"1\":{\"13\":2}}],[\"有些則不需要\",{\"1\":{\"10\":1}}],[\"有些環境需要更多的\",{\"1\":{\"10\":1}}],[\"有些\",{\"1\":{\"10\":1,\"79\":1}}],[\"有時會很不穩定\",{\"1\":{\"10\":1}}],[\"只不過是把\",{\"1\":{\"198\":1}}],[\"只不過輸入上會丟\",{\"1\":{\"9\":1}}],[\"只要我們的\",{\"1\":{\"196\":1}}],[\"只要正確的\",{\"1\":{\"133\":1}}],[\"只在\",{\"1\":{\"192\":1}}],[\"只選擇信度高於某個閥值的預測作為\",{\"1\":{\"187\":1}}],[\"只需要產出\",{\"1\":{\"175\":1}}],[\"只需要一張\",{\"1\":{\"102\":1}}],[\"只會每經過\",{\"1\":{\"116\":1}}],[\"只會取出最後\",{\"1\":{\"114\":1}}],[\"只會儲存最後\",{\"1\":{\"114\":1}}],[\"只會被提取一次\",{\"1\":{\"48\":1}}],[\"只是調整\",{\"1\":{\"171\":1}}],[\"只是類別增加到\",{\"1\":{\"133\":1}}],[\"只是為了\",{\"1\":{\"93\":1}}],[\"只是單純符合條件給\",{\"1\":{\"90\":1,\"148\":1}}],[\"只是用來限制\",{\"1\":{\"6\":1}}],[\"只對簡單的\",{\"1\":{\"78\":1}}],[\"只有\",{\"1\":{\"67\":1,\"151\":1}}],[\"只訓練在\",{\"1\":{\"64\":1,\"91\":1}}],[\"只使用一個gpu是放不下用120萬個training\",{\"1\":{\"42\":1}}],[\"只用了一個\",{\"1\":{\"10\":1}}],[\"∣st+i​\",{\"1\":{\"177\":1}}],[\"∣st​\",{\"1\":{\"7\":1}}],[\"∣x∣​\",{\"1\":{\"175\":1}}],[\"∣z∣+1+ϵ\",{\"1\":{\"8\":1}}],[\"∣z∣+1​−1\",{\"1\":{\"8\":1}}],[\"zt\",{\"1\":{\"196\":3}}],[\"zt​∥zt\",{\"1\":{\"196\":1}}],[\"zt​\",{\"1\":{\"196\":2}}],[\"zero\",{\"1\":{\"174\":2}}],[\"ziyu\",{\"1\":{\"169\":1,\"171\":1}}],[\"zi\",{\"1\":{\"129\":4}}],[\"zurich\",{\"1\":{\"86\":1,\"144\":1}}],[\"zou\",{\"1\":{\"67\":1}}],[\"zhang1\",{\"1\":{\"186\":1}}],[\"zhang\",{\"1\":{\"64\":1,\"186\":2,\"187\":3,\"192\":3,\"201\":3}}],[\"z=2\",{\"1\":{\"44\":1}}],[\"z=3\",{\"1\":{\"44\":1}}],[\"z×z的大小\",{\"1\":{\"44\":1}}],[\"z則代表每次做pooling的大小\",{\"1\":{\"44\":1}}],[\"z\",{\"1\":{\"8\":4,\"129\":1}}],[\"∀z∈r\",{\"1\":{\"8\":2}}],[\"+lcet​\",{\"1\":{\"197\":1}}],[\"+\",{\"1\":{\"194\":1}}],[\"+p1​​\",{\"1\":{\"177\":1}}],[\"+p3​​\",{\"1\":{\"177\":1}}],[\"+​a\",{\"1\":{\"171\":1}}],[\"+a\",{\"1\":{\"171\":1}}],[\"+λlv\",{\"1\":{\"172\":1}}],[\"+λd​lce​\",{\"1\":{\"151\":2}}],[\"+λh\",{\"1\":{\"71\":1}}],[\"+ζ\",{\"1\":{\"151\":1}}],[\"+bi\",{\"1\":{\"129\":2}}],[\"+0\",{\"1\":{\"80\":1}}],[\"+1\",{\"1\":{\"80\":2}}],[\"+2\",{\"1\":{\"80\":1}}],[\"+βkl\",{\"1\":{\"197\":1}}],[\"+βlce​\",{\"1\":{\"195\":1}}],[\"+βi=0∑k​∇θπ​​h\",{\"1\":{\"172\":1,\"177\":1}}],[\"+βnk−1​\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"+βj​q\",{\"1\":{\"12\":1}}],[\"+ϵz=sgn\",{\"1\":{\"8\":1}}],[\"+t≥0∑​γt\",{\"1\":{\"8\":1}}],[\"+s=t∑t+k−1​γs−t\",{\"1\":{\"8\":1}}],[\"x+\",{\"1\":{\"174\":1}}],[\"xu\",{\"1\":{\"161\":1}}],[\"xd​=xc\",{\"1\":{\"150\":1}}],[\"xd​\",{\"1\":{\"150\":2}}],[\"xc​=ζ\",{\"1\":{\"150\":1}}],[\"xc​\",{\"1\":{\"150\":3,\"151\":1}}],[\"xc\",{\"1\":{\"150\":3}}],[\"xlrt​\",{\"1\":{\"148\":1}}],[\"xlrt​=ζ\",{\"1\":{\"148\":1}}],[\"xlrs​\",{\"1\":{\"148\":1}}],[\"xhrt​\",{\"1\":{\"148\":1}}],[\"xhrt\",{\"1\":{\"148\":2}}],[\"xhrs\",{\"1\":{\"148\":2}}],[\"xm​\",{\"1\":{\"71\":4}}],[\"xs=\",{\"1\":{\"148\":1}}],[\"xs\",{\"1\":{\"90\":2,\"93\":3}}],[\"xs​=\",{\"1\":{\"90\":1,\"190\":1}}],[\"xs​\",{\"1\":{\"71\":5,\"190\":1,\"191\":1}}],[\"xsb​\",{\"1\":{\"12\":3}}],[\"xa​\",{\"1\":{\"68\":2}}],[\"x\",{\"1\":{\"8\":2,\"12\":6,\"22\":1,\"41\":5,\"43\":1,\"175\":2}}],[\"xt=\",{\"1\":{\"148\":1}}],[\"xt\",{\"1\":{\"90\":4,\"191\":1}}],[\"xt+1​\",{\"1\":{\"8\":3,\"19\":1}}],[\"xt​=\",{\"1\":{\"90\":1,\"190\":1}}],[\"xt​\",{\"1\":{\"8\":8,\"19\":2,\"71\":3,\"190\":1,\"191\":1,\"193\":4,\"194\":1,\"196\":8}}],[\"版本\",{\"1\":{\"8\":1}}],[\"改成透過\",{\"1\":{\"195\":1}}],[\"改成\",{\"1\":{\"8\":1}}],[\"上同樣可以獲得更多改善\",{\"1\":{\"203\":1}}],[\"上半部分是用\",{\"1\":{\"201\":1}}],[\"上也能好好地區分不同的\",{\"1\":{\"196\":1}}],[\"上也許我們能夠對各種物件去做標記\",{\"1\":{\"63\":1}}],[\"上產生比較雜亂的特徵\",{\"1\":{\"187\":1}}],[\"上適用\",{\"1\":{\"181\":1}}],[\"上較為顯著\",{\"1\":{\"179\":1}}],[\"上加上\",{\"1\":{\"168\":1}}],[\"上加\",{\"1\":{\"168\":1}}],[\"上增加\",{\"1\":{\"168\":2}}],[\"上增加了\",{\"1\":{\"168\":1}}],[\"上鼓勵\",{\"1\":{\"168\":1}}],[\"上可以得到較好的結果\",{\"1\":{\"101\":1}}],[\"上圖就是在最後分開成兩個輸出結果\",{\"1\":{\"171\":1}}],[\"上圖展現出\",{\"1\":{\"99\":1}}],[\"上圖中可以看到在gpu1中的kernel\",{\"1\":{\"53\":1}}],[\"上多加上一項去\",{\"1\":{\"93\":1}}],[\"上述的三者分數都是以\",{\"1\":{\"91\":1}}],[\"上述的作法其實捕捉了自然圖像的一個重要性質\",{\"1\":{\"48\":1}}],[\"上會發生\",{\"1\":{\"70\":1}}],[\"上訓練\",{\"1\":{\"176\":1,\"177\":1}}],[\"上訓練一個模型\",{\"1\":{\"67\":1}}],[\"上訓練的模型難以直接\",{\"1\":{\"63\":1}}],[\"上\",{\"1\":{\"63\":1,\"68\":2,\"82\":1,\"87\":1,\"93\":1,\"96\":1,\"101\":2,\"108\":1,\"136\":1,\"175\":1,\"179\":1}}],[\"上表為alexnet參加ilsvrc\",{\"1\":{\"52\":1}}],[\"上面的表現\",{\"1\":{\"200\":1}}],[\"上面的差異就是這裡傳入的分別是\",{\"1\":{\"12\":1}}],[\"上面基本的做法作者稱他為\",{\"1\":{\"175\":1}}],[\"上面提及的是單純的\",{\"1\":{\"8\":1}}],[\"上的簡短說明當中給出的架構圖也許會有更加直觀的理解\",{\"1\":{\"149\":1}}],[\"上的文章\",{\"1\":{\"135\":1}}],[\"上的重要性\",{\"1\":{\"87\":1}}],[\"上的認知\",{\"1\":{\"67\":1}}],[\"上的時候\",{\"1\":{\"64\":1}}],[\"上的例子\",{\"1\":{\"63\":1}}],[\"上的\",{\"1\":{\"8\":1,\"66\":1,\"117\":1}}],[\"−∥f\",{\"1\":{\"196\":2}}],[\"−∥f~​\",{\"1\":{\"193\":2,\"196\":2}}],[\"−η\",{\"1\":{\"193\":2,\"196\":4}}],[\"−scorerandom​scoreagent​−scorebaseline​​\",{\"1\":{\"179\":1}}],[\"−p1​​\",{\"1\":{\"177\":1}}],[\"−p3​​\",{\"1\":{\"177\":1}}],[\"−∣a∣1​a\",{\"1\":{\"171\":1}}],[\"−v\",{\"1\":{\"171\":1}}],[\"−trt\",{\"1\":{\"110\":1}}],[\"−t^r\",{\"1\":{\"12\":1}}],[\"−fθ​\",{\"1\":{\"93\":1}}],[\"−1等saturating\",{\"1\":{\"41\":1}}],[\"−1\",{\"1\":{\"8\":1,\"116\":1}}],[\"−h−1q\",{\"1\":{\"8\":1}}],[\"−yt​^​\",{\"1\":{\"8\":1}}],[\"−q\",{\"1\":{\"8\":1,\"169\":1,\"171\":1,\"176\":4}}],[\"θ=defμ+σ⊙ϵ\",{\"1\":{\"174\":1,\"180\":1}}],[\"θv\",{\"1\":{\"172\":3}}],[\"θv​\",{\"1\":{\"171\":1,\"172\":5,\"177\":3}}],[\"θπ​\",{\"1\":{\"172\":5,\"177\":3}}],[\"θa​\",{\"1\":{\"171\":1}}],[\"θ~=θ+n\",{\"1\":{\"168\":1}}],[\"θt​\",{\"1\":{\"90\":1}}],[\"θl​\",{\"1\":{\"19\":1}}],[\"θi−1​\",{\"1\":{\"110\":2}}],[\"θi​\",{\"1\":{\"110\":3}}],[\"θi\",{\"1\":{\"12\":2}}],[\"θe∪θi\",{\"1\":{\"12\":1}}],[\"θe\",{\"1\":{\"12\":2}}],[\"θ\",{\"1\":{\"8\":2,\"12\":6,\"71\":1,\"169\":2,\"170\":2,\"171\":8,\"172\":3,\"174\":3,\"176\":5}}],[\"θ−\",{\"1\":{\"8\":2,\"12\":3,\"169\":2,\"170\":3,\"171\":1,\"176\":2}}],[\"一次\",{\"1\":{\"179\":1}}],[\"一如既往\",{\"1\":{\"154\":1}}],[\"一塊\",{\"1\":{\"150\":1}}],[\"一\",{\"1\":{\"140\":1}}],[\"一開始都不太相同\",{\"1\":{\"116\":1}}],[\"一開始都是一樣的\",{\"1\":{\"13\":1}}],[\"一開始我們一樣先看一下這一篇論文當中會用到的\",{\"1\":{\"90\":1}}],[\"一起\",{\"1\":{\"71\":1}}],[\"一個假的\",{\"1\":{\"187\":1}}],[\"一個理想上必定能夠避免\",{\"1\":{\"124\":1}}],[\"一個\",{\"1\":{\"111\":1}}],[\"一個常見的問題是產出的結果通常會傾向去預測結果為常見的\",{\"1\":{\"67\":1}}],[\"一個簡單的方法是想辦法給這些\",{\"1\":{\"67\":1}}],[\"一個gpu就放得下了\",{\"1\":{\"42\":1}}],[\"一個是\",{\"1\":{\"8\":2}}],[\"一般的模型通常會使用像f\",{\"1\":{\"41\":1}}],[\"一些\",{\"1\":{\"19\":1,\"67\":1}}],[\"一樣好\",{\"1\":{\"179\":1}}],[\"一樣糟\",{\"1\":{\"179\":1}}],[\"一樣\",{\"1\":{\"8\":1,\"73\":1}}],[\"​exp\",{\"1\":{\"193\":1,\"196\":2}}],[\"​otherwise​\",{\"1\":{\"191\":1}}],[\"​if\",{\"1\":{\"191\":1}}],[\"​log\",{\"1\":{\"191\":1}}],[\"​logπ\",{\"1\":{\"172\":1}}],[\"​loggθ​\",{\"1\":{\"90\":2}}],[\"​∈rohc​​×owc​​\",{\"1\":{\"151\":1}}],[\"​bd\",{\"1\":{\"150\":1}}],[\"​bc\",{\"1\":{\"150\":1}}],[\"​gϕ​\",{\"1\":{\"148\":1}}],[\"​c=1∑c​qij​yijc​logζ\",{\"1\":{\"148\":1}}],[\"​j=1∑w\",{\"1\":{\"148\":1}}],[\"​∼bernoulli\",{\"1\":{\"129\":1}}],[\"​y~​\",{\"1\":{\"129\":2}}],[\"​y\",{\"1\":{\"129\":1}}],[\"​yi\",{\"1\":{\"129\":2}}],[\"​∑j=1hf​×wf​​d\",{\"1\":{\"93\":1}}],[\"​⋅\",{\"1\":{\"93\":1}}],[\"​pt\",{\"1\":{\"90\":1,\"191\":1,\"192\":1,\"195\":1}}],[\"​hϕ​\",{\"1\":{\"90\":2}}],[\"​α1​λ1​α2​λ2​α3​λ​​\",{\"1\":{\"48\":1}}],[\"​​=wi\",{\"1\":{\"129\":2}}],[\"​​\",{\"1\":{\"14\":1,\"15\":1,\"93\":1,\"172\":2}}],[\"​​​∀0≤k≤n−1∀n≤k≤k−1​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​==1\",{\"1\":{\"194\":2}}],[\"​=∑k\",{\"1\":{\"193\":1,\"196\":2}}],[\"​=∑j​mthings\",{\"1\":{\"93\":1}}],[\"​=ξ\",{\"1\":{\"192\":1,\"195\":1}}],[\"​=−e\",{\"1\":{\"177\":1}}],[\"​=−eπ\",{\"1\":{\"172\":1,\"177\":1}}],[\"​=−j=1∑h×w​c=1∑c​qt\",{\"1\":{\"90\":1}}],[\"​=−j=1∑h×w​c=1∑c​ys\",{\"1\":{\"90\":1}}],[\"​=lce​\",{\"1\":{\"148\":1}}],[\"​=pw\",{\"1\":{\"129\":1}}],[\"​=f\",{\"1\":{\"129\":2}}],[\"​=c\",{\"1\":{\"93\":1}}],[\"​=h⋅w∑j=1h×w​\",{\"1\":{\"90\":1}}],[\"​=\",{\"1\":{\"90\":1,\"93\":1,\"191\":1}}],[\"​=m=max\",{\"1\":{\"15\":1}}],[\"​=m=0∑k−1​1\",{\"1\":{\"14\":1}}],[\"​=nk​\",{\"1\":{\"14\":1,\"15\":1}}],[\"​=sgn\",{\"1\":{\"8\":1}}],[\"​−1​\",{\"1\":{\"8\":1}}],[\"​\",{\"1\":{\"8\":3,\"90\":11,\"92\":3,\"93\":5,\"110\":3,\"129\":5,\"148\":2,\"151\":5,\"171\":1,\"172\":3,\"175\":1,\"177\":1,\"191\":4,\"192\":5,\"193\":1,\"194\":1,\"195\":1,\"196\":5,\"197\":2}}],[\"δth​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δth​\",{\"1\":{\"8\":1}}],[\"δt​cs​​=rt​+γa∈a∑​π\",{\"1\":{\"8\":1}}],[\"δs​\",{\"1\":{\"8\":1}}],[\"=∑xt​∈xt​​∑i​1\",{\"1\":{\"194\":1}}],[\"=∑c\",{\"1\":{\"92\":1}}],[\"=sgn\",{\"1\":{\"175\":1}}],[\"=f\",{\"1\":{\"175\":1}}],[\"=∇e\",{\"1\":{\"174\":1}}],[\"=lπ\",{\"1\":{\"172\":1}}],[\"=i=0∑k​eπ\",{\"1\":{\"172\":1,\"177\":1}}],[\"=v\",{\"1\":{\"171\":2}}],[\"=−i=1∑h\",{\"1\":{\"148\":1}}],[\"=r\",{\"1\":{\"129\":1}}],[\"=x\",{\"1\":{\"129\":1}}],[\"=t∑t​γt\",{\"1\":{\"110\":1}}],[\"=tanh\",{\"1\":{\"41\":1}}],[\"=πmax​e\",{\"1\":{\"110\":1}}],[\"=1∑c​ys\",{\"1\":{\"93\":1}}],[\"=1c​e1−fc\",{\"1\":{\"92\":1}}],[\"=∥fimagenet​\",{\"1\":{\"93\":1}}],[\"=max\",{\"1\":{\"41\":1}}],[\"=b=0∑b−1​s=t∑t+h−1​\",{\"1\":{\"12\":1}}],[\"=arga∈amax​q\",{\"1\":{\"12\":1}}],[\"=\",{\"1\":{\"8\":1,\"41\":1,\"90\":1,\"148\":1,\"151\":2}}],[\"=λmin\",{\"1\":{\"8\":1}}],[\"=q\",{\"1\":{\"8\":1,\"12\":1,\"171\":1}}],[\"=es\",{\"1\":{\"110\":1}}],[\"=eμ​\",{\"1\":{\"8\":1}}],[\"=e\",{\"1\":{\"7\":1,\"71\":1,\"169\":1,\"171\":1,\"174\":2,\"176\":4,\"177\":1}}],[\"定義底下的平均\",{\"1\":{\"180\":1}}],[\"定義一個\",{\"1\":{\"92\":1}}],[\"定義\",{\"1\":{\"8\":1,\"93\":2,\"110\":1}}],[\"τ∈n∗\",{\"1\":{\"15\":1}}],[\"τ=\",{\"1\":{\"8\":1}}],[\"τ\",{\"1\":{\"8\":1,\"15\":8,\"16\":2,\"90\":1,\"193\":3,\"196\":4}}],[\"π​q\",{\"1\":{\"12\":1}}],[\"π\",{\"1\":{\"8\":2,\"12\":4,\"110\":2,\"172\":3,\"177\":3}}],[\"μi\",{\"1\":{\"177\":2}}],[\"μ+σ⊙ξ\",{\"1\":{\"174\":1}}],[\"μ+σ⊙ϵ\",{\"1\":{\"174\":1}}],[\"μb+σb⊙ϵb\",{\"1\":{\"174\":1}}],[\"μw+σw⊙ϵw\",{\"1\":{\"174\":1}}],[\"μ^​k​\",{\"1\":{\"14\":2,\"15\":2}}],[\"μ\",{\"1\":{\"8\":3,\"12\":3,\"174\":1,\"180\":1}}],[\"演算法\",{\"1\":{\"8\":1}}],[\"計算方式如下\",{\"1\":{\"197\":1}}],[\"計算上負擔過大\",{\"1\":{\"194\":1}}],[\"計算的是\",{\"1\":{\"193\":1}}],[\"計算分別如下\",{\"1\":{\"172\":1}}],[\"計算機視覺\",{\"1\":{\"33\":1}}],[\"計算\",{\"0\":{\"194\":1,\"195\":1},\"1\":{\"8\":1,\"19\":1}}],[\"那就直接把原圖交給\",{\"1\":{\"152\":1}}],[\"那就會在辨識上出現問題\",{\"1\":{\"145\":1}}],[\"那就必然會考慮到所有的面向\",{\"1\":{\"124\":1}}],[\"那我們就可以用\",{\"1\":{\"67\":1}}],[\"那也就會有\",{\"1\":{\"8\":1}}],[\"那麼這裡加上的\",{\"1\":{\"180\":1}}],[\"那麼就會有更高的機會在\",{\"1\":{\"87\":1}}],[\"那麼\",{\"1\":{\"3\":1,\"175\":1,\"196\":1}}],[\"選擇一個\",{\"1\":{\"192\":1}}],[\"選擇採用\",{\"1\":{\"73\":1}}],[\"選擇中最大的\",{\"1\":{\"24\":1}}],[\"選擇出一組\",{\"1\":{\"19\":1}}],[\"選擇出現傾向\",{\"1\":{\"13\":1}}],[\"選擇其中最大的當成這次的選擇\",{\"1\":{\"14\":1}}],[\"選擇\",{\"1\":{\"13\":1,\"19\":1}}],[\"選擇的分布\",{\"1\":{\"7\":1}}],[\"選大一些\",{\"1\":{\"7\":1}}],[\"選小一些\",{\"1\":{\"7\":1}}],[\"需要\",{\"1\":{\"172\":1}}],[\"需要額外的\",{\"1\":{\"172\":1}}],[\"需要特別注意到對於\",{\"1\":{\"110\":1}}],[\"需要透過與環境互動取得\",{\"1\":{\"108\":1}}],[\"需要看遠一些\",{\"1\":{\"7\":1}}],[\"需要相當大量的探索之後才能得到\",{\"1\":{\"3\":1}}],[\"小物件的預測更加精確\",{\"1\":{\"156\":1}}],[\"小\",{\"1\":{\"7\":1,\"15\":1}}],[\"傾向在\",{\"1\":{\"187\":1}}],[\"傾向\",{\"1\":{\"7\":2}}],[\"大物件則有降低的趨勢\",{\"1\":{\"156\":1}}],[\"大概念上\",{\"1\":{\"150\":1}}],[\"大致上也會是比較好的選擇點\",{\"1\":{\"137\":1}}],[\"大多都是大學部ml\",{\"1\":{\"57\":1}}],[\"大幅降低overfitting發生的可能\",{\"1\":{\"37\":1}}],[\"大小\",{\"1\":{\"13\":1}}],[\"大小為\",{\"1\":{\"12\":1}}],[\"大\",{\"1\":{\"7\":1}}],[\"γ2​=0\",{\"1\":{\"196\":1}}],[\"γ1​=10\",{\"1\":{\"196\":1}}],[\"γ=0\",{\"1\":{\"24\":1}}],[\"γj​\",{\"1\":{\"13\":1,\"19\":1}}],[\"γ\",{\"1\":{\"7\":4,\"16\":1,\"21\":1,\"24\":1,\"110\":1}}],[\"qi​\",{\"1\":{\"172\":1}}],[\"qi​−v\",{\"1\":{\"172\":1,\"177\":2}}],[\"qdt​\",{\"1\":{\"151\":1}}],[\"qc\",{\"1\":{\"151\":1}}],[\"qlrt​\",{\"1\":{\"148\":2}}],[\"q∗\",{\"1\":{\"110\":4,\"169\":2,\"171\":2}}],[\"qt\",{\"1\":{\"90\":2}}],[\"quality\",{\"1\":{\"151\":1}}],[\"qualitative\",{\"0\":{\"53\":1}}],[\"quantitative\",{\"0\":{\"52\":1}}],[\"quan\",{\"1\":{\"19\":1}}],[\"qπ\",{\"1\":{\"8\":1}}],[\"q\",{\"0\":{\"110\":1},\"1\":{\"7\":2,\"8\":6,\"12\":3,\"19\":1,\"107\":1,\"109\":3,\"110\":2,\"112\":2,\"114\":2,\"116\":1,\"117\":1,\"148\":1,\"169\":1,\"171\":4}}],[\"去計算\",{\"1\":{\"196\":1}}],[\"去計算出\",{\"1\":{\"148\":1}}],[\"去調整\",{\"1\":{\"195\":1}}],[\"去調整選擇不同\",{\"1\":{\"13\":1}}],[\"去決定要加怎樣的\",{\"1\":{\"174\":1}}],[\"去加上\",{\"1\":{\"174\":1}}],[\"去儲存\",{\"1\":{\"172\":1}}],[\"去避免訓練資料上的強關聯性\",{\"1\":{\"172\":1}}],[\"去試圖得到\",{\"1\":{\"169\":1}}],[\"去增加\",{\"1\":{\"166\":1}}],[\"去看過整體的圖片\",{\"1\":{\"152\":1}}],[\"去看最終的效能是不是確實有提升\",{\"1\":{\"132\":1}}],[\"去幫助訓練\",{\"1\":{\"148\":1}}],[\"去當\",{\"1\":{\"133\":1}}],[\"去比較\",{\"1\":{\"118\":1}}],[\"去選擇\",{\"1\":{\"114\":1}}],[\"去學\",{\"1\":{\"112\":1}}],[\"去學習\",{\"1\":{\"13\":1,\"22\":1,\"90\":1,\"114\":1,\"148\":1,\"169\":1,\"171\":1}}],[\"去學習導致\",{\"1\":{\"10\":1}}],[\"去預測\",{\"1\":{\"111\":1,\"151\":1,\"191\":1}}],[\"去提升\",{\"1\":{\"99\":1}}],[\"去初始化權重\",{\"1\":{\"93\":1}}],[\"去定義如下\",{\"1\":{\"92\":1}}],[\"去處理即可\",{\"1\":{\"152\":1}}],[\"去處理\",{\"1\":{\"91\":1}}],[\"去評估取得\",{\"1\":{\"91\":1}}],[\"去更新\",{\"1\":{\"90\":1,\"148\":1}}],[\"去更新參數學習\",{\"1\":{\"9\":1}}],[\"去訓練基本上不會得到太好的結果\",{\"1\":{\"148\":1}}],[\"去訓練\",{\"1\":{\"87\":1,\"113\":1,\"148\":1,\"174\":1}}],[\"去訓練顯然很糟糕\",{\"1\":{\"78\":1,\"79\":1}}],[\"去\",{\"1\":{\"71\":1,\"78\":1,\"110\":1,\"148\":1}}],[\"去轉移出來\",{\"1\":{\"67\":1}}],[\"去判別現在給我的究竟是\",{\"1\":{\"66\":1}}],[\"去拉近\",{\"1\":{\"66\":1}}],[\"去紀錄訓練過程當中的\",{\"1\":{\"21\":1}}],[\"去跟環境互動\",{\"1\":{\"19\":1}}],[\"去得到目標\",{\"1\":{\"8\":1}}],[\"去逼近\",{\"1\":{\"8\":1}}],[\"去近似\",{\"1\":{\"7\":1,\"174\":1}}],[\"去找到藏在地圖當中的寶藏\",{\"1\":{\"3\":1}}],[\"f~​\",{\"1\":{\"193\":1,\"196\":1}}],[\"ft​\",{\"1\":{\"151\":3}}],[\"f​=ζ\",{\"1\":{\"151\":1}}],[\"fs​\",{\"1\":{\"151\":1}}],[\"fs\",{\"1\":{\"150\":1}}],[\"fusion\",{\"0\":{\"151\":1},\"1\":{\"160\":1}}],[\"fused\",{\"1\":{\"151\":2}}],[\"fuse\",{\"1\":{\"149\":1,\"151\":1}}],[\"fully\",{\"1\":{\"133\":1}}],[\"functiona\",{\"1\":{\"171\":1}}],[\"functions\",{\"1\":{\"41\":2,\"182\":1}}],[\"function將平坦化後的向量轉成機率分佈\",{\"1\":{\"35\":1}}],[\"function\",{\"0\":{\"12\":1,\"22\":1,\"35\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"34\":1,\"36\":2,\"58\":1,\"93\":2,\"109\":1,\"110\":1,\"111\":1,\"112\":1,\"114\":1,\"129\":1,\"148\":2,\"169\":3,\"171\":2,\"172\":4,\"177\":1,\"193\":1}}],[\"f\",{\"1\":{\"129\":1,\"175\":2,\"191\":1,\"193\":1,\"196\":1}}],[\"fitted\",{\"1\":{\"109\":1}}],[\"fine\",{\"1\":{\"67\":1,\"130\":1}}],[\"fc​=ns​⋅h⋅w∑i=1ns​​∑j=1h×w​\",{\"1\":{\"92\":1}}],[\"fc​\",{\"1\":{\"92\":1}}],[\"fd\",{\"0\":{\"93\":1,\"100\":1},\"1\":{\"91\":1,\"97\":2,\"100\":1}}],[\"fe\",{\"1\":{\"150\":3,\"151\":1}}],[\"fence\",{\"1\":{\"77\":1}}],[\"features\",{\"1\":{\"93\":3,\"118\":1,\"194\":1,\"196\":2}}],[\"feature\",{\"0\":{\"93\":1,\"100\":1},\"1\":{\"66\":1,\"91\":3,\"93\":3,\"148\":1,\"150\":1,\"191\":1,\"192\":1,\"193\":2,\"194\":2,\"196\":4}}],[\"fθ​\",{\"1\":{\"71\":3,\"148\":3}}],[\"fortunato\",{\"1\":{\"165\":1,\"179\":3,\"180\":1}}],[\"for\",{\"0\":{\"85\":1,\"90\":1,\"94\":1,\"164\":1,\"168\":1,\"185\":1},\"1\":{\"28\":1,\"39\":3,\"67\":1,\"82\":4,\"86\":1,\"91\":3,\"103\":2,\"144\":1,\"145\":1,\"148\":1,\"167\":1,\"174\":1,\"182\":2,\"203\":1,\"204\":3}}],[\"follow\",{\"1\":{\"8\":1,\"12\":1,\"110\":1}}],[\"fang\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"fa\",{\"1\":{\"151\":2}}],[\"factorised\",{\"1\":{\"175\":1,\"176\":1,\"177\":1}}],[\"factor\",{\"1\":{\"24\":1,\"110\":1}}],[\"family\",{\"0\":{\"13\":1}}],[\"free\",{\"1\":{\"111\":1,\"112\":1}}],[\"freeway\",{\"1\":{\"24\":1}}],[\"frame\",{\"1\":{\"116\":1}}],[\"framework\",{\"1\":{\"81\":1,\"96\":1,\"160\":1}}],[\"frames\",{\"1\":{\"19\":1,\"114\":1,\"116\":2,\"179\":2}}],[\"from\",{\"0\":{\"122\":1},\"1\":{\"3\":2,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"63\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2,\"70\":2,\"71\":2,\"75\":1,\"76\":1,\"77\":1,\"78\":2,\"79\":1,\"86\":1,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":3,\"101\":2,\"114\":1,\"117\":1,\"118\":1,\"126\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1,\"145\":1,\"148\":1,\"149\":2,\"150\":1,\"151\":1,\"152\":2,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2,\"168\":2,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"179\":3,\"180\":1,\"187\":1,\"188\":1,\"192\":1,\"201\":1}}],[\"會採用\",{\"1\":{\"197\":1}}],[\"會做為\",{\"1\":{\"197\":1}}],[\"會很容易忽略了細部的特徵\",{\"1\":{\"192\":1}}],[\"會想要讓\",{\"1\":{\"192\":1}}],[\"會太慢\",{\"1\":{\"192\":1}}],[\"會使模型被誤導\",{\"1\":{\"187\":1}}],[\"會使得\",{\"1\":{\"6\":1}}],[\"會\",{\"1\":{\"179\":1}}],[\"會比較大\",{\"1\":{\"176\":1}}],[\"會直接去學\",{\"1\":{\"172\":1}}],[\"會隱晦地在訓練過程中學習到\",{\"1\":{\"152\":1}}],[\"會限制輸出在\",{\"1\":{\"151\":1}}],[\"會對於\",{\"1\":{\"148\":1}}],[\"會需要儲存許多不同\",{\"1\":{\"145\":1}}],[\"會是比較好的選擇\",{\"1\":{\"137\":1}}],[\"會被其他的\",{\"1\":{\"136\":1}}],[\"會互相影響\",{\"1\":{\"136\":1}}],[\"會透過\",{\"1\":{\"114\":1}}],[\"會將\",{\"1\":{\"114\":1}}],[\"會有幾個明顯的問題\",{\"1\":{\"108\":1}}],[\"會有怎樣的影響\",{\"1\":{\"91\":1}}],[\"會有大量的下降\",{\"1\":{\"91\":1}}],[\"會有更好的\",{\"1\":{\"91\":1}}],[\"會有兩組總和\",{\"1\":{\"12\":1}}],[\"會有兩個\",{\"1\":{\"8\":1}}],[\"會期待預測出來的\",{\"1\":{\"90\":1}}],[\"會造成的問題是吻合的\",{\"1\":{\"70\":1}}],[\"會不同\",{\"1\":{\"64\":1}}],[\"會導致訓練前期較為緩慢\",{\"1\":{\"23\":1}}],[\"會掉\",{\"1\":{\"22\":1}}],[\"會偏向\",{\"1\":{\"22\":1}}],[\"會得到\",{\"1\":{\"22\":1}}],[\"會從\",{\"1\":{\"19\":1,\"150\":1}}],[\"會把每個\",{\"1\":{\"13\":1}}],[\"會選擇不同的\",{\"1\":{\"7\":1}}],[\"會一直到遊戲的最後依照最後通過的時間決定\",{\"1\":{\"3\":1}}],[\"β=1\",{\"1\":{\"195\":1}}],[\"β=0\",{\"1\":{\"9\":1,\"43\":1}}],[\"β為hyper\",{\"1\":{\"43\":1}}],[\"βj​=0\",{\"1\":{\"22\":2}}],[\"βj​=maxj​βj​\",{\"1\":{\"22\":2}}],[\"βj​\",{\"1\":{\"13\":1,\"19\":1,\"22\":4}}],[\"β\",{\"1\":{\"6\":1,\"7\":1,\"16\":1,\"22\":6,\"171\":4,\"172\":1}}],[\"βi​\",{\"1\":{\"6\":1,\"7\":5,\"9\":2,\"10\":1}}],[\"當距離很小時\",{\"1\":{\"193\":1}}],[\"當距離很大時\",{\"1\":{\"193\":1}}],[\"當我們把\",{\"1\":{\"156\":1}}],[\"當然\",{\"1\":{\"133\":1,\"192\":1}}],[\"當作\",{\"1\":{\"133\":1}}],[\"當作輸入\",{\"1\":{\"119\":1}}],[\"當要去更新模型的時候\",{\"1\":{\"114\":1}}],[\"當兩個\",{\"1\":{\"63\":1}}],[\"當\",{\"1\":{\"6\":1,\"22\":2}}],[\"當中中心點的\",{\"1\":{\"194\":1}}],[\"當中使用了\",{\"1\":{\"172\":1}}],[\"當中使用的是\",{\"1\":{\"68\":1}}],[\"當中加上\",{\"1\":{\"168\":1}}],[\"當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",{\"1\":{\"166\":1}}],[\"當中就是環境給予的\",{\"1\":{\"6\":1}}],[\"當中則對於大物件有較差的表現\",{\"1\":{\"159\":1}}],[\"當中則對大多數的\",{\"1\":{\"155\":1}}],[\"當中整體包含了幾個方法\",{\"1\":{\"159\":1}}],[\"當中全部的\",{\"1\":{\"155\":1}}],[\"當中切出來\",{\"1\":{\"150\":1}}],[\"當中有\",{\"1\":{\"137\":1}}],[\"當中有包含的\",{\"1\":{\"93\":1}}],[\"當中包含\",{\"1\":{\"135\":1}}],[\"當中包含了\",{\"1\":{\"134\":2,\"152\":1,\"154\":1}}],[\"當中包含的\",{\"1\":{\"91\":1}}],[\"當中並沒有\",{\"1\":{\"117\":1}}],[\"當中如果要評估一個\",{\"1\":{\"117\":1}}],[\"當中的信心水平\",{\"1\":{\"148\":1}}],[\"當中的\",{\"1\":{\"114\":1,\"172\":1,\"191\":1,\"197\":1}}],[\"當中取得隨機幾筆去更新\",{\"1\":{\"114\":1}}],[\"當中取得的\",{\"1\":{\"8\":1}}],[\"當中我們需要同時訓練兩個\",{\"1\":{\"170\":1}}],[\"當中我們往往仰賴對\",{\"1\":{\"166\":1}}],[\"當中我們看到了使用\",{\"1\":{\"114\":1}}],[\"當中我們會透過\",{\"1\":{\"110\":1}}],[\"當中我們會預設資料之間是沒有什麼相依性的\",{\"1\":{\"108\":1}}],[\"當中同一個\",{\"1\":{\"108\":1}}],[\"當中呢\",{\"1\":{\"108\":1}}],[\"當中常見的\",{\"1\":{\"96\":1}}],[\"當中通常\",{\"1\":{\"67\":1}}],[\"當中都獲得了超過人類的成效\",{\"1\":{\"25\":1}}],[\"當中獲得啟發\",{\"1\":{\"124\":1}}],[\"當中獲得比人類平均還要好的成果\",{\"1\":{\"25\":1}}],[\"當中獲得相當不錯的\",{\"1\":{\"3\":1}}],[\"當中是小許多的\",{\"1\":{\"24\":1}}],[\"當中你可以得到最好的\",{\"1\":{\"14\":1}}],[\"當中\",{\"1\":{\"3\":1,\"6\":1,\"19\":1,\"70\":1,\"114\":1,\"126\":1,\"137\":1,\"187\":1,\"192\":1,\"194\":1}}],[\"dθv​←dθv​+∂\",{\"1\":{\"172\":1}}],[\"dθ←dθ+∇θ\",{\"1\":{\"172\":1}}],[\"dueling\",{\"0\":{\"171\":1,\"176\":1},\"1\":{\"167\":1,\"171\":3,\"176\":2,\"179\":2}}],[\"d=e1​\",{\"1\":{\"114\":1}}],[\"dt​\",{\"1\":{\"71\":1}}],[\"ds​\",{\"1\":{\"71\":1}}],[\"dong\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"downsampling\",{\"1\":{\"148\":1}}],[\"downsample\",{\"1\":{\"93\":1,\"150\":1}}],[\"domain\",{\"0\":{\"61\":2,\"63\":1,\"66\":1,\"71\":2,\"85\":1,\"143\":1,\"185\":1},\"1\":{\"63\":14,\"64\":5,\"66\":7,\"67\":10,\"70\":2,\"71\":6,\"78\":2,\"79\":1,\"82\":5,\"87\":1,\"88\":1,\"90\":6,\"91\":3,\"92\":1,\"100\":1,\"103\":2,\"145\":1,\"146\":1,\"148\":7,\"151\":1,\"161\":2,\"187\":5,\"192\":2,\"196\":5,\"197\":2,\"198\":2,\"201\":1,\"204\":1},\"2\":{\"84\":1,\"105\":1,\"163\":1,\"206\":1}}],[\"does\",{\"1\":{\"58\":1}}],[\"double\",{\"0\":{\"170\":1},\"1\":{\"8\":2,\"167\":1,\"170\":4,\"171\":1}}],[\"dl\",{\"1\":{\"108\":3}}],[\"dl課必教的概念\",{\"1\":{\"57\":1}}],[\"dl相關課程就會學到的基本知識\",{\"1\":{\"57\":1}}],[\"degeneration\",{\"1\":{\"196\":1}}],[\"deterministic\",{\"1\":{\"180\":2}}],[\"detail\",{\"0\":{\"150\":1},\"1\":{\"150\":6,\"151\":8,\"152\":3,\"157\":2,\"159\":8}}],[\"details\",{\"0\":{\"50\":1},\"1\":{\"177\":1}}],[\"description\",{\"0\":{\"129\":1}}],[\"descent\",{\"1\":{\"50\":1,\"58\":1}}],[\"denoising\",{\"0\":{\"126\":1,\"185\":1,\"192\":1},\"1\":{\"187\":1,\"198\":1,\"204\":1}}],[\"dengxin\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"depthwise\",{\"1\":{\"101\":1}}],[\"decoder\",{\"0\":{\"101\":1},\"1\":{\"91\":3,\"96\":1,\"101\":1,\"126\":2,\"150\":1,\"151\":1,\"154\":2}}],[\"deeplabv2\",{\"1\":{\"91\":1,\"98\":1,\"154\":1,\"200\":1}}],[\"deeplabv3+\",{\"1\":{\"91\":1}}],[\"deeplab\",{\"1\":{\"73\":1,\"78\":1,\"87\":1}}],[\"deep\",{\"0\":{\"31\":1,\"106\":1},\"1\":{\"57\":1,\"58\":1,\"107\":1,\"108\":4,\"117\":1,\"119\":2,\"182\":2}}],[\"deepmind\",{\"1\":{\"2\":1,\"165\":1}}],[\"dropout是用來解決overfitting問題的一個基本技巧\",{\"1\":{\"37\":1}}],[\"dropout\",{\"0\":{\"37\":1,\"49\":1,\"122\":1,\"130\":1,\"136\":1},\"1\":{\"34\":1,\"37\":4,\"45\":1,\"124\":1,\"126\":2,\"128\":1,\"129\":1,\"130\":4,\"132\":2,\"133\":8,\"134\":1,\"135\":3,\"136\":5,\"137\":2,\"138\":3,\"139\":1}}],[\"driven\",{\"1\":{\"6\":1}}],[\"divergence代表什麼意義嗎\",{\"1\":{\"204\":1}}],[\"divergence\",{\"1\":{\"196\":1}}],[\"dimensional\",{\"1\":{\"108\":1}}],[\"dilation\",{\"1\":{\"91\":1,\"96\":1}}],[\"differences\",{\"1\":{\"182\":1}}],[\"difference\",{\"1\":{\"58\":1}}],[\"di​上的平均\",{\"1\":{\"50\":1}}],[\"distilation\",{\"1\":{\"145\":1}}],[\"distillation\",{\"0\":{\"197\":1},\"1\":{\"90\":1,\"148\":1,\"197\":1,\"198\":1,\"200\":1,\"203\":1}}],[\"distance\",{\"0\":{\"93\":1,\"100\":1},\"1\":{\"91\":1,\"93\":1,\"148\":1}}],[\"distribution\",{\"1\":{\"64\":1,\"117\":1,\"150\":1,\"169\":1}}],[\"distributed\",{\"0\":{\"177\":1},\"1\":{\"28\":2}}],[\"discriminator\",{\"1\":{\"66\":1}}],[\"discussion\",{\"0\":{\"26\":1,\"54\":1}}],[\"discount\",{\"1\":{\"24\":1,\"110\":2}}],[\"directed\",{\"1\":{\"28\":1}}],[\"dae\",{\"1\":{\"126\":2}}],[\"daes\",{\"0\":{\"126\":1},\"1\":{\"126\":2}}],[\"dai\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"daformer\",{\"0\":{\"85\":1,\"91\":1,\"101\":1},\"1\":{\"86\":1,\"90\":1,\"91\":1,\"92\":1,\"97\":1,\"101\":3,\"103\":5,\"145\":2,\"146\":1,\"148\":5,\"152\":1,\"154\":3,\"155\":2,\"156\":1,\"158\":1}}],[\"day\",{\"1\":{\"82\":2}}],[\"dacs\",{\"0\":{\"61\":1,\"71\":1},\"1\":{\"66\":1,\"68\":1,\"71\":1,\"78\":1,\"79\":1,\"80\":3,\"82\":1,\"96\":1,\"187\":1,\"200\":1}}],[\"datatset\",{\"1\":{\"136\":1}}],[\"data\",{\"0\":{\"46\":1,\"138\":1},\"1\":{\"45\":1,\"63\":1,\"64\":2,\"66\":2,\"67\":7,\"68\":2,\"82\":1,\"90\":1,\"138\":2,\"187\":1,\"193\":1,\"196\":2,\"198\":1}}],[\"datasets\",{\"0\":{\"132\":1,\"133\":1},\"1\":{\"96\":1,\"132\":1,\"154\":1,\"200\":2}}],[\"dataset\",{\"0\":{\"39\":1,\"74\":1,\"135\":1},\"1\":{\"70\":6,\"91\":5,\"96\":1,\"124\":1,\"133\":1,\"134\":1,\"136\":1,\"190\":3,\"191\":1,\"194\":1,\"200\":1}}],[\"david\",{\"1\":{\"19\":1,\"107\":1,\"114\":1,\"117\":1,\"118\":1,\"170\":1}}],[\"dan\",{\"1\":{\"19\":1}}],[\"d\",{\"1\":{\"12\":2,\"93\":2,\"114\":1,\"169\":1}}],[\"dqn\",{\"0\":{\"169\":1,\"170\":1,\"171\":1,\"176\":2},\"1\":{\"8\":1,\"107\":1,\"113\":2,\"114\":2,\"118\":3,\"167\":3,\"169\":1,\"170\":8,\"171\":6,\"172\":1,\"176\":4,\"179\":3}}],[\"ce\",{\"1\":{\"195\":1}}],[\"cluster\",{\"1\":{\"192\":3,\"196\":1}}],[\"clustering\",{\"1\":{\"192\":1}}],[\"classifier\",{\"1\":{\"191\":1}}],[\"classifications\",{\"0\":{\"31\":1},\"1\":{\"58\":1}}],[\"classes\",{\"1\":{\"68\":1,\"75\":1,\"76\":2,\"77\":3,\"79\":1,\"80\":2,\"90\":1,\"91\":2,\"92\":4,\"93\":4,\"97\":2,\"190\":1,\"201\":1}}],[\"classmix\",{\"1\":{\"68\":2,\"70\":1,\"71\":1,\"73\":1,\"81\":1,\"82\":1,\"90\":1}}],[\"class\",{\"0\":{\"92\":1,\"93\":1,\"99\":1,\"100\":1},\"1\":{\"67\":2,\"70\":5,\"78\":2,\"79\":1,\"82\":1,\"91\":2,\"92\":3,\"93\":2,\"97\":1,\"99\":2,\"100\":1,\"101\":1,\"148\":2,\"155\":2,\"156\":3,\"191\":2,\"192\":1,\"193\":2,\"194\":2,\"195\":4,\"196\":1}}],[\"cifar\",{\"1\":{\"133\":4}}],[\"cityscape\",{\"1\":{\"81\":1,\"91\":3}}],[\"cityscapes\",{\"0\":{\"75\":1,\"78\":1,\"79\":1,\"201\":1,\"202\":1},\"1\":{\"74\":3,\"76\":1,\"77\":1,\"80\":1,\"96\":1,\"100\":1,\"145\":1,\"154\":1,\"155\":2,\"156\":1,\"200\":1,\"201\":1,\"202\":1}}],[\"city\",{\"1\":{\"77\":1}}],[\"cthings​\",{\"1\":{\"93\":1}}],[\"c=argmaxc\",{\"1\":{\"90\":1,\"148\":1}}],[\"cce\",{\"1\":{\"90\":1,\"148\":1}}],[\"c\",{\"1\":{\"90\":9,\"92\":5,\"93\":4}}],[\"cbst\",{\"1\":{\"67\":1}}],[\"cnn\",{\"1\":{\"64\":2,\"91\":1,\"94\":1,\"108\":1}}],[\"cnn模型包含了5層的convolutional\",{\"1\":{\"38\":1}}],[\"cvpr2022\",{\"1\":{\"103\":1}}],[\"cvpr22\",{\"1\":{\"103\":1}}],[\"cvpr\",{\"1\":{\"86\":1,\"186\":1},\"2\":{\"105\":1,\"206\":1}}],[\"cv\",{\"1\":{\"63\":1}}],[\"caution\",{\"1\":{\"133\":1,\"198\":1}}],[\"categorical\",{\"1\":{\"90\":1,\"148\":1}}],[\"carlo\",{\"1\":{\"174\":1}}],[\"car\",{\"1\":{\"78\":1}}],[\"cars\",{\"1\":{\"62\":1}}],[\"capped\",{\"1\":{\"21\":1,\"25\":2}}],[\"china\",{\"1\":{\"186\":1}}],[\"chieh\",{\"1\":{\"161\":1}}],[\"chiehchen\",{\"1\":{\"64\":1}}],[\"chen\",{\"1\":{\"151\":1,\"161\":1,\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"channel\",{\"1\":{\"118\":1,\"148\":1,\"149\":1,\"152\":1}}],[\"channels\",{\"1\":{\"91\":1}}],[\"chalmers\",{\"1\":{\"62\":1}}],[\"chns=max\",{\"1\":{\"21\":1}}],[\"chns\",{\"1\":{\"21\":1}}],[\"critic\",{\"1\":{\"172\":1,\"182\":1}}],[\"crop\",{\"0\":{\"150\":1,\"156\":1,\"157\":1},\"1\":{\"150\":9,\"151\":6,\"152\":3,\"156\":4,\"157\":8,\"158\":1,\"159\":2,\"160\":1}}],[\"cross\",{\"0\":{\"61\":1},\"1\":{\"71\":1,\"82\":1,\"90\":1,\"148\":1,\"151\":1,\"191\":1,\"195\":2,\"204\":1}}],[\"crowd\",{\"1\":{\"39\":1}}],[\"credit\",{\"1\":{\"3\":1,\"10\":1,\"17\":2}}],[\"count\",{\"1\":{\"182\":1}}],[\"cover\",{\"1\":{\"136\":1}}],[\"covariance\",{\"1\":{\"48\":1}}],[\"co\",{\"1\":{\"136\":2}}],[\"common\",{\"1\":{\"92\":1}}],[\"compact\",{\"1\":{\"187\":1}}],[\"comparison\",{\"0\":{\"158\":1}}],[\"computer\",{\"1\":{\"62\":1,\"82\":1,\"113\":1},\"2\":{\"60\":1,\"84\":1,\"105\":1,\"163\":1,\"206\":1}}],[\"component\",{\"1\":{\"58\":1}}],[\"color\",{\"1\":{\"90\":1,\"118\":1}}],[\"column\",{\"1\":{\"67\":1}}],[\"cordts\",{\"1\":{\"75\":1}}],[\"corss\",{\"0\":{\"71\":1}}],[\"coin\",{\"1\":{\"22\":6}}],[\"consistency\",{\"0\":{\"196\":1}}],[\"conv\",{\"1\":{\"133\":2}}],[\"convolution\",{\"1\":{\"57\":1,\"101\":1}}],[\"convolutional\",{\"0\":{\"31\":1},\"1\":{\"33\":1,\"58\":1},\"2\":{\"60\":1}}],[\"contingency\",{\"1\":{\"118\":1}}],[\"context\",{\"0\":{\"143\":1,\"150\":1},\"1\":{\"66\":1,\"150\":7,\"151\":6,\"157\":3,\"159\":3,\"161\":2}}],[\"contrast\",{\"1\":{\"204\":2}}],[\"contribution\",{\"0\":{\"27\":1,\"81\":1,\"102\":1,\"119\":1,\"139\":1,\"160\":1,\"181\":1,\"203\":1}}],[\"controller\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":7,\"25\":2,\"27\":1}}],[\"control\",{\"1\":{\"8\":1,\"112\":2,\"118\":1}}],[\"conflation\",{\"1\":{\"70\":1}}],[\"conference\",{\"1\":{\"62\":1}}],[\"confidence\",{\"0\":{\"14\":1}}],[\"connected\",{\"1\":{\"35\":1,\"38\":1,\"40\":1,\"49\":1,\"50\":1,\"133\":1}}],[\"conroller\",{\"1\":{\"24\":1}}],[\"curiosity\",{\"1\":{\"6\":1}}],[\"1m\",{\"1\":{\"179\":1}}],[\"1k\",{\"1\":{\"96\":1}}],[\"18\",{\"1\":{\"96\":1}}],[\"1−λ\",{\"1\":{\"194\":1}}],[\"1−λd​\",{\"1\":{\"151\":2}}],[\"1−ac\",{\"1\":{\"151\":1}}],[\"1−fc​\",{\"1\":{\"92\":1}}],[\"1−α\",{\"1\":{\"90\":1}}],[\"17\",{\"1\":{\"80\":1}}],[\"1774\",{\"1\":{\"24\":1}}],[\"13\",{\"1\":{\"77\":2,\"79\":2,\"80\":1}}],[\"19\",{\"1\":{\"75\":1,\"76\":1}}],[\"1與top\",{\"1\":{\"52\":1}}],[\"1跟top\",{\"1\":{\"42\":1,\"44\":1}}],[\"1+e−x\",{\"1\":{\"41\":1}}],[\"15\",{\"1\":{\"100\":1}}],[\"15萬\",{\"1\":{\"39\":1}}],[\"15×15\",{\"1\":{\"22\":1}}],[\"120\",{\"1\":{\"100\":1}}],[\"120萬\",{\"1\":{\"39\":1}}],[\"12\",{\"1\":{\"96\":1}}],[\"12326\",{\"1\":{\"24\":1}}],[\"1187\",{\"1\":{\"24\":1}}],[\"11361\",{\"1\":{\"24\":1}}],[\"1177\",{\"1\":{\"24\":1}}],[\"14814\",{\"1\":{\"24\":1}}],[\"14\",{\"1\":{\"24\":1,\"97\":1}}],[\"16\",{\"1\":{\"24\":1,\"77\":1,\"79\":2,\"80\":1,\"102\":1}}],[\"1664\",{\"1\":{\"24\":1}}],[\"16926\",{\"1\":{\"24\":1}}],[\"160\",{\"1\":{\"17\":1}}],[\"108k\",{\"1\":{\"179\":1}}],[\"1024x512\",{\"1\":{\"145\":1}}],[\"10​ifpistrueotherwise​\",{\"1\":{\"90\":1,\"148\":1}}],[\"101\",{\"1\":{\"78\":1,\"200\":2}}],[\"10上\",{\"1\":{\"41\":1}}],[\"100×max\",{\"1\":{\"179\":1}}],[\"100×scorehuman​−scorerandom​scoreagent​−scorerandom​​\",{\"1\":{\"179\":1}}],[\"100\",{\"1\":{\"25\":1,\"133\":3,\"179\":1}}],[\"10362\",{\"1\":{\"24\":1}}],[\"10\",{\"1\":{\"23\":1,\"24\":3,\"80\":1,\"133\":3}}],[\"1​​≤i<obd\",{\"1\":{\"151\":1}}],[\"1​​≤i<s⋅obd\",{\"1\":{\"151\":1}}],[\"1​​\",{\"1\":{\"151\":1}}],[\"1​​yk​​∀0≤k≤n−1∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"1​+hd​bd\",{\"1\":{\"150\":1}}],[\"1​+shc​bc\",{\"1\":{\"150\":1}}],[\"1​∼u\",{\"1\":{\"150\":2}}],[\"1​\",{\"1\":{\"150\":2}}],[\"1​m=max\",{\"1\":{\"15\":1}}],[\"1​m=0∑k−1​rk​\",{\"1\":{\"14\":1}}],[\"1\",{\"1\":{\"6\":1,\"8\":1,\"14\":1,\"15\":1,\"16\":1,\"21\":1,\"22\":2,\"36\":1,\"43\":1,\"48\":1,\"52\":1,\"80\":1,\"90\":1,\"96\":1,\"97\":2,\"116\":1,\"129\":1,\"130\":1,\"133\":3,\"148\":4,\"150\":1,\"151\":4,\"156\":2,\"158\":1,\"159\":2,\"191\":1,\"193\":1,\"195\":1,\"196\":1}}],[\"如上面\",{\"1\":{\"100\":1}}],[\"如同\",{\"1\":{\"145\":1,\"148\":1}}],[\"如同過去看過的\",{\"1\":{\"87\":1}}],[\"如同前面看過的\",{\"1\":{\"187\":1}}],[\"如同前面所描述\",{\"1\":{\"151\":1}}],[\"如同前面描述\",{\"1\":{\"133\":1}}],[\"如同前面non\",{\"1\":{\"41\":1}}],[\"如同前面提及\",{\"1\":{\"22\":1}}],[\"如\",{\"1\":{\"24\":1,\"93\":2,\"108\":2,\"145\":1,\"155\":2}}],[\"如果採用低解析度的圖片\",{\"1\":{\"157\":1}}],[\"如果在很少量資料的狀況下\",{\"1\":{\"138\":1}}],[\"如果搭配\",{\"1\":{\"98\":1}}],[\"如果我們看過於細節\",{\"1\":{\"145\":1}}],[\"如果我們用更加強大的\",{\"1\":{\"87\":1}}],[\"如果我們想要訓練一個模型去做自駕車的街景物件偵測\",{\"1\":{\"63\":1}}],[\"如果出現道路或甚至機車\",{\"1\":{\"67\":1}}],[\"如果讓s<z\",{\"1\":{\"44\":1}}],[\"如果讓s=z\",{\"1\":{\"44\":1}}],[\"如果使用傳統的saturating\",{\"1\":{\"41\":1}}],[\"如果一個neuron被\",{\"1\":{\"37\":1}}],[\"如果前一層的activation為0\",{\"1\":{\"37\":1}}],[\"如果選擇較大\",{\"1\":{\"22\":1}}],[\"如果\",{\"1\":{\"15\":1}}],[\"如果每個\",{\"1\":{\"13\":1}}],[\"如此一來就能在\",{\"1\":{\"93\":1}}],[\"如此一來\",{\"1\":{\"13\":1,\"71\":1,\"175\":1,\"196\":1}}],[\"如下圖所示\",{\"1\":{\"91\":1,\"136\":1}}],[\"如下\",{\"1\":{\"6\":1,\"19\":1,\"92\":1,\"93\":2,\"110\":1,\"129\":1,\"151\":1}}],[\"如何計算\",{\"1\":{\"195\":1}}],[\"如何讓\",{\"1\":{\"3\":1}}],[\"如何決定哪些\",{\"1\":{\"3\":1}}],[\"lkd​=lces​\",{\"1\":{\"197\":1}}],[\"lkd​\",{\"1\":{\"197\":1,\"198\":1}}],[\"lklt​=kl\",{\"1\":{\"196\":1}}],[\"lklt​\",{\"1\":{\"196\":1}}],[\"lces​\",{\"1\":{\"198\":1}}],[\"lcet​=−i=1∑h×w​k=1∑k​y^​t\",{\"1\":{\"191\":1}}],[\"lce​\",{\"1\":{\"151\":2}}],[\"lˉ\",{\"1\":{\"174\":1,\"176\":2}}],[\"lv\",{\"1\":{\"172\":1,\"177\":2}}],[\"lhrdat​=\",{\"1\":{\"151\":1}}],[\"lhrdas​=\",{\"1\":{\"151\":1}}],[\"lregt​=−i=1∑h×w​j=1∑k​logpt\",{\"1\":{\"196\":1}}],[\"lregt​\",{\"1\":{\"196\":1}}],[\"lr\",{\"1\":{\"145\":2,\"149\":3,\"150\":5,\"151\":9,\"157\":1,\"159\":1,\"160\":1}}],[\"lrn的概念在後來就很少被使用了\",{\"1\":{\"43\":1}}],[\"lrn\",{\"1\":{\"43\":1}}],[\"l1\",{\"1\":{\"130\":1}}],[\"l+1\",{\"1\":{\"129\":10}}],[\"l∈\",{\"1\":{\"129\":1}}],[\"l=ls​+lt​+λfd​lfd​\",{\"1\":{\"93\":1}}],[\"l=5\",{\"1\":{\"6\":1}}],[\"lfd\",{\"1\":{\"93\":1}}],[\"ltotoal​\",{\"1\":{\"198\":1}}],[\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",{\"1\":{\"196\":1}}],[\"ltotal​\",{\"1\":{\"196\":1}}],[\"lt=lce​\",{\"1\":{\"148\":1}}],[\"lt\",{\"1\":{\"90\":1,\"148\":1}}],[\"lscet​=αlce​\",{\"1\":{\"195\":1}}],[\"lslce​\",{\"1\":{\"148\":1}}],[\"ls​\",{\"1\":{\"93\":1}}],[\"ls\",{\"1\":{\"90\":1}}],[\"lsvrc\",{\"1\":{\"33\":1}}],[\"luc\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"lukas\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"148\":1,\"149\":2,\"150\":1,\"152\":2,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"li​\",{\"1\":{\"110\":1}}],[\"linear\",{\"1\":{\"94\":1,\"112\":1,\"118\":1,\"174\":1}}],[\"liang\",{\"1\":{\"64\":1,\"161\":1}}],[\"life\",{\"1\":{\"6\":2}}],[\"large\",{\"1\":{\"150\":2}}],[\"layout\",{\"1\":{\"66\":2}}],[\"layer\",{\"1\":{\"111\":2,\"129\":5,\"133\":1,\"136\":1,\"137\":1,\"174\":1,\"180\":2}}],[\"layer做出的預測進行平均\",{\"1\":{\"47\":1}}],[\"layers的bias都初始化為1\",{\"1\":{\"50\":1}}],[\"layers與fully\",{\"1\":{\"50\":1}}],[\"layers與3層的fully\",{\"1\":{\"38\":1,\"40\":1}}],[\"layers\",{\"1\":{\"38\":1,\"40\":1,\"133\":1}}],[\"layer的前兩層使用了名為dropout的技巧\",{\"1\":{\"49\":1}}],[\"layer的kernel只會連接到前一層中在同一gpu的kernel\",{\"1\":{\"42\":1}}],[\"layer的運作方式為\",{\"1\":{\"37\":1}}],[\"layer的最後\",{\"1\":{\"35\":1}}],[\"labels\",{\"1\":{\"87\":1,\"90\":2,\"148\":1,\"151\":1,\"187\":1,\"188\":1,\"190\":1,\"191\":3,\"192\":2,\"195\":2}}],[\"labelled\",{\"1\":{\"70\":1}}],[\"labelling\",{\"0\":{\"67\":1},\"1\":{\"68\":1,\"70\":1,\"90\":1,\"187\":1}}],[\"labeling\",{\"1\":{\"67\":1}}],[\"labeled\",{\"1\":{\"67\":2}}],[\"label\",{\"0\":{\"152\":1,\"185\":1,\"192\":1},\"1\":{\"63\":2,\"67\":4,\"71\":1,\"90\":7,\"100\":1,\"108\":1,\"133\":1,\"148\":6,\"152\":1,\"187\":2,\"190\":1,\"192\":4,\"195\":1,\"196\":2,\"198\":1,\"203\":1,\"204\":1}}],[\"lebel\",{\"1\":{\"71\":1,\"195\":1}}],[\"level\",{\"0\":{\"18\":1},\"1\":{\"66\":3,\"91\":1}}],[\"length\",{\"1\":{\"15\":1,\"17\":2,\"23\":1}}],[\"learned\",{\"1\":{\"159\":1}}],[\"learner\",{\"1\":{\"9\":1,\"19\":1}}],[\"learn\",{\"1\":{\"92\":1}}],[\"learning的方式取得非常好的效果\",{\"1\":{\"55\":1}}],[\"learning四個部份\",{\"1\":{\"38\":1}}],[\"learning\",{\"0\":{\"50\":1,\"94\":1,\"98\":1,\"106\":1,\"130\":1,\"185\":1,\"196\":1},\"1\":{\"8\":2,\"28\":5,\"33\":1,\"66\":1,\"67\":3,\"82\":5,\"87\":2,\"91\":2,\"94\":1,\"98\":1,\"108\":4,\"109\":3,\"112\":1,\"117\":1,\"119\":3,\"148\":1,\"156\":2,\"169\":1,\"182\":3,\"187\":3,\"188\":2,\"198\":1,\"204\":3},\"2\":{\"30\":1,\"60\":1,\"121\":1,\"184\":1}}],[\"l\",{\"1\":{\"6\":1,\"8\":1,\"12\":1,\"13\":1,\"71\":1,\"129\":17,\"161\":1,\"169\":1,\"171\":1,\"172\":1,\"174\":2,\"176\":2}}],[\"low\",{\"1\":{\"145\":2,\"149\":2,\"150\":4,\"151\":1,\"157\":1}}],[\"local\",{\"0\":{\"43\":1},\"1\":{\"43\":1,\"58\":1,\"66\":1,\"91\":1}}],[\"log\",{\"1\":{\"14\":1,\"15\":1,\"16\":1}}],[\"losses2\",{\"1\":{\"198\":1}}],[\"losses\",{\"1\":{\"198\":1}}],[\"loss\",{\"0\":{\"8\":1,\"195\":1},\"1\":{\"5\":1,\"8\":5,\"12\":4,\"71\":1,\"90\":3,\"93\":3,\"110\":1,\"145\":1,\"148\":3,\"151\":1,\"159\":2,\"168\":1,\"169\":1,\"171\":1,\"172\":3,\"174\":1,\"176\":1,\"180\":1,\"191\":1,\"195\":1,\"196\":4,\"197\":1}}],[\"long\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"17\":4,\"23\":2,\"25\":1}}],[\"h=f∘g\",{\"1\":{\"191\":1}}],[\"hackmd\",{\"1\":{\"204\":1}}],[\"hasselt\",{\"1\":{\"170\":1}}],[\"hado\",{\"1\":{\"170\":1}}],[\"hard\",{\"1\":{\"5\":1,\"10\":1,\"191\":2,\"195\":1}}],[\"h−shc​\",{\"1\":{\"150\":1}}],[\"h−1\",{\"1\":{\"8\":3}}],[\"hc​=hd​\",{\"1\":{\"150\":1}}],[\"h×w\",{\"1\":{\"150\":1}}],[\"head\",{\"1\":{\"149\":1}}],[\"hs​×ws​×c\",{\"1\":{\"148\":1}}],[\"hsuan\",{\"1\":{\"66\":2,\"73\":1}}],[\"hr1​\",{\"1\":{\"158\":1}}],[\"hr0\",{\"1\":{\"158\":2}}],[\"hrs​\",{\"1\":{\"151\":1}}],[\"hr​=xhr​\",{\"1\":{\"150\":1}}],[\"hr​\",{\"1\":{\"150\":3}}],[\"hr\",{\"1\":{\"145\":1,\"149\":3,\"150\":5,\"151\":9,\"157\":1,\"158\":2,\"159\":1,\"160\":1}}],[\"hrda\",{\"0\":{\"143\":1},\"1\":{\"145\":2,\"148\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":1,\"158\":2,\"161\":2}}],[\"hneat\",{\"1\":{\"118\":2}}],[\"hns=humanscore​−randomscore​agentscore​−randomscore​​\",{\"1\":{\"21\":1}}],[\"hns\",{\"1\":{\"21\":3}}],[\"hf​\",{\"1\":{\"93\":1}}],[\"hϕ​\",{\"1\":{\"90\":1}}],[\"history\",{\"1\":{\"114\":1}}],[\"hidden\",{\"1\":{\"111\":1,\"126\":2,\"128\":3,\"129\":6,\"130\":1,\"136\":3,\"137\":1}}],[\"hinton\",{\"1\":{\"32\":1,\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"high\",{\"0\":{\"18\":1,\"143\":1},\"1\":{\"24\":2,\"81\":1,\"108\":1,\"145\":1,\"149\":2,\"150\":3,\"151\":1,\"158\":1,\"161\":2}}],[\"ht−1​\",{\"1\":{\"19\":1}}],[\"hyperparameters\",{\"1\":{\"80\":1,\"133\":1}}],[\"hyperparameter\",{\"0\":{\"137\":1},\"1\":{\"16\":1,\"21\":1,\"73\":1,\"93\":1,\"96\":1}}],[\"how\",{\"0\":{\"136\":1}}],[\"house\",{\"1\":{\"133\":1}}],[\"hoyer\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"148\":1,\"149\":2,\"150\":1,\"152\":2,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"horgan\",{\"1\":{\"19\":1}}],[\"horizon\",{\"1\":{\"14\":1}}],[\"hot\",{\"1\":{\"12\":1,\"90\":1}}],[\"hμ\",{\"1\":{\"12\":1}}],[\"h\",{\"1\":{\"8\":2,\"12\":4,\"71\":2,\"90\":1,\"93\":1,\"148\":2,\"172\":2,\"191\":1,\"197\":2}}],[\"human\",{\"0\":{\"1\":1},\"1\":{\"21\":2,\"24\":2,\"27\":1,\"28\":1,\"118\":1,\"179\":2}}],[\"uda\",{\"0\":{\"70\":1,\"90\":1,\"94\":1,\"156\":1},\"1\":{\"67\":4,\"70\":2,\"81\":1,\"86\":1,\"87\":4,\"88\":1,\"90\":1,\"91\":5,\"93\":1,\"96\":1,\"101\":1,\"102\":2,\"145\":2,\"146\":1,\"148\":4,\"155\":1,\"156\":2,\"160\":2,\"187\":1,\"188\":1,\"191\":1,\"196\":1,\"203\":2,\"204\":1}}],[\"unsuvervised\",{\"1\":{\"87\":1}}],[\"unsupervised\",{\"1\":{\"67\":1,\"88\":1,\"130\":1,\"146\":1,\"187\":1,\"188\":1,\"204\":2}}],[\"unlabelled\",{\"1\":{\"70\":1}}],[\"unlabeled\",{\"1\":{\"67\":3,\"68\":1}}],[\"unlebelled\",{\"1\":{\"70\":1}}],[\"unit\",{\"1\":{\"129\":4,\"130\":1,\"136\":4,\"137\":1}}],[\"units\",{\"1\":{\"126\":2,\"128\":4,\"136\":2}}],[\"units間的間隔\",{\"1\":{\"44\":1}}],[\"unity\",{\"1\":{\"77\":1}}],[\"university\",{\"1\":{\"32\":1,\"62\":1,\"123\":1,\"186\":1}}],[\"universal\",{\"1\":{\"7\":1}}],[\"uncapped\",{\"1\":{\"25\":1}}],[\"undiscounted\",{\"1\":{\"21\":1}}],[\"uk​\",{\"1\":{\"16\":1}}],[\"uk​<ϵucb​​\",{\"1\":{\"16\":1}}],[\"uk​≥ϵucb​∀n≤k≤k−1\",{\"1\":{\"16\":1}}],[\"ucb\",{\"0\":{\"14\":1,\"15\":1,\"16\":1},\"1\":{\"14\":5,\"15\":2,\"16\":1,\"28\":1}}],[\"uvfa\",{\"0\":{\"7\":1},\"1\":{\"5\":1,\"7\":1}}],[\"upsample\",{\"1\":{\"157\":1}}],[\"upernet\",{\"1\":{\"101\":1}}],[\"upper\",{\"0\":{\"14\":1}}],[\"up\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"usage\",{\"0\":{\"158\":1}}],[\"us\",{\"0\":{\"0\":1}}],[\"模型採用\",{\"1\":{\"200\":1}}],[\"模型就越相信他是屬於\",{\"1\":{\"192\":1}}],[\"模型通常會傾向\",{\"1\":{\"138\":1}}],[\"模型能夠得到的\",{\"1\":{\"117\":1}}],[\"模型很可能已經被\",{\"1\":{\"92\":1}}],[\"模型還是能夠順利學習\",{\"1\":{\"24\":1}}],[\"模型\",{\"1\":{\"3\":1,\"200\":1}}],[\"模型已經能夠在大多的\",{\"1\":{\"3\":1}}],[\"都需要\",{\"1\":{\"175\":1}}],[\"都採用的話則可以進一步降低至\",{\"1\":{\"133\":1}}],[\"都有正面的影響\",{\"1\":{\"179\":1}}],[\"都有\",{\"1\":{\"128\":1,\"190\":1}}],[\"都有相當大的改進\",{\"1\":{\"101\":1}}],[\"都對於\",{\"1\":{\"98\":1}}],[\"都可以獲得更好的結果\",{\"1\":{\"157\":1}}],[\"都可以有獲得提升\",{\"1\":{\"97\":1}}],[\"都可以使用\",{\"1\":{\"73\":1}}],[\"都被\",{\"1\":{\"93\":1}}],[\"都被其他\",{\"1\":{\"70\":1}}],[\"都具有\",{\"1\":{\"90\":1}}],[\"都不太好\",{\"1\":{\"79\":1}}],[\"都是平等對待所導致\",{\"1\":{\"201\":1}}],[\"都是平等的\",{\"1\":{\"195\":1}}],[\"都是會逐漸趨近於\",{\"1\":{\"180\":1}}],[\"都是要對\",{\"1\":{\"174\":1}}],[\"都是\",{\"1\":{\"78\":1,\"171\":1}}],[\"都是虛擬世界當中的影像\",{\"1\":{\"74\":1}}],[\"都是採用\",{\"1\":{\"13\":1,\"96\":1}}],[\"都會帶來\",{\"1\":{\"179\":1}}],[\"都會有正面的影響\",{\"1\":{\"156\":1}}],[\"都會有機率\",{\"1\":{\"129\":1}}],[\"都會在\",{\"1\":{\"136\":1}}],[\"都會變得比較分散\",{\"1\":{\"136\":1}}],[\"都會高過於原本的狀況\",{\"1\":{\"99\":1}}],[\"都會習慣使用\",{\"1\":{\"94\":1}}],[\"都會使用\",{\"1\":{\"93\":1}}],[\"都會對到\",{\"1\":{\"77\":1}}],[\"都會特別大\",{\"1\":{\"67\":1}}],[\"都會接收同樣的\",{\"1\":{\"12\":1}}],[\"都能夠有效地提升最後的\",{\"1\":{\"155\":1}}],[\"都能夠透過\",{\"1\":{\"67\":1}}],[\"都能夠學習什麼時候該\",{\"1\":{\"13\":1}}],[\"都獲得比過去\",{\"1\":{\"155\":1}}],[\"都獲得比\",{\"1\":{\"27\":1}}],[\"都另外加上一個\",{\"1\":{\"21\":1}}],[\"都當成是工廠生產出來的機器人\",{\"1\":{\"13\":1}}],[\"都\",{\"1\":{\"3\":1}}],[\"也有介紹過同樣的\",{\"1\":{\"200\":1}}],[\"也有部分是源自於這樣的相似性帶來的好處\",{\"1\":{\"66\":1}}],[\"也額外定義\",{\"1\":{\"191\":1}}],[\"也是想要解決\",{\"1\":{\"187\":1}}],[\"也並不是每次加上\",{\"1\":{\"179\":1}}],[\"也希望能夠有一個部分能夠讓模型能同時考慮\",{\"1\":{\"151\":1}}],[\"也因此在圖片的輸入上通常會刻意先將輸入圖片的解析度降低\",{\"1\":{\"145\":1}}],[\"也因為如此\",{\"1\":{\"13\":1}}],[\"也許某一個\",{\"1\":{\"136\":1}}],[\"也許才有機會遇到\",{\"1\":{\"3\":1}}],[\"也可以拆成\",{\"1\":{\"191\":1}}],[\"也可以達到類似的效果\",{\"1\":{\"175\":1}}],[\"也可以看出\",{\"1\":{\"155\":1}}],[\"也可以再進一步加上其他的技巧去降低\",{\"1\":{\"133\":1}}],[\"也可以應用在\",{\"1\":{\"130\":1}}],[\"也可以搭配其他的\",{\"1\":{\"130\":1}}],[\"也可以是\",{\"1\":{\"90\":1}}],[\"也確實發現會平滑許多\",{\"1\":{\"117\":1}}],[\"也避免了上述提及的幾個問題\",{\"1\":{\"108\":1}}],[\"也變得能夠預測了\",{\"1\":{\"97\":1}}],[\"也跟最後評估的\",{\"1\":{\"80\":1}}],[\"也獲得不錯的成果\",{\"1\":{\"64\":1}}],[\"也透過dropout以及data\",{\"1\":{\"56\":1}}],[\"也帶起了後來在各處使用convolution\",{\"1\":{\"56\":1}}],[\"也突顯了non\",{\"1\":{\"41\":1}}],[\"也會因為擷取到過於細節的特徵導致大的物件無法好好辨認\",{\"1\":{\"145\":1}}],[\"也會最大\",{\"1\":{\"99\":1}}],[\"也會期待其信心水平也要是高的\",{\"1\":{\"90\":1}}],[\"也會導致gradient是0\",{\"1\":{\"37\":1}}],[\"也會依據得到的\",{\"1\":{\"13\":1}}],[\"也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的\",{\"1\":{\"180\":1}}],[\"也說明了\",{\"1\":{\"25\":1}}],[\"也限制了數值範圍\",{\"1\":{\"21\":1}}],[\"也就可以更好地減輕\",{\"1\":{\"196\":1}}],[\"也就可以用\",{\"1\":{\"148\":1}}],[\"也就不需要再使用\",{\"1\":{\"176\":1,\"177\":1}}],[\"也就不會有需要儲存過多資訊導致的記憶體不足問題\",{\"1\":{\"152\":1}}],[\"也就意味著需要\",{\"1\":{\"175\":1}}],[\"也就如下\",{\"1\":{\"172\":1}}],[\"也就形成\",{\"1\":{\"93\":1}}],[\"也就有更高的機會可以學更多次\",{\"1\":{\"92\":1}}],[\"也就是有些\",{\"1\":{\"196\":1}}],[\"也就是對應到\",{\"1\":{\"133\":1}}],[\"也就是要讓底下的\",{\"1\":{\"110\":1}}],[\"也就是要找到\",{\"1\":{\"110\":1}}],[\"也就是在\",{\"1\":{\"110\":1}}],[\"也就是\",{\"1\":{\"17\":1,\"151\":1,\"170\":1,\"172\":1,\"191\":1,\"195\":1}}],[\"也就是讓底下的期望值最大化\",{\"1\":{\"14\":1}}],[\"也就是說會在訓練的過程當中透過當下的預測給這些\",{\"1\":{\"187\":1}}],[\"也就是說最後的\",{\"1\":{\"180\":1}}],[\"也就是說對於一個參數\",{\"1\":{\"174\":1}}],[\"也就是說理想上每經過一輪更新\",{\"1\":{\"117\":1}}],[\"也就是說可以直接從\",{\"1\":{\"113\":1}}],[\"也就是說我們會期待產生出來的\",{\"1\":{\"90\":1}}],[\"也就是說我們對於\",{\"1\":{\"67\":1}}],[\"也就是說我現在面前有\",{\"1\":{\"14\":1}}],[\"也就是說能夠順利到達\",{\"1\":{\"22\":1}}],[\"也就是說\",{\"1\":{\"14\":1,\"90\":1,\"91\":1,\"92\":1,\"110\":1,\"133\":1,\"180\":1}}],[\"也就是說這種做法的正確性是被確保的\",{\"1\":{\"12\":1}}],[\"也就是前面定義的\",{\"1\":{\"6\":1}}],[\"也就能夠得到\",{\"1\":{\"8\":1}}],[\"也提出了一個可以在所有\",{\"1\":{\"3\":1}}],[\"也需要嘗試越過那些障礙\",{\"1\":{\"3\":1}}],[\"的解決方案\",{\"1\":{\"201\":1}}],[\"的知識\",{\"1\":{\"197\":1}}],[\"的不平衡導致預測錯誤\",{\"1\":{\"195\":1}}],[\"的不同\",{\"1\":{\"66\":1}}],[\"的容忍程度\",{\"1\":{\"195\":1}}],[\"的所有\",{\"1\":{\"194\":1}}],[\"的中心點\",{\"1\":{\"193\":1}}],[\"的中心點就如同這裡的\",{\"1\":{\"192\":1}}],[\"的特徵中心點\",{\"1\":{\"193\":1}}],[\"的更新\",{\"1\":{\"192\":1}}],[\"的前提下\",{\"1\":{\"191\":1}}],[\"的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同\",{\"1\":{\"180\":1}}],[\"的數量\",{\"1\":{\"177\":2}}],[\"的形式\",{\"1\":{\"176\":2}}],[\"的計算本質上就是找中心點\",{\"1\":{\"194\":1}}],[\"的計算方式如下\",{\"1\":{\"194\":1}}],[\"的計算\",{\"1\":{\"174\":1}}],[\"的想法基本上是相同方向\",{\"1\":{\"174\":1}}],[\"的想法跟\",{\"1\":{\"174\":1}}],[\"的算式\",{\"1\":{\"172\":1}}],[\"的論文\",{\"1\":{\"172\":2}}],[\"的概念就如同火影忍者的影分身之術\",{\"1\":{\"172\":1}}],[\"的概念仍然是透過\",{\"1\":{\"171\":1}}],[\"的總和就能夠得到\",{\"1\":{\"171\":1}}],[\"的決定上採用了\",{\"1\":{\"169\":1,\"171\":1}}],[\"的限制\",{\"1\":{\"168\":1,\"171\":1}}],[\"的效果\",{\"1\":{\"168\":1}}],[\"的效益參差不齊\",{\"1\":{\"136\":1}}],[\"的亂度越高越好\",{\"1\":{\"168\":1}}],[\"的時候都是透過增加\",{\"1\":{\"168\":1}}],[\"的各種\",{\"1\":{\"167\":1}}],[\"的記憶體\",{\"1\":{\"158\":1}}],[\"的記憶體空間足夠\",{\"1\":{\"145\":1}}],[\"的顯卡了\",{\"1\":{\"158\":1}}],[\"的有無對於最終結果的影響會是更大的\",{\"1\":{\"157\":1}}],[\"的預測有改善\",{\"1\":{\"195\":1}}],[\"的預測可以有更進一步的提升\",{\"1\":{\"156\":1}}],[\"的預測都有提升\",{\"1\":{\"156\":1}}],[\"的預測結果要接近\",{\"1\":{\"71\":1}}],[\"的預測結果\",{\"1\":{\"66\":1,\"149\":1}}],[\"的部份使用的是\",{\"1\":{\"154\":1}}],[\"的部分如下\",{\"1\":{\"202\":1}}],[\"的部分明顯可以看到最後的\",{\"1\":{\"201\":1}}],[\"的部分一如既往採用\",{\"1\":{\"200\":1}}],[\"的部分採用了\",{\"1\":{\"200\":1}}],[\"的部分在退步也是有幾項退步蠻多\",{\"1\":{\"179\":1}}],[\"的部分改用\",{\"1\":{\"176\":1,\"177\":1}}],[\"的部分作者另外的調整時選用的\",{\"1\":{\"96\":1}}],[\"的部分\",{\"1\":{\"91\":1}}],[\"的部分只能取得\",{\"1\":{\"91\":1}}],[\"的部分也可以發現到兩者的發展方向會稍有不同\",{\"1\":{\"22\":1}}],[\"的部分是分別給\",{\"1\":{\"12\":1}}],[\"的部分目的也是希望能夠促使\",{\"1\":{\"6\":1}}],[\"的項\",{\"1\":{\"151\":1}}],[\"的值相乘\",{\"1\":{\"151\":1}}],[\"的交互考量\",{\"1\":{\"151\":1}}],[\"的一部分\",{\"1\":{\"151\":1}}],[\"的一種\",{\"1\":{\"68\":1}}],[\"的兩倍\",{\"1\":{\"150\":1}}],[\"的邊長都會是\",{\"1\":{\"150\":1}}],[\"的小圖片\",{\"1\":{\"150\":1}}],[\"的大圖片\",{\"1\":{\"150\":1}}],[\"的大小相差越來越懸殊\",{\"1\":{\"22\":1}}],[\"的大小下\",{\"1\":{\"22\":1}}],[\"的大小\",{\"1\":{\"10\":1,\"22\":1,\"116\":1,\"145\":1,\"190\":1}}],[\"的優點帶走\",{\"1\":{\"160\":1}}],[\"的優勢\",{\"1\":{\"149\":1}}],[\"的優劣\",{\"1\":{\"25\":1}}],[\"的圖片\",{\"1\":{\"150\":1}}],[\"的圖片解析度有\",{\"1\":{\"145\":1}}],[\"的圖片由於是透過車子上裝設攝影鏡頭去蒐集的\",{\"1\":{\"100\":1}}],[\"的資訊\",{\"1\":{\"145\":1,\"148\":1,\"152\":2}}],[\"的資料拿進來使用\",{\"1\":{\"197\":1}}],[\"的資料分佈會隨著\",{\"1\":{\"108\":1,\"110\":1,\"114\":1}}],[\"的資料通常都會先\",{\"1\":{\"108\":1}}],[\"的資料\",{\"1\":{\"90\":2,\"148\":2,\"198\":1}}],[\"的資料數量\",{\"1\":{\"90\":2,\"148\":2}}],[\"的資料不存在任何\",{\"1\":{\"67\":1}}],[\"的資料上只有一些\",{\"1\":{\"67\":1}}],[\"的彩色圖片\",{\"1\":{\"133\":1}}],[\"的門牌號碼彩色照片\",{\"1\":{\"133\":1}}],[\"的架構上採用\",{\"1\":{\"154\":1}}],[\"的架構都可以得到更好的結果\",{\"1\":{\"133\":1}}],[\"的架構如同前面所述\",{\"1\":{\"96\":1}}],[\"的手寫數字\",{\"1\":{\"133\":1}}],[\"的性能以及通用性\",{\"1\":{\"132\":1}}],[\"的編號\",{\"1\":{\"129\":1}}],[\"的輸出\",{\"1\":{\"129\":1}}],[\"的輸出結果都乘上機率\",{\"1\":{\"128\":1}}],[\"的輸入輸出就會變成底下的樣子\",{\"1\":{\"129\":1}}],[\"的輸入\",{\"1\":{\"129\":1}}],[\"的輸入去學習一直是一個很大的挑戰\",{\"1\":{\"108\":1}}],[\"的類別超級多\",{\"1\":{\"133\":1}}],[\"的類別\",{\"1\":{\"126\":1}}],[\"的位置以及類型\",{\"1\":{\"118\":1}}],[\"的平均去評估\",{\"1\":{\"117\":1}}],[\"的好壞就相對困難\",{\"1\":{\"117\":1}}],[\"的好壞\",{\"1\":{\"117\":1}}],[\"的目的是要讓整體的\",{\"1\":{\"110\":1}}],[\"的目標就是要讓整體的\",{\"1\":{\"110\":1}}],[\"的目標是希望結合\",{\"1\":{\"145\":1}}],[\"的目標是在改採用更佳的\",{\"1\":{\"87\":1}}],[\"的目標是把兩個不同分佈的\",{\"1\":{\"63\":1}}],[\"的定義如下\",{\"1\":{\"110\":1,\"176\":1}}],[\"的改變而有巨大幅度的變化\",{\"1\":{\"108\":1,\"110\":1,\"114\":1}}],[\"的訓練資料具有高度相關性\",{\"1\":{\"108\":1,\"114\":1}}],[\"的訓練資料\",{\"1\":{\"108\":1}}],[\"的訓練上也是使用\",{\"1\":{\"90\":1}}],[\"的角度來看\",{\"1\":{\"108\":1}}],[\"的感官資料\",{\"1\":{\"108\":1}}],[\"的成長\",{\"1\":{\"155\":2,\"158\":2}}],[\"的成功\",{\"1\":{\"118\":1}}],[\"的成功也放進\",{\"1\":{\"108\":1}}],[\"的成功帶進\",{\"1\":{\"102\":1}}],[\"的成本過高\",{\"1\":{\"87\":1}}],[\"的影響力\",{\"1\":{\"172\":1}}],[\"的影響\",{\"1\":{\"102\":1,\"156\":1,\"160\":1,\"196\":1}}],[\"的影響程度\",{\"1\":{\"6\":1,\"172\":1}}],[\"的表達能力可以更強\",{\"1\":{\"100\":1}}],[\"的表現都比起其他架構來得好許多\",{\"1\":{\"91\":1}}],[\"的變化就比較不與\",{\"1\":{\"99\":1}}],[\"的變化很大程度跟\",{\"1\":{\"99\":1}}],[\"的情況下\",{\"1\":{\"99\":1,\"157\":1}}],[\"的情況下有很大的不同\",{\"1\":{\"92\":1}}],[\"的使用上如同過去我們看過的\",{\"1\":{\"96\":1}}],[\"的第\",{\"1\":{\"93\":1}}],[\"的實作比較簡單\",{\"1\":{\"90\":1}}],[\"的實驗\",{\"1\":{\"24\":1}}],[\"的產生方式可以是\",{\"1\":{\"90\":1}}],[\"的符號\",{\"1\":{\"90\":1,\"148\":1}}],[\"的描述\",{\"1\":{\"90\":1,\"129\":1}}],[\"的同時\",{\"1\":{\"87\":1}}],[\"的關聯性就能被連結起來\",{\"1\":{\"71\":1}}],[\"的核心做法是不單只是跟\",{\"1\":{\"71\":1}}],[\"的步驟\",{\"1\":{\"68\":1}}],[\"的技巧\",{\"1\":{\"68\":1,\"116\":1}}],[\"的例子\",{\"1\":{\"67\":1}}],[\"的認識嚴重缺乏\",{\"1\":{\"196\":1}}],[\"的認識\",{\"1\":{\"67\":1}}],[\"的方式來處理\",{\"1\":{\"187\":1}}],[\"的方式\",{\"1\":{\"172\":1}}],[\"的方式是採用\",{\"1\":{\"8\":1}}],[\"的方法上\",{\"1\":{\"155\":1}}],[\"的方法上雖然任何\",{\"1\":{\"73\":1}}],[\"的方法對於辨識細節相當不利\",{\"1\":{\"145\":1}}],[\"的方法會使用\",{\"1\":{\"90\":1}}],[\"的方法是把套上\",{\"1\":{\"90\":1}}],[\"的方法\",{\"1\":{\"70\":1,\"91\":1,\"112\":1,\"139\":1,\"148\":1,\"160\":1,\"166\":1,\"180\":1,\"203\":1}}],[\"的方法來降低這種問題\",{\"1\":{\"67\":1}}],[\"的方法解決了\",{\"1\":{\"67\":1}}],[\"的觀念\",{\"1\":{\"56\":1}}],[\"的超大超深cnn\",{\"1\":{\"55\":1}}],[\"的kernel\",{\"1\":{\"43\":1}}],[\"的四個缺陷\",{\"1\":{\"25\":1}}],[\"的比較當中明顯看到在所有的成績都有所提升\",{\"1\":{\"25\":1}}],[\"的比例改變\",{\"1\":{\"13\":1}}],[\"的普遍性\",{\"1\":{\"25\":1}}],[\"的重要性\",{\"1\":{\"22\":1}}],[\"的趨勢仍然是隨著\",{\"1\":{\"22\":1}}],[\"的狀況缺乏認知\",{\"1\":{\"64\":1}}],[\"的狀況下\",{\"1\":{\"22\":1}}],[\"的狀況\",{\"1\":{\"22\":2}}],[\"的做法簡單來說就是兩件事情\",{\"1\":{\"128\":1}}],[\"的做法是對\",{\"1\":{\"126\":1}}],[\"的做法就是照著\",{\"1\":{\"70\":1}}],[\"的做法之所以能夠成功\",{\"1\":{\"66\":1}}],[\"的做法\",{\"1\":{\"22\":1,\"175\":1}}],[\"的缺陷\",{\"1\":{\"22\":1}}],[\"的設計上也相當直覺\",{\"1\":{\"71\":1}}],[\"的設計是採用\",{\"1\":{\"17\":1}}],[\"的設定上對於目標被發現存在高估的問題\",{\"1\":{\"170\":1}}],[\"的設定上參考了許多過去的研究\",{\"1\":{\"73\":1}}],[\"的設定基本上跟\",{\"1\":{\"73\":1}}],[\"的設定下會大程度影響到最終\",{\"1\":{\"22\":1}}],[\"的設定取得的\",{\"1\":{\"22\":1}}],[\"的設定會透過\",{\"1\":{\"22\":1}}],[\"的設定詳閱論文的\",{\"1\":{\"21\":1}}],[\"的話大致上會在\",{\"1\":{\"137\":1}}],[\"的話會導致\",{\"1\":{\"112\":1}}],[\"的話\",{\"1\":{\"17\":1,\"37\":1,\"108\":1}}],[\"的選用有關\",{\"1\":{\"99\":1}}],[\"的選項有更高機率被選擇到\",{\"1\":{\"14\":1}}],[\"的選擇相關\",{\"1\":{\"99\":1}}],[\"的選擇根據\",{\"1\":{\"19\":1}}],[\"的選擇應遠比\",{\"1\":{\"15\":1}}],[\"的選擇\",{\"1\":{\"10\":1}}],[\"的傾向\",{\"1\":{\"13\":1}}],[\"的存在\",{\"1\":{\"13\":1}}],[\"的機率不被使用到\",{\"1\":{\"128\":1}}],[\"的機率\",{\"1\":{\"13\":1,\"99\":1,\"191\":1}}],[\"的版本去訓練\",{\"1\":{\"151\":1}}],[\"的版本是少了\",{\"1\":{\"77\":1}}],[\"的版本\",{\"1\":{\"12\":1,\"79\":1,\"110\":1}}],[\"的模型都跟\",{\"1\":{\"179\":1}}],[\"的模型都是採用如\",{\"1\":{\"87\":1}}],[\"的模型在訓練過程當中那些被暫時移除的\",{\"1\":{\"130\":1}}],[\"的模型萃取出圖片的特徵\",{\"1\":{\"113\":1}}],[\"的模型得出來的結果\",{\"1\":{\"110\":1}}],[\"的模型\",{\"1\":{\"78\":1}}],[\"的模型對於\",{\"1\":{\"64\":1}}],[\"的模型雖然有許多\",{\"1\":{\"64\":1}}],[\"的模型會盡可能避開\",{\"1\":{\"22\":1}}],[\"的模型變成底下的樣子\",{\"1\":{\"12\":1}}],[\"的模型當成最後的結果\",{\"1\":{\"9\":1}}],[\"的參數更新\",{\"1\":{\"192\":1}}],[\"的參數加上\",{\"1\":{\"174\":1}}],[\"的參數\",{\"1\":{\"12\":2,\"172\":1}}],[\"的作者認為是因為\",{\"1\":{\"10\":1}}],[\"的問題似乎能夠得到改善\",{\"1\":{\"17\":1}}],[\"的問題\",{\"0\":{\"10\":1},\"1\":{\"10\":1,\"17\":1,\"87\":1,\"102\":1,\"136\":1,\"187\":1}}],[\"的\",{\"1\":{\"8\":2,\"10\":1,\"12\":7,\"13\":2,\"16\":1,\"22\":2,\"24\":1,\"66\":2,\"71\":1,\"73\":1,\"76\":1,\"77\":1,\"79\":1,\"87\":1,\"91\":3,\"92\":1,\"93\":4,\"100\":1,\"108\":1,\"114\":2,\"126\":1,\"129\":1,\"133\":1,\"135\":1,\"136\":2,\"154\":1,\"166\":1,\"168\":1,\"169\":2,\"172\":1,\"174\":2,\"180\":2,\"187\":1,\"191\":1,\"193\":1,\"194\":2,\"197\":1,\"203\":1}}],[\"的分布往往分散\",{\"1\":{\"195\":1}}],[\"的分布上有做了一點調整\",{\"1\":{\"21\":1}}],[\"的分布也會變動\",{\"1\":{\"15\":1}}],[\"的分布會變動的話\",{\"1\":{\"15\":1}}],[\"的分布是固定的狀況下會使用\",{\"1\":{\"14\":1}}],[\"的分布\",{\"1\":{\"7\":1}}],[\"的研究\",{\"1\":{\"6\":1}}],[\"的範圍\",{\"1\":{\"6\":1}}],[\"的環境當中有更好的成效\",{\"1\":{\"5\":1}}],[\"的地方只能參考\",{\"1\":{\"151\":1}}],[\"的地方\",{\"1\":{\"3\":1}}],[\"的結果在細節上會有缺失\",{\"1\":{\"159\":1}}],[\"的結果後相加就會是\",{\"1\":{\"151\":1}}],[\"的結果以外\",{\"1\":{\"151\":1}}],[\"的結果要接近\",{\"1\":{\"71\":1}}],[\"的結果就當作是他的\",{\"1\":{\"67\":1}}],[\"的結果\",{\"1\":{\"3\":1,\"22\":1}}],[\"是空的狀態\",{\"1\":{\"196\":1}}],[\"是由弱增強得到\",{\"1\":{\"196\":1}}],[\"是把一個可訓練參數拆成\",{\"1\":{\"180\":1}}],[\"是為了採用底下的特性方便後續\",{\"1\":{\"174\":1}}],[\"是使用\",{\"1\":{\"172\":1}}],[\"是被固定的參數\",{\"1\":{\"169\":1}}],[\"是上一個\",{\"1\":{\"169\":1}}],[\"是相當重要的\",{\"1\":{\"157\":1}}],[\"是單純的\",{\"1\":{\"151\":1}}],[\"是真的有在學習特徵\",{\"1\":{\"136\":1}}],[\"是類似的\",{\"1\":{\"136\":1}}],[\"是因為\",{\"1\":{\"158\":1}}],[\"是因為不同的\",{\"1\":{\"136\":1}}],[\"是因為即便是在很多\",{\"1\":{\"3\":1}}],[\"是只有使用\",{\"1\":{\"78\":1}}],[\"是源自於即便\",{\"1\":{\"66\":1}}],[\"是非常尖端且突破性的研究\",{\"1\":{\"56\":1}}],[\"是沒有辦法對這麼大的神經網路進行實驗的\",{\"1\":{\"41\":1}}],[\"是每個\",{\"1\":{\"13\":1}}],[\"是在\",{\"1\":{\"12\":2,\"168\":1,\"176\":1,\"177\":1}}],[\"是一種\",{\"1\":{\"68\":1}}],[\"是一樣的\",{\"1\":{\"12\":1,\"80\":1}}],[\"是一個用於手寫辨識的資料集\",{\"1\":{\"133\":1}}],[\"是一個\",{\"1\":{\"16\":3,\"93\":1,\"191\":1}}],[\"是一個可以用來評估或是用在\",{\"1\":{\"8\":1}}],[\"是一個相當重要的\",{\"1\":{\"3\":1}}],[\"是不同的\",{\"1\":{\"6\":1}}],[\"是\",{\"1\":{\"6\":2,\"8\":1,\"73\":1,\"90\":1,\"91\":1,\"113\":1,\"133\":1,\"148\":2,\"152\":1,\"177\":2,\"191\":1,\"193\":1}}],[\"或是從低解析度\",{\"1\":{\"157\":1}}],[\"或是到\",{\"1\":{\"137\":1}}],[\"或是再額外加上其他優化的技巧都可以得到更好的結果\",{\"1\":{\"135\":1}}],[\"或是使用其他的優化手段都有辦法獲得更好的\",{\"1\":{\"134\":1}}],[\"或是汽車比卡車更常見\",{\"1\":{\"67\":1}}],[\"或是\",{\"1\":{\"3\":1,\"14\":1,\"87\":2,\"94\":1,\"98\":1,\"112\":1,\"151\":1,\"154\":1,\"156\":1,\"172\":1}}],[\"truncate\",{\"1\":{\"179\":1}}],[\"train\",{\"1\":{\"78\":2,\"93\":1,\"100\":1}}],[\"training\",{\"0\":{\"42\":1,\"67\":1,\"85\":1,\"90\":1},\"1\":{\"39\":1,\"68\":2,\"75\":1,\"76\":1,\"77\":1,\"82\":1,\"88\":1,\"90\":2,\"91\":2,\"103\":1,\"146\":1,\"148\":1,\"187\":2,\"188\":1,\"192\":2,\"201\":1}}],[\"transition\",{\"1\":{\"169\":1}}],[\"transformer\",{\"1\":{\"88\":1,\"91\":7,\"94\":1,\"100\":1,\"102\":1,\"148\":1,\"152\":1}}],[\"transformed\",{\"1\":{\"8\":2,\"12\":3}}],[\"tranheden\",{\"1\":{\"70\":2,\"71\":2,\"78\":2,\"79\":1}}],[\"trajectory\",{\"1\":{\"19\":1}}],[\"trajectories\",{\"1\":{\"8\":1,\"12\":1,\"19\":1}}],[\"trace\",{\"1\":{\"17\":4,\"23\":4,\"25\":1}}],[\"table\",{\"1\":{\"169\":1}}],[\"target\",{\"0\":{\"185\":1,\"191\":1},\"1\":{\"8\":3,\"12\":1,\"63\":3,\"64\":1,\"66\":2,\"67\":7,\"70\":2,\"71\":4,\"90\":3,\"110\":1,\"114\":1,\"119\":1,\"148\":5,\"151\":1,\"169\":1,\"187\":3,\"190\":3,\"191\":1,\"193\":1,\"194\":1,\"196\":4,\"198\":1,\"204\":1}}],[\"t\",{\"1\":{\"110\":1,\"114\":1,\"172\":1,\"196\":8}}],[\"td\",{\"0\":{\"111\":1},\"1\":{\"109\":1,\"112\":1,\"114\":1}}],[\"twarm​\",{\"1\":{\"94\":1}}],[\"t​\",{\"1\":{\"92\":1}}],[\"tuning\",{\"1\":{\"130\":1}}],[\"tune\",{\"1\":{\"67\":1,\"198\":2}}],[\"turk\",{\"1\":{\"39\":1}}],[\"temporal\",{\"1\":{\"182\":1}}],[\"temperature\",{\"1\":{\"92\":1,\"193\":1}}],[\"tensorflow\",{\"1\":{\"182\":1}}],[\"text\",{\"0\":{\"135\":1}}],[\"te\",{\"1\":{\"92\":1}}],[\"teacher\",{\"1\":{\"90\":2,\"145\":1,\"148\":2,\"197\":1,\"198\":2}}],[\"test\",{\"1\":{\"133\":1,\"136\":1}}],[\"testset\",{\"1\":{\"80\":1}}],[\"testing\",{\"1\":{\"39\":1,\"91\":1}}],[\"technology\",{\"1\":{\"62\":1,\"186\":1}}],[\"term\",{\"1\":{\"3\":1,\"10\":1,\"17\":2,\"58\":1}}],[\"to\",{\"0\":{\"70\":1,\"122\":1,\"197\":1},\"1\":{\"70\":1,\"74\":1,\"80\":2,\"81\":2,\"82\":2,\"113\":1,\"182\":1}}],[\"top\",{\"1\":{\"43\":1,\"52\":1,\"133\":3}}],[\"tool\",{\"1\":{\"39\":1}}],[\"toronto\",{\"1\":{\"32\":1,\"123\":1}}],[\"tsai\",{\"1\":{\"66\":2,\"73\":1}}],[\"ts\",{\"1\":{\"28\":1}}],[\"ting\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"timit\",{\"1\":{\"134\":1}}],[\"time\",{\"0\":{\"17\":1,\"23\":1}}],[\"ti\",{\"1\":{\"102\":1}}],[\"tips\",{\"1\":{\"8\":1,\"12\":1,\"13\":1,\"16\":1,\"17\":1,\"22\":1,\"66\":1,\"70\":1,\"168\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"174\":1}}],[\"thread\",{\"1\":{\"176\":1,\"177\":1}}],[\"through\",{\"0\":{\"17\":1,\"23\":1}}],[\"thing\",{\"0\":{\"93\":1,\"100\":1},\"1\":{\"91\":1,\"93\":2,\"148\":1}}],[\"thq\",{\"1\":{\"8\":1}}],[\"theory\",{\"1\":{\"124\":1}}],[\"thelimeydragon\",{\"1\":{\"3\":1}}],[\"the\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1,\"58\":2,\"124\":1}}],[\"t^q\",{\"1\":{\"8\":1}}],[\"t∈n​\",{\"1\":{\"8\":1}}],[\"找到寶藏可以加分\",{\"1\":{\"3\":1}}],[\"過程當中也會有一些機率出現\",{\"1\":{\"124\":1}}],[\"過程中有許多陷阱\",{\"1\":{\"3\":1}}],[\"過於老舊的部分作者先透過一些實驗去尋找好的架構\",{\"1\":{\"91\":1}}],[\"過高\",{\"1\":{\"63\":1}}],[\"過去增加探索的方法大多都是在\",{\"1\":{\"168\":1}}],[\"過去類似的作法出現在\",{\"1\":{\"126\":1}}],[\"過去會透過多次遊戲中\",{\"1\":{\"117\":1}}],[\"過去在\",{\"1\":{\"108\":1}}],[\"過去訓練\",{\"1\":{\"94\":1}}],[\"過去對於\",{\"1\":{\"14\":1}}],[\"過去的經驗上都還需要額外設計一些\",{\"1\":{\"148\":1}}],[\"過去的研究當中發現到如果是\",{\"1\":{\"112\":1}}],[\"過去的\",{\"1\":{\"3\":1,\"148\":1}}],[\"分開\",{\"1\":{\"91\":1}}],[\"分為大概22000個類別\",{\"1\":{\"39\":1}}],[\"分布相當不同時\",{\"1\":{\"10\":1}}],[\"分成了兩個部分\",{\"1\":{\"6\":1}}],[\"分鐘的時間探索\",{\"1\":{\"3\":1}}],[\"分別表示\",{\"1\":{\"172\":1,\"190\":3}}],[\"分別表示圖片的高寬\",{\"1\":{\"90\":1}}],[\"分別下降了0\",{\"1\":{\"44\":1}}],[\"分別落在哪個\",{\"1\":{\"24\":1}}],[\"分別用\",{\"1\":{\"23\":1}}],[\"分別去針對\",{\"1\":{\"12\":1}}],[\"分別是\",{\"1\":{\"6\":1,\"96\":1,\"196\":1}}],[\"分別是這些遊戲\",{\"1\":{\"3\":1}}],[\"分別在\",{\"1\":{\"3\":1}}],[\"玩家要操作主角在\",{\"1\":{\"3\":1}}],[\"玩家要操作角色滑雪\",{\"1\":{\"3\":1}}],[\"number\",{\"0\":{\"175\":1},\"1\":{\"133\":1,\"175\":2}}],[\"nitish\",{\"1\":{\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"nfq\",{\"0\":{\"113\":1},\"1\":{\"109\":1,\"113\":2}}],[\"nt​\",{\"1\":{\"90\":1,\"148\":1,\"190\":1}}],[\"ns​\",{\"1\":{\"90\":1,\"148\":1,\"190\":1}}],[\"naive\",{\"0\":{\"70\":1},\"1\":{\"70\":2,\"90\":1}}],[\"n是該層的總kernel數\",{\"1\":{\"43\":1}}],[\"nk​\",{\"1\":{\"14\":2,\"15\":2}}],[\"n−1\",{\"1\":{\"14\":1,\"16\":1}}],[\"n\",{\"1\":{\"14\":1,\"114\":1,\"128\":1,\"137\":1}}],[\"nn\",{\"1\":{\"8\":3,\"10\":1,\"12\":3,\"22\":1}}],[\"ngu+sep\",{\"1\":{\"24\":1}}],[\"ngu\",{\"0\":{\"9\":1,\"10\":1},\"1\":{\"5\":3,\"7\":2,\"8\":1,\"9\":2,\"10\":2,\"12\":2,\"13\":1,\"16\":1,\"17\":1,\"19\":1,\"22\":5,\"24\":1,\"25\":2}}],[\"newswire\",{\"1\":{\"135\":1}}],[\"net\",{\"1\":{\"133\":2,\"167\":1,\"174\":1,\"176\":2}}],[\"nets\",{\"0\":{\"130\":1}}],[\"network在computer\",{\"1\":{\"57\":1}}],[\"network都會有不同的架構\",{\"1\":{\"49\":1}}],[\"network來分類imagenet\",{\"1\":{\"33\":1}}],[\"networks\",{\"0\":{\"31\":1,\"110\":1,\"122\":1,\"164\":1},\"1\":{\"24\":1,\"25\":1,\"27\":1,\"28\":1,\"33\":1,\"58\":1,\"107\":1,\"109\":1},\"2\":{\"60\":1}}],[\"network\",{\"0\":{\"85\":1,\"91\":1,\"136\":1},\"1\":{\"8\":4,\"12\":3,\"22\":6,\"24\":2,\"56\":1,\"66\":2,\"71\":1,\"73\":1,\"91\":2,\"93\":1,\"96\":1,\"103\":2,\"110\":4,\"112\":2,\"114\":4,\"117\":1,\"119\":1,\"124\":2,\"126\":1,\"129\":2,\"136\":2,\"145\":1,\"148\":4,\"150\":2,\"152\":1,\"154\":1,\"166\":1,\"169\":4,\"171\":1,\"174\":2,\"176\":2,\"177\":2,\"187\":1,\"191\":1,\"192\":3}}],[\"next\",{\"1\":{\"114\":1}}],[\"needs\",{\"1\":{\"82\":1}}],[\"need\",{\"1\":{\"82\":1}}],[\"neurips\",{\"1\":{\"107\":1},\"2\":{\"121\":1}}],[\"neuron\",{\"1\":{\"41\":1}}],[\"neurons的重要\",{\"1\":{\"41\":1}}],[\"neurons所提\",{\"1\":{\"41\":1}}],[\"neurons\",{\"0\":{\"36\":1},\"1\":{\"34\":1}}],[\"neurlips\",{\"1\":{\"32\":1}}],[\"neural\",{\"0\":{\"31\":1,\"122\":1},\"1\":{\"28\":1,\"33\":2,\"49\":1,\"56\":1,\"57\":1,\"58\":1,\"109\":1,\"110\":1,\"114\":1,\"124\":1,\"126\":1,\"129\":2,\"166\":1,\"169\":3,\"171\":1},\"2\":{\"60\":1}}],[\"never\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"negative\",{\"1\":{\"3\":3,\"116\":1}}],[\"noisynet\",{\"1\":{\"177\":1,\"179\":6,\"180\":1}}],[\"noisy\",{\"0\":{\"164\":1},\"1\":{\"167\":1,\"174\":1,\"176\":3,\"177\":1,\"188\":1}}],[\"noise\",{\"0\":{\"168\":1},\"1\":{\"126\":3,\"167\":1,\"168\":12,\"174\":8,\"175\":3,\"176\":1,\"177\":4,\"180\":2,\"182\":2,\"195\":1,\"196\":1}}],[\"normal\",{\"1\":{\"150\":1}}],[\"normalization\",{\"0\":{\"43\":1},\"1\":{\"43\":2,\"58\":2}}],[\"normalized\",{\"1\":{\"21\":2}}],[\"notation\",{\"1\":{\"90\":1,\"129\":1,\"148\":1,\"190\":1}}],[\"notes\",{\"1\":{\"82\":1}}],[\"note\",{\"1\":{\"12\":1,\"172\":2,\"179\":1},\"2\":{\"29\":1,\"59\":1,\"83\":1,\"104\":1,\"120\":1,\"141\":1,\"162\":1,\"183\":1,\"205\":1}}],[\"nonlinearities\",{\"1\":{\"58\":1}}],[\"nonlinearity的activity\",{\"1\":{\"43\":1}}],[\"nonlinearity\",{\"0\":{\"41\":1},\"1\":{\"41\":2}}],[\"non\",{\"0\":{\"36\":1},\"1\":{\"34\":1,\"112\":1}}],[\"novelty\",{\"1\":{\"6\":1}}],[\"noveltyαt​\",{\"1\":{\"6\":1}}],[\"noveltyrtepisodic​\",{\"1\":{\"6\":1}}],[\"no\",{\"1\":{\"3\":1}}],[\"秒的\",{\"1\":{\"3\":1}}],[\"就能夠迫使模型對於這些略有不同的\",{\"1\":{\"196\":1}}],[\"就能夠比較好發揮作用\",{\"1\":{\"68\":1}}],[\"就需要看過整個\",{\"1\":{\"194\":1}}],[\"就被錯誤分類\",{\"1\":{\"187\":1}}],[\"就應該隨著訓練慢慢被忽視\",{\"1\":{\"180\":1}}],[\"就都是用這個\",{\"1\":{\"174\":1}}],[\"就只是這樣而已\",{\"1\":{\"174\":1}}],[\"就小到幾乎不存在了\",{\"1\":{\"170\":1}}],[\"就比較有系統性一些\",{\"1\":{\"168\":1}}],[\"就比較像是在亂試\",{\"1\":{\"168\":1}}],[\"就像是看到有殼的動物就當成是昆蟲\",{\"1\":{\"187\":1}}],[\"就像是\",{\"1\":{\"171\":1}}],[\"就像是可以換個角度去想其他人會怎麼做\",{\"1\":{\"168\":1}}],[\"就像是獵人裡面的凱特\",{\"1\":{\"168\":1}}],[\"就像是這邊的\",{\"1\":{\"151\":1}}],[\"就像是有時候你的隊友會請假\",{\"1\":{\"136\":1}}],[\"就很不相同\",{\"1\":{\"117\":1}}],[\"就持續上一個做出的\",{\"1\":{\"116\":1}}],[\"就定義成\",{\"1\":{\"114\":1}}],[\"就基本上沒有\",{\"1\":{\"93\":1}}],[\"就不是單純的\",{\"1\":{\"90\":1}}],[\"就相當地雷同\",{\"1\":{\"66\":1}}],[\"就如同上面所提\",{\"1\":{\"49\":1}}],[\"就是上述的\",{\"1\":{\"192\":1}}],[\"就是在\",{\"1\":{\"168\":1}}],[\"就是用\",{\"1\":{\"110\":1}}],[\"就是用來描述一群資料他們的分布狀況\",{\"1\":{\"63\":1}}],[\"就是解決了使用更好的\",{\"1\":{\"91\":1}}],[\"就是希望\",{\"1\":{\"71\":1}}],[\"就是\",{\"1\":{\"66\":1,\"70\":1}}],[\"就是說將一張狗的圖片變換顏色與亮度\",{\"1\":{\"48\":1}}],[\"就是物件的identity對於顏色的照度與強度是不變的\",{\"1\":{\"48\":1}}],[\"就是對於每個pixel的rgb值進行pca降維\",{\"1\":{\"48\":1}}],[\"就會導致互相的不理解\",{\"1\":{\"64\":1}}],[\"就會導致單純在\",{\"1\":{\"63\":1}}],[\"就會因為\",{\"1\":{\"13\":1}}],[\"就會去環境當中互動\",{\"1\":{\"13\":1}}],[\"就會多\",{\"1\":{\"3\":1}}],[\"就有不同重要程度了\",{\"1\":{\"13\":1}}],[\"就做得頗差\",{\"1\":{\"10\":1}}],[\"就可以分別得到\",{\"1\":{\"150\":1}}],[\"就可以用\",{\"1\":{\"92\":1}}],[\"就可以結合起來形成新的\",{\"1\":{\"90\":1}}],[\"就可以再拿去\",{\"1\":{\"67\":1}}],[\"就可以得到相當好的影像分割結果\",{\"1\":{\"64\":1}}],[\"就可以得到單純\",{\"1\":{\"9\":1}}],[\"就可以透過\",{\"1\":{\"8\":1,\"148\":1}}],[\"就跟\",{\"1\":{\"8\":1}}],[\"就通常完全沒辦法學習\",{\"1\":{\"3\":1}}],[\"途中要盡可能快速通過指定數量的\",{\"1\":{\"3\":1}}],[\"這三個\",{\"1\":{\"200\":1}}],[\"這篇\",{\"1\":{\"145\":1}}],[\"這篇其實沒有用到什麼複雜的技巧\",{\"1\":{\"57\":1}}],[\"這就像是不同地方的人行道也許會有不同的地磚設計\",{\"1\":{\"145\":1}}],[\"這就像是平常訓練的時候你有隊友可以\",{\"1\":{\"136\":1}}],[\"這就像是同理心\",{\"1\":{\"64\":1}}],[\"這跟前面提到只使用\",{\"1\":{\"70\":1}}],[\"這種包含了\",{\"1\":{\"91\":1}}],[\"這種相似的\",{\"1\":{\"70\":1}}],[\"這種\",{\"1\":{\"68\":2,\"145\":1}}],[\"這種狀況下訓練模型就被稱為半監督式學習\",{\"1\":{\"67\":1}}],[\"這種差距被描述為\",{\"1\":{\"63\":1}}],[\"這類的\",{\"1\":{\"66\":1}}],[\"這樣所需要的成本會過大\",{\"1\":{\"63\":1}}],[\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升\",{\"1\":{\"201\":1}}],[\"這樣的方法會有過多的計算量\",{\"1\":{\"124\":1}}],[\"這樣的狀況之所以會出現有可能有幾個原因\",{\"1\":{\"124\":1}}],[\"這樣的想法自然而然就出現了\",{\"1\":{\"108\":1}}],[\"這樣的結果有多少\",{\"1\":{\"90\":1}}],[\"這樣的問題只在\",{\"1\":{\"70\":1}}],[\"這樣的做法下每一個\",{\"1\":{\"175\":1}}],[\"這樣的做法有趣的是能夠將\",{\"1\":{\"68\":1}}],[\"這樣的做法之所以可行\",{\"1\":{\"66\":1}}],[\"這樣的作法成功讓top\",{\"1\":{\"48\":1}}],[\"這樣的作法讓top\",{\"1\":{\"42\":1}}],[\"這樣的normalization方式分別讓top\",{\"1\":{\"43\":1}}],[\"這張圖展示了\",{\"1\":{\"41\":1}}],[\"這是一篇將deep\",{\"1\":{\"33\":1}}],[\"這一點尤其在\",{\"1\":{\"99\":1}}],[\"這一款遊戲\",{\"1\":{\"23\":1}}],[\"這一篇論文成功將\",{\"1\":{\"108\":1}}],[\"這一篇同樣也是先說明了\",{\"1\":{\"87\":1}}],[\"這一篇\",{\"1\":{\"3\":1,\"91\":1,\"187\":1}}],[\"這個參數會漸漸趨近於\",{\"1\":{\"180\":1}}],[\"這個論文提出的做法稱為\",{\"1\":{\"107\":1}}],[\"這個類別居然會隨著訓練時間預測結果越糟糕\",{\"1\":{\"100\":1}}],[\"這個\",{\"1\":{\"80\":1,\"134\":1}}],[\"這個是前面提到kernel間特殊的連接方式所造成的結果\",{\"1\":{\"53\":1}}],[\"這個non\",{\"1\":{\"41\":1}}],[\"這個結果如果在取得\",{\"1\":{\"22\":1}}],[\"這個測量標準比較強調那些\",{\"1\":{\"21\":1}}],[\"這個問題被稱為\",{\"1\":{\"136\":1}}],[\"這個問題\",{\"1\":{\"13\":1}}],[\"這裡作者採用\",{\"1\":{\"195\":1}}],[\"這裡設為\",{\"1\":{\"193\":1}}],[\"這裡之所以是拿\",{\"1\":{\"158\":1}}],[\"這裡也可以觀察到當我們把解析度提升\",{\"1\":{\"156\":1}}],[\"這裡也加進來\",{\"1\":{\"94\":1}}],[\"這裡使用的\",{\"1\":{\"154\":1,\"196\":1}}],[\"這裡先定義一下接下來會用到的基本\",{\"1\":{\"148\":1,\"190\":1}}],[\"這裡先簡單總結一下\",{\"1\":{\"97\":1}}],[\"這裡考慮有\",{\"1\":{\"110\":1}}],[\"這裡考慮到\",{\"1\":{\"90\":1}}],[\"這裡選用的\",{\"1\":{\"91\":1}}],[\"這裡會採用\",{\"1\":{\"90\":1}}],[\"這裡的距離是投射到高維空間之後\",{\"1\":{\"192\":1}}],[\"這裡的\",{\"1\":{\"90\":1,\"93\":1,\"148\":2,\"150\":1}}],[\"這裡的經驗指的是一個\",{\"1\":{\"15\":1}}],[\"這裡已經預設包含了\",{\"1\":{\"90\":1}}],[\"這裡要來實驗這一個做法實際上帶來多少影響\",{\"1\":{\"22\":1}}],[\"這裡就不贅述\",{\"1\":{\"21\":1}}],[\"這些圖片會被分類成\",{\"1\":{\"133\":1}}],[\"這些圖像都是從網路上收集\",{\"1\":{\"39\":1}}],[\"這些普遍做得不錯的\",{\"1\":{\"78\":1}}],[\"這些neuron的output永遠都是0\",{\"1\":{\"37\":1}}],[\"這些\",{\"1\":{\"13\":1,\"93\":1}}],[\"這兩個\",{\"1\":{\"93\":1,\"99\":1}}],[\"這兩個問題\",{\"1\":{\"16\":1}}],[\"這兩者分別會讓\",{\"1\":{\"6\":1}}],[\"這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到\",{\"1\":{\"3\":1}}],[\"這款遊戲來說\",{\"1\":{\"3\":2}}],[\"以達成\",{\"1\":{\"168\":1}}],[\"以達到更好的訓練成效\",{\"1\":{\"12\":1}}],[\"以外的範圍都是黑的\",{\"1\":{\"151\":1}}],[\"以至於實際上每個\",{\"1\":{\"136\":1}}],[\"以致於難以\",{\"1\":{\"92\":1}}],[\"以致於開始將研究的方向轉向如\",{\"1\":{\"87\":1}}],[\"以下就分別說明這三個部分的作法\",{\"1\":{\"91\":1}}],[\"以下採用兩種形式的data\",{\"1\":{\"46\":1}}],[\"以下根據重要性排序\",{\"1\":{\"40\":1}}],[\"以下簡稱cnn\",{\"1\":{\"33\":1}}],[\"以上的結果\",{\"1\":{\"149\":1}}],[\"以上\",{\"1\":{\"22\":1}}],[\"以\",{\"1\":{\"3\":2,\"58\":1,\"148\":1}}],[\"以及加上\",{\"1\":{\"179\":1}}],[\"以及多少的\",{\"1\":{\"151\":1}}],[\"以及各種新定義的\",{\"1\":{\"145\":1}}],[\"以及畫面下方\",{\"1\":{\"100\":1}}],[\"以及訓練模型的\",{\"1\":{\"93\":1}}],[\"以及一般性都並不是很理想\",{\"1\":{\"90\":1}}],[\"以及一個對應的\",{\"1\":{\"192\":1}}],[\"以及一個\",{\"1\":{\"22\":1}}],[\"以及他對於\",{\"1\":{\"90\":1}}],[\"以及他們的horizontal\",{\"1\":{\"47\":1}}],[\"以及model越來越deep的風潮\",{\"1\":{\"56\":1}}],[\"以及沒有的狀況\",{\"1\":{\"24\":1}}],[\"以及對應的\",{\"1\":{\"23\":1}}],[\"以及最傾向\",{\"1\":{\"22\":1}}],[\"以及\",{\"1\":{\"3\":1,\"6\":2,\"10\":1,\"12\":2,\"22\":2,\"23\":2,\"24\":1,\"63\":1,\"66\":2,\"70\":1,\"71\":1,\"74\":1,\"91\":1,\"114\":2,\"116\":1,\"119\":2,\"126\":1,\"133\":1,\"145\":1,\"148\":1,\"149\":1,\"150\":4,\"154\":1,\"156\":1,\"172\":3,\"179\":2,\"181\":1,\"187\":1,\"191\":1,\"196\":1,\"198\":1}}],[\"v\",{\"1\":{\"171\":5}}],[\"volodymyr\",{\"1\":{\"107\":1,\"114\":1,\"117\":1,\"118\":1}}],[\"volvo\",{\"1\":{\"62\":1}}],[\"vgg\",{\"1\":{\"87\":1}}],[\"v2\",{\"1\":{\"73\":1,\"78\":1}}],[\"v是momentum\",{\"1\":{\"50\":1}}],[\"van\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2,\"170\":1}}],[\"vanishing\",{\"1\":{\"41\":1,\"58\":2}}],[\"variational\",{\"1\":{\"182\":1}}],[\"variable\",{\"1\":{\"50\":1}}],[\"varied\",{\"1\":{\"82\":1}}],[\"validation\",{\"1\":{\"39\":1,\"80\":4,\"117\":2}}],[\"value\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"7\":2,\"8\":1,\"12\":1,\"19\":1,\"110\":1,\"111\":1,\"114\":1,\"169\":2,\"171\":1,\"172\":2,\"182\":1}}],[\"vime\",{\"1\":{\"182\":1}}],[\"view\",{\"1\":{\"133\":1}}],[\"visual\",{\"1\":{\"113\":1,\"204\":2}}],[\"vision領域最有影響力的論文之一\",{\"1\":{\"57\":1}}],[\"vision上才開始蔚為流行\",{\"1\":{\"57\":1}}],[\"vision\",{\"1\":{\"33\":1,\"62\":1,\"82\":1,\"113\":1},\"2\":{\"60\":1,\"84\":1,\"105\":1,\"163\":1,\"206\":1}}],[\"virtual\",{\"1\":{\"77\":1}}],[\"viktor\",{\"1\":{\"68\":2}}],[\"via\",{\"0\":{\"61\":1,\"71\":1},\"1\":{\"82\":2,\"182\":1}}],[\"videos\",{\"1\":{\"168\":1}}],[\"video\",{\"1\":{\"3\":2}}],[\"veg\",{\"1\":{\"78\":1}}],[\"vector\",{\"1\":{\"12\":1}}],[\"venture\",{\"1\":{\"3\":1,\"24\":1}}],[\"2∣st+i​\",{\"1\":{\"172\":1,\"177\":1}}],[\"2​​∧obd\",{\"1\":{\"151\":1}}],[\"2​​∧s⋅obd\",{\"1\":{\"151\":1}}],[\"2​=bd\",{\"1\":{\"150\":1}}],[\"2​=bc\",{\"1\":{\"150\":1}}],[\"2​\",{\"1\":{\"150\":2,\"177\":1}}],[\"22\",{\"1\":{\"133\":1}}],[\"22480\",{\"1\":{\"24\":1}}],[\"28×28\",{\"1\":{\"133\":1}}],[\"2n\",{\"1\":{\"128\":1}}],[\"25\",{\"1\":{\"82\":1}}],[\"255\",{\"1\":{\"3\":1}}],[\"24gb\",{\"1\":{\"158\":1}}],[\"24\",{\"1\":{\"82\":1}}],[\"24966\",{\"1\":{\"76\":1}}],[\"21\",{\"1\":{\"24\":1}}],[\"26\",{\"1\":{\"24\":2}}],[\"2600\",{\"1\":{\"3\":2}}],[\"2975\",{\"1\":{\"75\":1}}],[\"29\",{\"1\":{\"24\":1,\"135\":1}}],[\"2ϵ1+4ϵ\",{\"1\":{\"8\":1}}],[\"2\",{\"1\":{\"8\":1,\"12\":1,\"19\":1,\"42\":1,\"43\":1,\"97\":3,\"110\":1,\"133\":1,\"156\":1,\"159\":2,\"169\":1,\"171\":1,\"172\":1,\"176\":4,\"204\":1}}],[\"2048x1024\",{\"1\":{\"145\":1}}],[\"2080\",{\"1\":{\"102\":1}}],[\"2022\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"2021\",{\"1\":{\"62\":1,\"67\":1,\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"2020\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"62\":1,\"68\":2}}],[\"2017\",{\"1\":{\"168\":1}}],[\"2014\",{\"1\":{\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1,\"166\":1}}],[\"2013\",{\"1\":{\"107\":1}}],[\"2016\",{\"1\":{\"75\":1,\"77\":1,\"161\":1}}],[\"2015\",{\"1\":{\"64\":1,\"114\":1,\"117\":1,\"118\":1,\"169\":1,\"170\":1,\"171\":1}}],[\"2010中\",{\"1\":{\"52\":1}}],[\"2010與ilsvrc\",{\"1\":{\"52\":1}}],[\"2010作為資料集\",{\"1\":{\"39\":1}}],[\"2010資料集中的120萬張高解析度圖像\",{\"1\":{\"33\":1}}],[\"2012的測試集label沒有公開\",{\"1\":{\"52\":1}}],[\"2012的結果\",{\"1\":{\"52\":1}}],[\"2012的競賽\",{\"1\":{\"39\":1}}],[\"2012\",{\"1\":{\"32\":1}}],[\"2018\",{\"1\":{\"19\":1,\"64\":1,\"66\":2,\"67\":1,\"73\":1,\"165\":1,\"179\":3,\"180\":1}}],[\"200\",{\"1\":{\"22\":1}}],[\"20\",{\"1\":{\"3\":1,\"22\":1}}],[\"outlier\",{\"1\":{\"195\":1}}],[\"output\",{\"1\":{\"82\":1,\"150\":1}}],[\"outperform\",{\"1\":{\"3\":2,\"101\":1,\"111\":1,\"118\":1}}],[\"outperforming\",{\"0\":{\"1\":1},\"1\":{\"28\":1}}],[\"olivier\",{\"1\":{\"166\":1}}],[\"olsson\",{\"1\":{\"68\":2}}],[\"ohc​​×owc​​×c\",{\"1\":{\"151\":1}}],[\"o\",{\"1\":{\"150\":1}}],[\"object\",{\"1\":{\"118\":1}}],[\"observation\",{\"1\":{\"19\":1}}],[\"oracle\",{\"1\":{\"91\":2,\"101\":1}}],[\"or\",{\"0\":{\"67\":1},\"1\":{\"77\":1}}],[\"openai\",{\"1\":{\"168\":3,\"174\":1}}],[\"operation\",{\"1\":{\"8\":1}}],[\"operator\",{\"1\":{\"8\":1,\"12\":1}}],[\"optimal\",{\"1\":{\"110\":1,\"169\":1,\"171\":1}}],[\"optimization\",{\"1\":{\"58\":1}}],[\"offline\",{\"1\":{\"90\":1}}],[\"off\",{\"1\":{\"28\":1,\"112\":1,\"172\":2,\"181\":1}}],[\"of\",{\"0\":{\"13\":1,\"50\":1,\"138\":1,\"156\":1},\"1\":{\"32\":1,\"38\":1,\"58\":1,\"62\":2,\"124\":2,\"186\":2}}],[\"overhead\",{\"1\":{\"176\":1,\"177\":1}}],[\"overview\",{\"0\":{\"128\":1,\"149\":1,\"155\":1}}],[\"overfit\",{\"1\":{\"100\":1,\"136\":1,\"192\":2}}],[\"overfitting\",{\"0\":{\"45\":1,\"122\":1},\"1\":{\"87\":2,\"91\":1,\"102\":1,\"124\":3,\"126\":1,\"138\":1,\"139\":1}}],[\"overfitting與details\",{\"1\":{\"38\":1}}],[\"overlapping\",{\"0\":{\"44\":1,\"152\":1},\"1\":{\"44\":2,\"159\":2}}],[\"over\",{\"0\":{\"13\":1}}],[\"only\",{\"1\":{\"91\":1}}],[\"online\",{\"1\":{\"8\":1,\"12\":1,\"90\":2,\"172\":1,\"187\":1}}],[\"one\",{\"1\":{\"12\":1,\"90\":1}}],[\"on\",{\"0\":{\"42\":1,\"133\":1,\"134\":1,\"135\":1,\"156\":1},\"1\":{\"3\":2,\"62\":1,\"73\":2,\"81\":1,\"82\":1,\"118\":1,\"152\":1,\"172\":1,\"181\":1}}],[\"eπ\",{\"1\":{\"177\":1}}],[\"eπ​\",{\"1\":{\"14\":1}}],[\"embedding\",{\"1\":{\"152\":1}}],[\"ema\",{\"1\":{\"90\":1,\"148\":1,\"194\":1}}],[\"eccv22\",{\"1\":{\"161\":1}}],[\"eccv\",{\"1\":{\"144\":1},\"2\":{\"163\":1}}],[\"effect\",{\"0\":{\"136\":1,\"138\":1}}],[\"efficient\",{\"1\":{\"28\":1}}],[\"e2​\",{\"1\":{\"114\":1}}],[\"encode\",{\"1\":{\"196\":1}}],[\"encoder\",{\"1\":{\"91\":6,\"96\":3,\"126\":2,\"150\":1,\"154\":1,\"193\":2,\"196\":2}}],[\"enforcing\",{\"0\":{\"196\":1}}],[\"en​\",{\"1\":{\"114\":1}}],[\"enduro\",{\"1\":{\"116\":1}}],[\"end\",{\"1\":{\"113\":2}}],[\"entropy和kl\",{\"1\":{\"204\":1}}],[\"entropy\",{\"1\":{\"71\":1,\"90\":1,\"148\":1,\"151\":1,\"166\":1,\"167\":1,\"168\":1,\"172\":3,\"177\":1,\"191\":1,\"195\":2,\"204\":1}}],[\"evolution\",{\"1\":{\"124\":1,\"182\":1}}],[\"everything\",{\"1\":{\"82\":1}}],[\"evaluation\",{\"0\":{\"52\":1,\"53\":1,\"80\":1}}],[\"evaluator\",{\"1\":{\"21\":1}}],[\"early\",{\"1\":{\"80\":1}}],[\"error\",{\"1\":{\"42\":1,\"43\":1,\"44\":1,\"48\":1,\"50\":1,\"52\":3,\"133\":3,\"134\":1,\"135\":1,\"136\":1}}],[\"e\",{\"1\":{\"32\":1,\"176\":2}}],[\"extension\",{\"1\":{\"103\":1}}],[\"extractor\",{\"1\":{\"191\":1,\"196\":1}}],[\"extraction\",{\"0\":{\"47\":1},\"1\":{\"46\":1}}],[\"extrinsic\",{\"1\":{\"6\":1,\"9\":1,\"12\":3,\"19\":1,\"22\":4}}],[\"examples訓練出來的networks的\",{\"1\":{\"42\":1}}],[\"exp\",{\"1\":{\"193\":1,\"196\":2}}],[\"experiments\",{\"0\":{\"179\":1}}],[\"experience\",{\"1\":{\"9\":2,\"10\":1,\"13\":4,\"16\":1,\"19\":2,\"28\":2,\"114\":9,\"119\":1,\"169\":2,\"172\":1}}],[\"exponential\",{\"1\":{\"90\":1,\"148\":1,\"194\":1}}],[\"explanation\",{\"1\":{\"58\":1}}],[\"explore\",{\"1\":{\"13\":1}}],[\"exploration\",{\"0\":{\"13\":1,\"24\":1,\"164\":1,\"168\":1},\"1\":{\"3\":1,\"5\":1,\"6\":2,\"7\":1,\"10\":2,\"13\":1,\"14\":1,\"22\":5,\"28\":1,\"166\":2,\"167\":1,\"168\":5,\"171\":1,\"172\":1,\"174\":1,\"180\":1,\"181\":1,\"182\":4}}],[\"exploit\",{\"1\":{\"13\":1}}],[\"exploitation\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"13\":1,\"14\":1,\"22\":5}}],[\"episode\",{\"1\":{\"6\":2,\"13\":1,\"19\":2,\"21\":1,\"22\":3,\"108\":1,\"174\":2,\"175\":1,\"179\":1}}],[\"eye\",{\"1\":{\"3\":2,\"24\":1}}],[\"et​=\",{\"1\":{\"114\":1}}],[\"eth\",{\"1\":{\"86\":1,\"144\":1}}],[\"et\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2,\"73\":1,\"75\":1,\"76\":1,\"77\":1,\"107\":1,\"114\":1,\"117\":1,\"118\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1,\"165\":1,\"169\":1,\"171\":1,\"179\":3,\"180\":1}}],[\"mlp\",{\"1\":{\"154\":1}}],[\"m=1nt​​\",{\"1\":{\"148\":1}}],[\"m=1ns​​\",{\"1\":{\"148\":2}}],[\"m​∈rht​×wt​×3\",{\"1\":{\"148\":1}}],[\"m​∈rhs​×ws​×3\",{\"1\":{\"148\":1}}],[\"m​∈\",{\"1\":{\"148\":1}}],[\"m​\",{\"1\":{\"148\":3}}],[\"mnist\",{\"1\":{\"133\":2,\"136\":1}}],[\"mnih\",{\"1\":{\"107\":1,\"114\":1,\"117\":1,\"118\":1}}],[\"mdp\",{\"1\":{\"110\":1}}],[\"mmsegmentation\",{\"1\":{\"96\":1}}],[\"mthings\",{\"1\":{\"93\":2}}],[\"mpi\",{\"1\":{\"86\":1,\"144\":1}}],[\"mscoco\",{\"1\":{\"73\":1}}],[\"m\",{\"1\":{\"68\":3}}],[\"microsoft\",{\"1\":{\"186\":1}}],[\"mit\",{\"1\":{\"91\":2,\"96\":1,\"100\":1,\"154\":1}}],[\"miou\",{\"1\":{\"79\":1,\"87\":1,\"91\":1,\"97\":5,\"101\":1,\"155\":2,\"156\":1,\"158\":2,\"159\":3,\"201\":1,\"202\":1}}],[\"mix\",{\"1\":{\"71\":1}}],[\"mixup\",{\"1\":{\"68\":1}}],[\"mixing\",{\"0\":{\"68\":1,\"70\":1},\"1\":{\"68\":3,\"70\":2,\"71\":2,\"73\":2}}],[\"mixed\",{\"0\":{\"61\":1,\"71\":1},\"1\":{\"70\":1,\"82\":1}}],[\"mini\",{\"1\":{\"194\":2}}],[\"min\",{\"1\":{\"6\":1,\"15\":1,\"21\":1}}],[\"moco\",{\"1\":{\"204\":1}}],[\"momentum\",{\"1\":{\"193\":1,\"204\":2}}],[\"momentum為0\",{\"1\":{\"50\":1}}],[\"modified\",{\"1\":{\"187\":1}}],[\"models\",{\"1\":{\"93\":1,\"201\":1}}],[\"model\",{\"0\":{\"129\":1,\"197\":1},\"1\":{\"67\":1,\"78\":1,\"90\":4,\"91\":1,\"110\":1,\"111\":1,\"112\":1,\"130\":1,\"148\":1,\"154\":1,\"160\":1,\"170\":1,\"197\":3,\"198\":6,\"203\":1}}],[\"monte\",{\"1\":{\"174\":1}}],[\"montezuma\",{\"1\":{\"3\":2,\"24\":1}}],[\"mohammad\",{\"1\":{\"165\":1,\"179\":3,\"180\":1}}],[\"moving\",{\"1\":{\"90\":1,\"148\":1,\"194\":2}}],[\"median\",{\"1\":{\"179\":1}}],[\"medium\",{\"1\":{\"63\":2,\"151\":1,\"172\":1}}],[\"meire\",{\"1\":{\"165\":1,\"179\":3,\"180\":1}}],[\"memory\",{\"0\":{\"158\":1},\"1\":{\"114\":2,\"172\":1}}],[\"mechanical\",{\"1\":{\"39\":1}}],[\"mean\",{\"1\":{\"25\":3,\"58\":1,\"174\":2,\"179\":1}}],[\"methods\",{\"1\":{\"182\":1}}],[\"method\",{\"1\":{\"34\":1,\"81\":1}}],[\"methodology\",{\"0\":{\"11\":1,\"38\":1,\"69\":1,\"89\":1,\"114\":1,\"127\":1,\"147\":1,\"173\":1,\"189\":1}}],[\"meta\",{\"1\":{\"13\":5,\"14\":1,\"19\":1,\"24\":8,\"25\":2,\"27\":1}}],[\"matthieu\",{\"1\":{\"166\":1}}],[\"matrix的第i個eigenvector與eigenvalue\",{\"1\":{\"48\":1}}],[\"marius\",{\"1\":{\"75\":1}}],[\"mask\",{\"1\":{\"68\":3,\"73\":1,\"93\":1}}],[\"majchrowska\",{\"1\":{\"67\":1}}],[\"map做出預測\",{\"1\":{\"91\":1}}],[\"map\",{\"1\":{\"66\":1,\"68\":3,\"91\":1,\"151\":1}}],[\"maps\",{\"1\":{\"66\":2,\"91\":1}}],[\"mab\",{\"1\":{\"14\":2,\"28\":1}}],[\"maximizing\",{\"1\":{\"182\":1}}],[\"maxb∈a​q\",{\"1\":{\"170\":1}}],[\"max​q\",{\"1\":{\"110\":1}}],[\"maxc\",{\"1\":{\"90\":1}}],[\"maxout\",{\"1\":{\"58\":1}}],[\"max\",{\"1\":{\"6\":2}}],[\"mutation\",{\"1\":{\"124\":1}}],[\"mulitiple\",{\"0\":{\"42\":1}}],[\"multi\",{\"0\":{\"151\":1},\"1\":{\"14\":1,\"111\":1,\"160\":1,\"177\":1}}],[\"muzero\",{\"1\":{\"3\":3,\"25\":3}}],[\"不知道\",{\"1\":{\"191\":1}}],[\"不要想太多\",{\"1\":{\"174\":1}}],[\"不難發現到確實都存在高估的狀況\",{\"1\":{\"170\":1}}],[\"不被選到\",{\"1\":{\"128\":1}}],[\"不需要事先經過其他的分解\",{\"1\":{\"119\":1}}],[\"不需要看太遠\",{\"1\":{\"7\":1}}],[\"不是\",{\"1\":{\"113\":1}}],[\"不是那麼地\",{\"1\":{\"10\":1,\"16\":1}}],[\"不採用\",{\"1\":{\"91\":1}}],[\"不論是prediction還是classification\",{\"1\":{\"53\":1}}],[\"不論重新設定初始權重跑幾次都會如此\",{\"1\":{\"53\":1}}],[\"不確定性低\",{\"1\":{\"14\":1}}],[\"不確定性高\",{\"1\":{\"14\":1}}],[\"不會被固定下來\",{\"1\":{\"13\":1}}],[\"不穩定\",{\"1\":{\"12\":1}}],[\"不過進步主要在\",{\"1\":{\"179\":1}}],[\"不過並沒有保證收斂\",{\"1\":{\"166\":1}}],[\"不過相較之下\",{\"1\":{\"157\":1}}],[\"不過使用的目標是\",{\"1\":{\"151\":1}}],[\"不過只運用這樣的\",{\"1\":{\"148\":1}}],[\"不過當你們現在處在一個未知的環境當中\",{\"1\":{\"136\":1}}],[\"不過實際上對於\",{\"1\":{\"136\":1}}],[\"不過實際上訓練時因為拆開來訓練\",{\"1\":{\"12\":1}}],[\"不過隨著參數量的上升\",{\"1\":{\"124\":1}}],[\"不過作者發現在他們的模型得出來的結果往往會是很不穩定的\",{\"1\":{\"117\":1}}],[\"不過\",{\"1\":{\"113\":1}}],[\"不過從\",{\"1\":{\"108\":1}}],[\"不過也考慮到\",{\"1\":{\"93\":1}}],[\"不過過去使用\",{\"1\":{\"91\":1}}],[\"不過直覺上\",{\"1\":{\"87\":1}}],[\"不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索\",{\"1\":{\"166\":1}}],[\"不過這裡的比較卻乏單純的\",{\"1\":{\"133\":1}}],[\"不過這裡的成功只停止在雙陸棋上\",{\"1\":{\"111\":1}}],[\"不過這裡最主要都是使用\",{\"1\":{\"73\":1}}],[\"不過這種情況下一個直覺的問題是\",{\"1\":{\"63\":1}}],[\"不過像是馬路\",{\"1\":{\"66\":1}}],[\"不過如果遇到新的\",{\"1\":{\"64\":1}}],[\"不過在計算\",{\"1\":{\"12\":1}}],[\"不過可惜的是\",{\"1\":{\"3\":1}}],[\"不同的地方在於他並不是直接去學習\",{\"1\":{\"171\":1}}],[\"不同的地方在於\",{\"1\":{\"113\":1}}],[\"不同的環境下需要的\",{\"1\":{\"6\":1}}],[\"不同\",{\"1\":{\"10\":1,\"13\":2,\"66\":1,\"91\":1,\"126\":1,\"195\":1}}],[\"不太好\",{\"1\":{\"3\":1,\"64\":1}}],[\"shc​−hd​\",{\"1\":{\"150\":1}}],[\"shift\",{\"1\":{\"63\":3,\"67\":2,\"71\":1}}],[\"s=2\",{\"1\":{\"150\":1}}],[\"s=1∏t​cs​\",{\"1\":{\"8\":1}}],[\"svhn\",{\"1\":{\"133\":2}}],[\"srivastava\",{\"1\":{\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"src\",{\"1\":{\"91\":1}}],[\"sce\",{\"1\":{\"195\":1}}],[\"science\",{\"1\":{\"186\":1}}],[\"scalable\",{\"1\":{\"182\":1}}],[\"scale\",{\"1\":{\"91\":1,\"151\":3,\"154\":1,\"159\":2,\"160\":1}}],[\"scorebaseline​\",{\"1\":{\"179\":1}}],[\"scorehuman​\",{\"1\":{\"179\":1}}],[\"scores\",{\"1\":{\"21\":2}}],[\"swc​−hw​\",{\"1\":{\"150\":1}}],[\"sw\",{\"1\":{\"78\":1,\"79\":1}}],[\"swear\",{\"1\":{\"3\":1}}],[\"skipping\",{\"1\":{\"116\":1}}],[\"skiiing\",{\"1\":{\"10\":1}}],[\"skiing\",{\"1\":{\"3\":5,\"24\":1}}],[\"sky\",{\"1\":{\"78\":1,\"93\":1}}],[\"sb​\",{\"1\":{\"68\":2}}],[\"symmetric\",{\"1\":{\"195\":1,\"204\":1}}],[\"synthia\",{\"0\":{\"77\":1,\"79\":1,\"202\":1},\"1\":{\"74\":2,\"79\":1,\"80\":1,\"96\":1,\"154\":1,\"155\":1,\"200\":1,\"202\":1}}],[\"synthetic\",{\"1\":{\"74\":1,\"76\":1,\"77\":1}}],[\"synethic\",{\"1\":{\"63\":1,\"64\":1,\"66\":1}}],[\"sylwia\",{\"1\":{\"67\":1}}],[\"ssl\",{\"1\":{\"67\":1,\"81\":1}}],[\"speech\",{\"0\":{\"134\":1}}],[\"spoiler\",{\"1\":{\"91\":1}}],[\"sparse\",{\"1\":{\"108\":1}}],[\"space\",{\"0\":{\"168\":1},\"1\":{\"82\":1,\"116\":2,\"167\":1,\"168\":8,\"174\":2,\"182\":1}}],[\"spatial\",{\"1\":{\"66\":2}}],[\"sprechmann\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"sgd\",{\"1\":{\"50\":1}}],[\"s代表pooling\",{\"1\":{\"44\":1}}],[\"salakhutdinov\",{\"1\":{\"123\":1}}],[\"sarsa\",{\"1\":{\"118\":2}}],[\"sa​\",{\"1\":{\"68\":3}}],[\"sampling\",{\"0\":{\"61\":1,\"71\":1,\"92\":1,\"99\":1},\"1\":{\"82\":1,\"91\":1,\"148\":1}}],[\"sample\",{\"1\":{\"10\":1,\"12\":2,\"13\":1,\"16\":1,\"19\":1,\"92\":2,\"99\":2,\"174\":1}}],[\"sampled到256×256的解析度\",{\"1\":{\"39\":1}}],[\"sampled\",{\"1\":{\"8\":1}}],[\"saturating的activation\",{\"1\":{\"36\":1}}],[\"saturating\",{\"0\":{\"36\":1},\"1\":{\"34\":1,\"41\":3,\"58\":1}}],[\"safe\",{\"1\":{\"28\":1}}],[\"success\",{\"1\":{\"192\":1,\"195\":1,\"196\":1}}],[\"supervise\",{\"1\":{\"67\":2}}],[\"supervised\",{\"0\":{\"197\":1},\"1\":{\"67\":1,\"82\":5,\"87\":2,\"91\":1,\"156\":2,\"187\":2},\"2\":{\"60\":1}}],[\"sutskever\",{\"1\":{\"32\":1,\"123\":1}}],[\"summary\",{\"0\":{\"25\":1,\"97\":1},\"1\":{\"100\":1}}],[\"surround\",{\"1\":{\"24\":1}}],[\"small\",{\"1\":{\"23\":1,\"93\":3,\"150\":2}}],[\"selection\",{\"0\":{\"157\":1}}],[\"self\",{\"0\":{\"67\":1,\"90\":1,\"197\":1},\"1\":{\"82\":1,\"88\":1,\"90\":2,\"91\":1,\"146\":1,\"148\":1,\"187\":1,\"188\":1,\"201\":1}}],[\"sex\",{\"1\":{\"124\":1}}],[\"seaquest\",{\"1\":{\"116\":1}}],[\"separable\",{\"1\":{\"101\":1}}],[\"separation\",{\"1\":{\"91\":1}}],[\"separate\",{\"1\":{\"22\":6,\"24\":1,\"25\":1,\"27\":1}}],[\"seed\",{\"1\":{\"92\":1,\"99\":3}}],[\"segformer\",{\"1\":{\"91\":3,\"98\":1,\"154\":1}}],[\"segmentation\",{\"0\":{\"85\":1,\"143\":1,\"185\":1},\"1\":{\"64\":1,\"66\":2,\"67\":1,\"68\":1,\"71\":1,\"73\":1,\"82\":5,\"86\":1,\"87\":3,\"88\":1,\"91\":2,\"93\":1,\"103\":2,\"145\":1,\"146\":1,\"149\":1,\"150\":5,\"161\":2,\"187\":1,\"190\":1,\"191\":2,\"195\":1,\"200\":2,\"203\":1,\"204\":1}}],[\"set\",{\"1\":{\"80\":5,\"117\":2}}],[\"settings\",{\"0\":{\"21\":1}}],[\"sematic\",{\"1\":{\"88\":1,\"146\":1}}],[\"semantic\",{\"0\":{\"85\":1,\"143\":1,\"185\":1},\"1\":{\"64\":1,\"66\":3,\"67\":1,\"68\":4,\"82\":4,\"86\":1,\"87\":3,\"91\":2,\"93\":1,\"103\":2,\"145\":1,\"150\":6,\"161\":2,\"187\":1,\"191\":2,\"195\":1,\"203\":1,\"204\":1}}],[\"semi\",{\"1\":{\"67\":2,\"82\":5,\"87\":1,\"187\":1}}],[\"sequence\",{\"1\":{\"12\":1}}],[\"sequences\",{\"1\":{\"8\":1}}],[\"simclrv2\",{\"1\":{\"197\":1,\"200\":1}}],[\"simple\",{\"0\":{\"122\":1},\"1\":{\"81\":1,\"182\":1}}],[\"simplified\",{\"0\":{\"16\":1}}],[\"single\",{\"1\":{\"176\":1}}],[\"si​\",{\"1\":{\"172\":2}}],[\"silver\",{\"1\":{\"107\":1,\"114\":1,\"117\":1,\"118\":1,\"170\":1}}],[\"sidewalk\",{\"1\":{\"70\":1}}],[\"sigmoid\",{\"1\":{\"58\":1}}],[\"size為128\",{\"1\":{\"50\":1}}],[\"size\",{\"0\":{\"17\":1,\"23\":1,\"138\":1,\"156\":1,\"157\":1},\"1\":{\"93\":1,\"138\":2,\"156\":4,\"157\":5,\"158\":1,\"160\":1}}],[\"sliding\",{\"0\":{\"15\":1,\"16\":1,\"152\":1},\"1\":{\"15\":1,\"16\":1,\"152\":1}}],[\"s\",{\"1\":{\"12\":2,\"39\":1,\"110\":4,\"114\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":2,\"152\":1,\"169\":4,\"171\":16,\"176\":8}}],[\"stage\",{\"1\":{\"192\":2}}],[\"state\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"6\":1,\"12\":4,\"19\":1,\"108\":1,\"110\":1,\"114\":3,\"117\":1,\"169\":1,\"171\":1,\"172\":1}}],[\"st+i​\",{\"1\":{\"172\":3,\"177\":4}}],[\"st+1​\",{\"1\":{\"114\":1}}],[\"study\",{\"0\":{\"159\":1}}],[\"student\",{\"1\":{\"90\":2,\"145\":1,\"148\":2,\"197\":2,\"198\":4}}],[\"stuff\",{\"1\":{\"93\":1}}],[\"structure\",{\"0\":{\"185\":1,\"196\":1},\"1\":{\"187\":1,\"198\":1,\"204\":1}}],[\"structured\",{\"1\":{\"82\":1}}],[\"stride\",{\"1\":{\"150\":1}}],[\"street\",{\"1\":{\"133\":1}}],[\"strong\",{\"1\":{\"82\":1}}],[\"strategies\",{\"0\":{\"85\":1},\"1\":{\"28\":1,\"91\":2,\"103\":1,\"182\":1}}],[\"stop\",{\"1\":{\"80\":1}}],[\"stephan\",{\"1\":{\"76\":1}}],[\"step\",{\"1\":{\"58\":2}}],[\"steps\",{\"1\":{\"22\":1}}],[\"steven\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"st​\",{\"1\":{\"7\":1,\"114\":1,\"148\":1}}],[\"soft\",{\"1\":{\"191\":1,\"192\":2,\"196\":1}}],[\"softmax\",{\"0\":{\"35\":1},\"1\":{\"34\":1,\"92\":1,\"191\":1,\"193\":2}}],[\"solution\",{\"1\":{\"180\":2}}],[\"solaris\",{\"1\":{\"3\":3,\"10\":1,\"23\":1,\"24\":1}}],[\"some\",{\"0\":{\"80\":1}}],[\"source\",{\"1\":{\"63\":3,\"64\":1,\"66\":2,\"67\":2,\"71\":3,\"78\":3,\"79\":1,\"87\":1,\"90\":3,\"91\":2,\"92\":1,\"100\":1,\"148\":5,\"151\":1,\"187\":1,\"190\":3,\"192\":2,\"197\":2,\"198\":3}}],[\"sourcing\",{\"1\":{\"39\":1}}],[\"sota\",{\"1\":{\"3\":1,\"81\":1,\"101\":3,\"132\":1,\"155\":3,\"160\":2,\"201\":1,\"202\":1,\"203\":1}}],[\"和一層\",{\"1\":{\"111\":1}}],[\"和1\",{\"1\":{\"42\":1}}],[\"和\",{\"1\":{\"3\":2,\"6\":1,\"10\":1,\"12\":3,\"15\":1,\"66\":1,\"67\":1,\"93\":1,\"99\":1,\"114\":1,\"129\":1,\"166\":1,\"171\":3,\"172\":1,\"175\":1,\"176\":2,\"190\":4,\"196\":1}}],[\"5​\",{\"1\":{\"177\":1}}],[\"55\",{\"1\":{\"80\":1,\"133\":1}}],[\"5595\",{\"1\":{\"24\":1}}],[\"53\",{\"1\":{\"80\":1}}],[\"5層convolution\",{\"1\":{\"42\":1,\"50\":1}}],[\"580\",{\"1\":{\"42\":1,\"50\":1}}],[\"5萬\",{\"1\":{\"39\":1}}],[\"50\",{\"1\":{\"24\":3,\"135\":1}}],[\"5\",{\"1\":{\"3\":1,\"42\":1,\"43\":1,\"44\":1,\"49\":1,\"52\":2,\"97\":2,\"133\":3,\"137\":1,\"155\":2}}],[\"52\",{\"1\":{\"3\":1}}],[\"51\",{\"1\":{\"3\":1}}],[\"57\",{\"1\":{\"3\":2,\"179\":1}}],[\"r−v\",{\"1\":{\"172\":3}}],[\"r=r\",{\"1\":{\"169\":1}}],[\"r+γq\",{\"1\":{\"170\":1,\"171\":1,\"176\":2}}],[\"r+γmaxb∈a​q\",{\"1\":{\"170\":1}}],[\"r+γb∈amax​q\",{\"1\":{\"169\":1,\"176\":2}}],[\"r+γa\",{\"1\":{\"110\":1}}],[\"rcv1\",{\"1\":{\"135\":1}}],[\"rcs\",{\"0\":{\"92\":1,\"99\":1},\"1\":{\"91\":1,\"97\":2,\"99\":3}}],[\"rj\",{\"1\":{\"129\":1}}],[\"ruslan\",{\"1\":{\"123\":1}}],[\"role\",{\"1\":{\"124\":1}}],[\"row\",{\"1\":{\"97\":4,\"156\":4,\"159\":4}}],[\"road\",{\"1\":{\"70\":1,\"78\":1,\"79\":1,\"93\":1}}],[\"rgb\",{\"0\":{\"48\":1},\"1\":{\"46\":1,\"119\":1}}],[\"raw\",{\"1\":{\"119\":1}}],[\"rare\",{\"0\":{\"92\":1,\"99\":1},\"1\":{\"91\":1,\"92\":3,\"97\":1,\"99\":1,\"148\":1}}],[\"rate\",{\"0\":{\"94\":1,\"98\":1},\"1\":{\"91\":1,\"94\":1,\"96\":1,\"98\":1,\"133\":3,\"134\":1,\"135\":1,\"137\":1,\"148\":1}}],[\"rate除以10\",{\"1\":{\"50\":1}}],[\"rate停滯時就將learning\",{\"1\":{\"50\":1}}],[\"rate都是一樣的\",{\"1\":{\"50\":1}}],[\"rate降低了超過1\",{\"1\":{\"48\":1}}],[\"rates差不到0\",{\"1\":{\"52\":1}}],[\"rates\",{\"1\":{\"52\":1,\"91\":1}}],[\"rates為validation\",{\"1\":{\"52\":1}}],[\"rates分別為37\",{\"1\":{\"52\":1}}],[\"rates分別下降了1\",{\"1\":{\"42\":1}}],[\"rates上\",{\"1\":{\"44\":1}}],[\"rates減少了1\",{\"1\":{\"43\":1}}],[\"randomized\",{\"1\":{\"182\":1}}],[\"randomness\",{\"1\":{\"166\":1}}],[\"random\",{\"0\":{\"175\":1},\"1\":{\"22\":1,\"92\":1,\"99\":3,\"124\":1,\"175\":4,\"176\":1,\"177\":1,\"179\":1}}],[\"r\",{\"1\":{\"12\":2,\"76\":1,\"169\":1,\"171\":1,\"176\":4}}],[\"rightarrow\",{\"0\":{\"201\":1,\"202\":1}}],[\"richter\",{\"1\":{\"76\":1}}],[\"rider\",{\"1\":{\"24\":1,\"70\":1,\"99\":1,\"116\":1}}],[\"ri\",{\"1\":{\"12\":3}}],[\"rtx\",{\"1\":{\"102\":1}}],[\"rt−1i​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt−1e​\",{\"1\":{\"9\":1,\"19\":1}}],[\"rt​=t\",{\"1\":{\"110\":1}}],[\"rt​∣st​=s\",{\"1\":{\"110\":1}}],[\"rt​\",{\"1\":{\"8\":1,\"110\":1,\"114\":1}}],[\"rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+\",{\"1\":{\"7\":1}}],[\"rti​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rti​=rtepisodic​⋅min\",{\"1\":{\"6\":1}}],[\"rte​\",{\"1\":{\"6\":1,\"10\":1,\"19\":1}}],[\"rtβi​​=rte​+βi​rti​\",{\"1\":{\"6\":1}}],[\"review\",{\"1\":{\"168\":1}}],[\"revenge\",{\"1\":{\"3\":2,\"24\":1}}],[\"reuters\",{\"1\":{\"135\":1}}],[\"recognition\",{\"0\":{\"134\":1}}],[\"recurrent\",{\"1\":{\"28\":2}}],[\"reply\",{\"1\":{\"172\":1}}],[\"replay\",{\"1\":{\"9\":1,\"17\":3,\"19\":3,\"23\":1,\"28\":2,\"114\":6,\"119\":1,\"169\":3,\"172\":1}}],[\"representation\",{\"1\":{\"118\":1,\"188\":1,\"204\":2}}],[\"regularize\",{\"1\":{\"93\":1}}],[\"regularization\",{\"1\":{\"34\":1,\"130\":1,\"166\":1,\"167\":1,\"168\":1,\"172\":1},\"2\":{\"142\":1}}],[\"real\",{\"1\":{\"64\":1,\"66\":1,\"74\":1}}],[\"read\",{\"2\":{\"30\":1,\"60\":1,\"84\":1,\"105\":1,\"121\":1,\"142\":1,\"163\":1,\"184\":1,\"206\":1}}],[\"references\",{\"0\":{\"58\":1}}],[\"reflections\",{\"1\":{\"47\":2}}],[\"reflection\",{\"0\":{\"47\":1},\"1\":{\"46\":1}}],[\"rel=oracleuda​\",{\"1\":{\"91\":1}}],[\"rel\",{\"1\":{\"91\":1}}],[\"release\",{\"1\":{\"62\":1}}],[\"relu\",{\"0\":{\"41\":1},\"1\":{\"58\":1}}],[\"related\",{\"0\":{\"4\":1,\"34\":1,\"65\":1,\"88\":1,\"109\":1,\"125\":1,\"146\":1,\"167\":1,\"188\":1}}],[\"research\",{\"1\":{\"186\":1}}],[\"researchgate\",{\"1\":{\"100\":1,\"101\":1}}],[\"resolution\",{\"0\":{\"143\":1,\"151\":1,\"156\":1},\"1\":{\"145\":3,\"149\":4,\"150\":7,\"151\":2,\"156\":1,\"157\":3,\"158\":1,\"160\":1,\"161\":2}}],[\"resolution圖像\",{\"1\":{\"39\":1}}],[\"result\",{\"0\":{\"133\":1,\"134\":1,\"135\":1}}],[\"results\",{\"0\":{\"20\":1,\"51\":1,\"72\":1,\"95\":1,\"115\":1,\"131\":1,\"153\":1,\"178\":1,\"199\":1}}],[\"residual\",{\"1\":{\"109\":1}}],[\"resnet101\",{\"1\":{\"73\":2,\"154\":1}}],[\"resnet\",{\"1\":{\"43\":1,\"78\":1,\"87\":1,\"200\":2}}],[\"response\",{\"0\":{\"43\":1},\"1\":{\"43\":1,\"58\":1}}],[\"reduce\",{\"0\":{\"45\":1},\"1\":{\"38\":1}}],[\"reinforcement\",{\"0\":{\"106\":1},\"1\":{\"28\":3,\"109\":1,\"119\":1,\"182\":3},\"2\":{\"30\":1,\"121\":1,\"184\":1}}],[\"returns\",{\"1\":{\"21\":1}}],[\"return\",{\"1\":{\"14\":8,\"15\":1,\"22\":2,\"24\":1,\"110\":3,\"171\":1,\"172\":1}}],[\"retrace\",{\"1\":{\"8\":6,\"12\":2,\"24\":1}}],[\"re\",{\"1\":{\"12\":3,\"92\":1}}],[\"reward\",{\"0\":{\"6\":1},\"1\":{\"3\":7,\"5\":1,\"6\":10,\"8\":1,\"9\":2,\"12\":7,\"13\":2,\"14\":1,\"15\":2,\"19\":2,\"22\":6,\"108\":2,\"110\":2,\"114\":1,\"116\":4,\"117\":2,\"169\":1}}],[\"r2d2+sep\",{\"1\":{\"24\":1}}],[\"r2d2\",{\"1\":{\"3\":3,\"9\":1,\"17\":1,\"23\":1,\"24\":2,\"25\":3}}],[\"rl\",{\"0\":{\"8\":1},\"1\":{\"3\":4,\"5\":2,\"6\":1,\"8\":1,\"108\":8,\"110\":4,\"111\":1,\"113\":1,\"114\":2,\"117\":1,\"119\":1,\"166\":1,\"167\":1,\"172\":1,\"181\":1}}],[\"例如下圖\",{\"1\":{\"187\":1}}],[\"例如說\",{\"1\":{\"148\":1}}],[\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的\",{\"1\":{\"67\":1}}],[\"例如第2\",{\"1\":{\"42\":1}}],[\"例如在\",{\"1\":{\"10\":1,\"166\":1}}],[\"例如\",{\"1\":{\"3\":1,\"166\":1}}],[\"guez\",{\"1\":{\"170\":1}}],[\"go\",{\"1\":{\"166\":1}}],[\"gool\",{\"1\":{\"86\":2,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":2,\"101\":1,\"144\":1,\"145\":1,\"149\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2}}],[\"google\",{\"1\":{\"2\":1,\"165\":1}}],[\"gheshlaghi\",{\"1\":{\"165\":1,\"179\":3,\"180\":1}}],[\"gϕ​\",{\"1\":{\"148\":2}}],[\"github\",{\"1\":{\"103\":1}}],[\"give\",{\"0\":{\"5\":1},\"1\":{\"5\":1,\"28\":1}}],[\"gta\",{\"1\":{\"91\":2}}],[\"gta5\",{\"0\":{\"76\":1,\"78\":1,\"201\":1},\"1\":{\"74\":2,\"76\":1,\"80\":1,\"81\":1,\"96\":1,\"154\":1,\"155\":1,\"156\":1,\"200\":1,\"201\":1}}],[\"gθ​\",{\"1\":{\"90\":1}}],[\"gpu\",{\"1\":{\"50\":1,\"102\":1,\"145\":2,\"149\":1}}],[\"gpu只有3gb的記憶體\",{\"1\":{\"42\":1}}],[\"gpus\",{\"0\":{\"42\":1}}],[\"geist\",{\"1\":{\"166\":1}}],[\"get\",{\"1\":{\"150\":1}}],[\"germanros\",{\"1\":{\"77\":1}}],[\"generation\",{\"0\":{\"152\":1}}],[\"generator\",{\"1\":{\"66\":1}}],[\"general\",{\"1\":{\"10\":1,\"16\":1,\"21\":1,\"25\":1}}],[\"geoffrey\",{\"1\":{\"32\":1,\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"gradient的問題\",{\"1\":{\"56\":1}}],[\"gradient\",{\"1\":{\"50\":1,\"58\":3,\"112\":1}}],[\"gradient問題\",{\"1\":{\"41\":1}}],[\"gridworld\",{\"1\":{\"22\":1}}],[\"greedy\",{\"1\":{\"13\":1,\"16\":1,\"19\":1,\"114\":1,\"166\":1,\"167\":1,\"168\":2,\"169\":1,\"171\":1,\"176\":1}}],[\"g\",{\"1\":{\"21\":1,\"191\":1}}],[\"gap\",{\"1\":{\"187\":2}}],[\"gaussian\",{\"1\":{\"90\":1,\"168\":1,\"174\":1,\"175\":2,\"176\":1,\"177\":3}}],[\"gan\",{\"1\":{\"66\":1}}],[\"gammon\",{\"0\":{\"111\":1},\"1\":{\"109\":1,\"114\":1}}],[\"gamma\",{\"1\":{\"24\":2}}],[\"gamer\",{\"1\":{\"3\":1}}],[\"game\",{\"1\":{\"3\":2}}],[\"games\",{\"1\":{\"3\":3,\"24\":1,\"25\":3,\"27\":1,\"116\":1,\"179\":1}}],[\"gate\",{\"1\":{\"3\":1}}],[\"gates\",{\"1\":{\"3\":1}}],[\"在絕大多數的類別當中也是比起過去的做法還要強\",{\"1\":{\"201\":1}}],[\"在絕大多數並非是最佳的結果上都不會離最佳太遠\",{\"1\":{\"78\":1,\"79\":1}}],[\"在兩個\",{\"1\":{\"200\":1}}],[\"在那些充滿噪點的\",{\"1\":{\"192\":1}}],[\"在實務上為了避免像是\",{\"1\":{\"171\":1}}],[\"在實作上會包含\",{\"1\":{\"148\":1}}],[\"在過往的研究可以發現到說往往我們在設計讓\",{\"1\":{\"168\":1}}],[\"在過去的\",{\"1\":{\"166\":1}}],[\"在過去的方法上由於\",{\"1\":{\"145\":1}}],[\"在相同量級或甚至更小量級的記憶體使用量下\",{\"1\":{\"160\":1}}],[\"在相同的\",{\"1\":{\"133\":1}}],[\"在細節上的處理比起過去的\",{\"1\":{\"155\":1}}],[\"在部分與其他\",{\"1\":{\"154\":1}}],[\"在一個\",{\"1\":{\"192\":1}}],[\"在一個4層的cnn上\",{\"1\":{\"41\":1}}],[\"在一起\",{\"1\":{\"151\":1}}],[\"在選擇上會是在圖片範圍當中的\",{\"1\":{\"150\":1}}],[\"在各種狀況下給出的效益\",{\"1\":{\"139\":1}}],[\"在同樣的網路架構下\",{\"1\":{\"138\":1}}],[\"在做的事情可以視為是在\",{\"1\":{\"126\":1}}],[\"在有性生殖的過程當中會融合雙親的基因\",{\"1\":{\"124\":1}}],[\"在近年來發現到\",{\"1\":{\"124\":1}}],[\"在幾乎所有的遊戲當中都\",{\"1\":{\"118\":1}}],[\"在經過\",{\"1\":{\"114\":1}}],[\"在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示\",{\"1\":{\"100\":1}}],[\"在上圖的橘色線是原本的模型隨著訓練後對於不同\",{\"1\":{\"100\":1}}],[\"在沒有使用\",{\"1\":{\"99\":1}}],[\"在加上不同的調整後得出的結果\",{\"1\":{\"97\":1}}],[\"在訓練過程中不被考慮到\",{\"1\":{\"129\":1}}],[\"在訓練後期才出現\",{\"1\":{\"92\":1}}],[\"在訓練時間上也有些微的縮短\",{\"1\":{\"42\":1}}],[\"在這裡採用\",{\"1\":{\"176\":1,\"177\":1}}],[\"在這裡能夠提供更好的幫助\",{\"1\":{\"91\":1}}],[\"在這一篇\",{\"1\":{\"159\":1}}],[\"在這一篇論文當中主要探討的是過去\",{\"1\":{\"87\":1}}],[\"在這邊我們在意的是評估的部分\",{\"1\":{\"8\":1}}],[\"在邊界上往往會出現誤差的問題解決\",{\"1\":{\"68\":1}}],[\"在讀這篇alexnet的時候\",{\"1\":{\"57\":1}}],[\"在讀paper的時候會發現\",{\"1\":{\"57\":1}}],[\"在當時這些技巧應該都是非常新穎的概念\",{\"1\":{\"57\":1}}],[\"在top\",{\"1\":{\"44\":1}}],[\"在cifar\",{\"1\":{\"41\":1}}],[\"在backpropagation的過程中\",{\"1\":{\"37\":1}}],[\"在fully\",{\"1\":{\"35\":1}}],[\"在限定幾款遊戲有特別出色的成效\",{\"1\":{\"25\":1}}],[\"在所有的\",{\"1\":{\"156\":1}}],[\"在所有\",{\"1\":{\"25\":1}}],[\"在搭配了\",{\"1\":{\"24\":1}}],[\"在下表當中可以看到\",{\"1\":{\"24\":1}}],[\"在最傾向\",{\"1\":{\"22\":1}}],[\"在不同選擇下的結果\",{\"1\":{\"137\":1}}],[\"在不同\",{\"1\":{\"22\":1,\"92\":1}}],[\"在每個\",{\"1\":{\"22\":1,\"24\":1}}],[\"在每一個\",{\"1\":{\"13\":1,\"174\":1}}],[\"在時間\",{\"1\":{\"14\":1,\"114\":1,\"172\":1}}],[\"在計算上分別都只會拿自己的\",{\"1\":{\"12\":1}}],[\"在目標的\",{\"1\":{\"8\":1}}],[\"在整個訓練過程當中沒有踏足過的狀態\",{\"1\":{\"6\":1}}],[\"在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響\",{\"1\":{\"3\":1}}],[\"在剩下的遊戲當中這些\",{\"1\":{\"3\":1}}],[\"在\",{\"1\":{\"3\":1,\"6\":2,\"14\":1,\"17\":1,\"21\":1,\"22\":1,\"23\":2,\"24\":2,\"63\":1,\"66\":1,\"67\":1,\"68\":2,\"70\":1,\"71\":1,\"73\":2,\"74\":1,\"86\":1,\"87\":1,\"90\":1,\"91\":1,\"92\":1,\"96\":2,\"101\":1,\"108\":1,\"110\":1,\"114\":1,\"117\":1,\"133\":1,\"151\":1,\"154\":2,\"155\":2,\"156\":1,\"157\":1,\"170\":1,\"172\":1,\"175\":1,\"192\":1,\"195\":1,\"201\":1,\"202\":1,\"203\":1}}],[\"iclr\",{\"1\":{\"165\":1,\"168\":1},\"2\":{\"184\":1}}],[\"icml\",{\"1\":{\"2\":1},\"2\":{\"30\":1}}],[\"i−obi\",{\"1\":{\"151\":1}}],[\"ijc\",{\"1\":{\"148\":1}}],[\"ijct​=\",{\"1\":{\"148\":1}}],[\"ijc​​\",{\"1\":{\"148\":1}}],[\"iou\",{\"1\":{\"99\":4,\"100\":1}}],[\"iverson\",{\"1\":{\"90\":1,\"148\":1}}],[\"i=0∑k​eπ\",{\"1\":{\"177\":1}}],[\"i=0∑k​∇ζπ​​log\",{\"1\":{\"177\":1}}],[\"i=0∑k​∇θπ​​log\",{\"1\":{\"172\":1,\"177\":1}}],[\"i=1nt​​\",{\"1\":{\"90\":2}}],[\"i=1ns​​\",{\"1\":{\"90\":2}}],[\"i=t+1∏s​ci​\",{\"1\":{\"8\":1}}],[\"issue\",{\"1\":{\"196\":1}}],[\"issues\",{\"0\":{\"80\":1}}],[\"is\",{\"0\":{\"63\":1}}],[\"ixy​=​irxy​​igxy​​ibxy​​​​\",{\"1\":{\"48\":1}}],[\"i\",{\"1\":{\"43\":1,\"90\":17,\"92\":1,\"93\":14,\"129\":2,\"151\":3,\"172\":1,\"191\":7,\"192\":7,\"193\":6,\"194\":3,\"195\":3,\"196\":7}}],[\"ilya\",{\"1\":{\"32\":1,\"123\":1}}],[\"improving\",{\"0\":{\"85\":1},\"1\":{\"91\":1,\"103\":2}}],[\"improvement\",{\"1\":{\"24\":1,\"179\":2}}],[\"images\",{\"1\":{\"75\":1,\"76\":1,\"77\":1}}],[\"imagenet包含了不同解析度的圖像\",{\"1\":{\"39\":1}}],[\"imagenet\",{\"0\":{\"31\":1,\"93\":1,\"100\":1},\"1\":{\"39\":1,\"58\":1,\"73\":1,\"91\":1,\"93\":4,\"96\":1,\"133\":3,\"148\":1}}],[\"image\",{\"0\":{\"47\":1,\"133\":1},\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":2,\"22\":3,\"23\":2,\"24\":2,\"25\":2,\"46\":1,\"48\":1,\"63\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":5,\"70\":2,\"71\":2,\"75\":1,\"76\":1,\"77\":1,\"78\":2,\"79\":1,\"86\":1,\"88\":1,\"91\":5,\"97\":2,\"98\":1,\"99\":1,\"100\":3,\"101\":2,\"114\":2,\"117\":2,\"118\":2,\"126\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1,\"145\":1,\"146\":1,\"148\":1,\"149\":2,\"150\":1,\"151\":1,\"152\":2,\"155\":3,\"156\":2,\"157\":2,\"158\":1,\"159\":2,\"168\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"179\":3,\"180\":1,\"187\":1,\"192\":1,\"201\":2,\"202\":1}}],[\"initialize\",{\"1\":{\"177\":1}}],[\"independent\",{\"1\":{\"175\":1,\"177\":2}}],[\"influence\",{\"0\":{\"156\":1}}],[\"informatics\",{\"1\":{\"86\":1,\"144\":1}}],[\"information\",{\"0\":{\"2\":1,\"32\":1,\"62\":1,\"86\":1,\"107\":1,\"123\":1,\"144\":1,\"165\":1,\"186\":1},\"1\":{\"91\":1,\"182\":1}}],[\"info\",{\"1\":{\"3\":1,\"42\":1,\"43\":1,\"64\":1,\"66\":1,\"67\":4,\"70\":1,\"78\":1,\"79\":1,\"90\":1,\"114\":1,\"135\":1,\"136\":1,\"145\":1,\"148\":1,\"166\":1,\"168\":1,\"171\":1,\"172\":1}}],[\"invaders\",{\"1\":{\"116\":2}}],[\"input\",{\"1\":{\"113\":1,\"126\":1,\"129\":1,\"177\":2}}],[\"introduce\",{\"1\":{\"81\":1}}],[\"intrinsic\",{\"0\":{\"6\":1},\"1\":{\"5\":1,\"6\":4,\"9\":1,\"12\":3,\"19\":1,\"22\":3}}],[\"intensities\",{\"0\":{\"48\":1},\"1\":{\"46\":1}}],[\"in\",{\"1\":{\"28\":2,\"81\":1,\"82\":1,\"124\":1}}],[\"py​\",{\"1\":{\"197\":1,\"198\":1}}],[\"psuedo\",{\"1\":{\"203\":1}}],[\"ps​\",{\"1\":{\"197\":1,\"198\":1}}],[\"pseudo\",{\"0\":{\"67\":1,\"152\":1,\"185\":1,\"192\":1},\"1\":{\"67\":3,\"68\":1,\"70\":1,\"71\":1,\"90\":5,\"100\":1,\"148\":2,\"151\":1,\"152\":1,\"187\":3,\"190\":1,\"191\":1,\"192\":5,\"195\":3,\"196\":2,\"198\":1,\"204\":1}}],[\"p+q\",{\"1\":{\"175\":1}}],[\"pq+q\",{\"1\":{\"175\":1}}],[\"pdt​\",{\"1\":{\"151\":1}}],[\"pc\",{\"1\":{\"151\":1}}],[\"pca\",{\"1\":{\"58\":2}}],[\"plr\",{\"1\":{\"148\":1}}],[\"plrt​\",{\"1\":{\"148\":2}}],[\"playing\",{\"0\":{\"106\":1}}],[\"p=0\",{\"1\":{\"137\":1}}],[\"pn\",{\"1\":{\"137\":2}}],[\"p\",{\"1\":{\"90\":1,\"92\":2,\"128\":3,\"129\":4,\"130\":2,\"133\":1,\"137\":1,\"148\":1,\"177\":2}}],[\"pt​∥pt\",{\"1\":{\"197\":1}}],[\"pt​\",{\"1\":{\"191\":1,\"195\":2,\"197\":1}}],[\"pt\",{\"1\":{\"90\":2,\"191\":2,\"192\":2,\"197\":1}}],[\"ptifall\",{\"1\":{\"3\":1}}],[\"p1​​p2​​p3​​\",{\"1\":{\"48\":1}}],[\"preliminary\",{\"0\":{\"148\":1,\"190\":1}}],[\"prevent\",{\"0\":{\"122\":1}}],[\"preprocess\",{\"1\":{\"114\":1}}],[\"pretraining\",{\"1\":{\"130\":1}}],[\"pretrain\",{\"1\":{\"96\":1,\"197\":1}}],[\"pretrained\",{\"1\":{\"73\":1,\"93\":1,\"130\":2,\"200\":1}}],[\"prediction\",{\"1\":{\"67\":2}}],[\"prototypes\",{\"1\":{\"203\":1}}],[\"prototype\",{\"0\":{\"194\":1},\"1\":{\"192\":4,\"194\":3,\"195\":1,\"196\":1}}],[\"prototypical\",{\"0\":{\"185\":1,\"192\":1},\"1\":{\"196\":1,\"198\":1,\"204\":1}}],[\"probability\",{\"1\":{\"169\":1,\"191\":1}}],[\"problem\",{\"1\":{\"58\":2}}],[\"proda\",{\"1\":{\"90\":1,\"96\":1,\"196\":1,\"198\":2,\"200\":1,\"203\":1,\"204\":1}}],[\"progress\",{\"1\":{\"28\":1}}],[\"principal\",{\"1\":{\"58\":1}}],[\"prioritized\",{\"1\":{\"28\":1}}],[\"private\",{\"1\":{\"3\":2,\"24\":1}}],[\"posts\",{\"0\":{\"207\":1}}],[\"positional\",{\"1\":{\"152\":1}}],[\"positive\",{\"1\":{\"3\":3,\"116\":1}}],[\"pole\",{\"1\":{\"77\":1}}],[\"policies\",{\"0\":{\"13\":1}}],[\"policy有什么区别\",{\"1\":{\"182\":1}}],[\"policy\",{\"1\":{\"8\":3,\"10\":2,\"12\":4,\"13\":5,\"16\":1,\"24\":1,\"27\":1,\"28\":1,\"108\":1,\"110\":2,\"112\":2,\"114\":1,\"117\":1,\"118\":1,\"166\":1,\"168\":1,\"172\":6,\"181\":2,\"182\":1}}],[\"pooling的models較不容易發生overfitting的情形\",{\"1\":{\"44\":1}}],[\"pooling為non\",{\"1\":{\"44\":1}}],[\"pooling\",{\"0\":{\"44\":1},\"1\":{\"44\":4,\"93\":1}}],[\"pong\",{\"1\":{\"24\":1,\"116\":1}}],[\"pan\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"part\",{\"1\":{\"182\":1}}],[\"parameters\",{\"1\":{\"43\":1,\"124\":1}}],[\"parameterization\",{\"0\":{\"12\":1,\"22\":1}}],[\"parameter\",{\"0\":{\"168\":1},\"1\":{\"8\":1,\"167\":1,\"168\":7,\"174\":2,\"182\":2}}],[\"pablo\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":2}}],[\"paper\",{\"1\":{\"3\":2,\"67\":1,\"79\":1,\"80\":1,\"87\":1,\"91\":1,\"96\":1,\"103\":1,\"113\":1,\"129\":1,\"145\":1,\"159\":1,\"168\":1,\"187\":1},\"2\":{\"30\":1,\"60\":1,\"84\":1,\"105\":1,\"121\":1,\"142\":1,\"163\":1,\"184\":1,\"206\":1}}],[\"perceptron\",{\"1\":{\"111\":1}}],[\"perturbations\",{\"1\":{\"82\":1}}],[\"person\",{\"1\":{\"70\":1,\"78\":1}}],[\"period\",{\"1\":{\"17\":2,\"23\":1}}],[\"per\",{\"1\":{\"6\":1}}],[\"performance\",{\"1\":{\"3\":2,\"21\":1,\"22\":1,\"23\":1,\"78\":2,\"79\":1,\"81\":1,\"90\":1,\"91\":1,\"92\":1,\"97\":4,\"98\":1,\"101\":1,\"102\":1,\"117\":1,\"155\":1,\"158\":1,\"159\":1,\"160\":1,\"187\":1}}],[\"penalty\",{\"1\":{\"3\":1}}],[\"pietquin\",{\"1\":{\"166\":1}}],[\"pixels\",{\"1\":{\"100\":2}}],[\"pixels做一次pooling\",{\"1\":{\"44\":1}}],[\"pixel的3×3\",{\"1\":{\"48\":1}}],[\"pixel\",{\"1\":{\"48\":1,\"66\":1,\"93\":1,\"118\":1,\"191\":1}}],[\"pitfall\",{\"1\":{\"3\":3,\"24\":1}}],[\"piot\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"165\":1,\"179\":3,\"180\":1}}],[\"puigdomènech\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"b∈rq\",{\"1\":{\"175\":1}}],[\"bd\",{\"1\":{\"150\":6}}],[\"bc​\",{\"1\":{\"150\":1}}],[\"bc\",{\"1\":{\"150\":6}}],[\"breakout\",{\"1\":{\"116\":1}}],[\"bracket\",{\"1\":{\"90\":1,\"148\":1}}],[\"bo\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"box\",{\"1\":{\"150\":1}}],[\"bottleneck\",{\"1\":{\"93\":1}}],[\"bounding\",{\"1\":{\"150\":1}}],[\"bound\",{\"0\":{\"14\":1}}],[\"b5\",{\"1\":{\"91\":2,\"96\":1,\"100\":1,\"154\":1}}],[\"blur\",{\"1\":{\"90\":1}}],[\"bilinearly\",{\"1\":{\"148\":1}}],[\"bilal\",{\"1\":{\"2\":1,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"165\":1,\"179\":3,\"180\":1}}],[\"bicycle\",{\"1\":{\"99\":2}}],[\"biases\",{\"1\":{\"129\":1}}],[\"bias\",{\"1\":{\"92\":1,\"175\":1}}],[\"binary\",{\"1\":{\"68\":1,\"73\":1}}],[\"by\",{\"0\":{\"196\":1},\"1\":{\"58\":1}}],[\"bn\",{\"1\":{\"43\":1}}],[\"bx\",{\"1\":{\"43\":1}}],[\"better\",{\"1\":{\"168\":2,\"182\":1}}],[\"between\",{\"1\":{\"58\":1}}],[\"bert\",{\"1\":{\"116\":1}}],[\"beat\",{\"1\":{\"81\":1}}],[\"beam\",{\"1\":{\"24\":1,\"116\":1}}],[\"best\",{\"1\":{\"80\":3,\"118\":1}}],[\"behaviour\",{\"1\":{\"28\":1}}],[\"benifit\",{\"1\":{\"24\":1}}],[\"benchmarks\",{\"1\":{\"74\":1,\"200\":1}}],[\"benchmark\",{\"0\":{\"1\":1},\"1\":{\"3\":1,\"28\":1}}],[\"bus\",{\"1\":{\"93\":1}}],[\"build\",{\"1\":{\"78\":1}}],[\"budden\",{\"1\":{\"19\":1}}],[\"buffer\",{\"1\":{\"9\":1,\"17\":1,\"19\":3,\"169\":2,\"172\":1}}],[\"b\",{\"1\":{\"12\":3,\"68\":2,\"129\":1,\"169\":1,\"170\":2,\"171\":1,\"176\":4}}],[\"balanced\",{\"1\":{\"82\":1}}],[\"baseline\",{\"1\":{\"80\":1,\"118\":1,\"133\":3,\"135\":1,\"158\":1,\"171\":1,\"179\":3}}],[\"based\",{\"1\":{\"73\":1,\"82\":1,\"91\":3,\"152\":1,\"182\":1}}],[\"basic\",{\"0\":{\"2\":1,\"32\":1,\"62\":1,\"86\":1,\"107\":1,\"123\":1,\"144\":1,\"165\":1,\"186\":1}}],[\"backbone\",{\"1\":{\"73\":1,\"78\":1,\"87\":3,\"91\":5,\"93\":1,\"102\":1,\"154\":1,\"200\":2}}],[\"backpropagation\",{\"1\":{\"130\":1,\"152\":1}}],[\"backprop\",{\"0\":{\"17\":1,\"23\":1}}],[\"bandit\",{\"0\":{\"13\":1},\"1\":{\"14\":1,\"16\":1,\"24\":3,\"25\":1}}],[\"batch\",{\"1\":{\"12\":4,\"50\":1,\"58\":1,\"194\":2}}],[\"badia\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"ai​∣si​\",{\"1\":{\"172\":1}}],[\"a3c\",{\"0\":{\"172\":1,\"177\":1},\"1\":{\"167\":1,\"168\":1,\"172\":7,\"177\":3,\"179\":2,\"182\":1}}],[\"azar\",{\"1\":{\"165\":1,\"179\":3,\"180\":1}}],[\"ablation\",{\"0\":{\"159\":1}}],[\"about\",{\"0\":{\"0\":1,\"80\":1}}],[\"ac\",{\"1\":{\"151\":3}}],[\"ac​=σ\",{\"1\":{\"151\":1}}],[\"ac​\",{\"1\":{\"151\":2}}],[\"activation\",{\"1\":{\"129\":1,\"136\":2}}],[\"action\",{\"0\":{\"12\":1,\"22\":1},\"1\":{\"3\":1,\"7\":1,\"8\":1,\"9\":1,\"12\":4,\"13\":1,\"14\":3,\"15\":1,\"16\":1,\"19\":2,\"108\":1,\"110\":2,\"114\":2,\"116\":2,\"168\":5,\"169\":4,\"171\":3}}],[\"actvation\",{\"1\":{\"58\":1}}],[\"actors\",{\"0\":{\"19\":1},\"1\":{\"19\":1}}],[\"actor\",{\"1\":{\"9\":1,\"13\":13,\"16\":1,\"24\":1,\"27\":1,\"172\":1,\"182\":1}}],[\"aware\",{\"0\":{\"143\":1},\"1\":{\"161\":2}}],[\"autoencoder\",{\"1\":{\"126\":1,\"136\":1,\"140\":1}}],[\"autoencoders\",{\"0\":{\"126\":1}}],[\"augumented\",{\"1\":{\"90\":1}}],[\"augumentation\",{\"1\":{\"68\":2,\"196\":2}}],[\"augmentation\",{\"0\":{\"46\":1},\"1\":{\"45\":1,\"46\":1,\"56\":1,\"82\":1}}],[\"a∼ρ\",{\"1\":{\"110\":1}}],[\"avgpool\",{\"1\":{\"93\":1}}],[\"average\",{\"1\":{\"24\":2,\"27\":1,\"90\":1,\"93\":1,\"148\":1,\"159\":1,\"194\":2}}],[\"analysis\",{\"0\":{\"180\":1},\"1\":{\"58\":1}}],[\"and\",{\"0\":{\"47\":1,\"85\":1,\"150\":1,\"156\":1,\"185\":1},\"1\":{\"28\":1,\"46\":1,\"58\":1,\"91\":1,\"103\":1,\"186\":1,\"204\":1}}],[\"amazon\",{\"1\":{\"39\":1}}],[\"am​=a\",{\"1\":{\"14\":2,\"15\":2}}],[\"approximation\",{\"1\":{\"109\":1,\"174\":1}}],[\"approximators\",{\"1\":{\"112\":1}}],[\"approximator\",{\"1\":{\"7\":1,\"112\":2,\"118\":1}}],[\"apply\",{\"1\":{\"63\":1,\"68\":2,\"81\":1}}],[\"applications\",{\"1\":{\"62\":1}}],[\"appendix\",{\"1\":{\"21\":1,\"198\":1}}],[\"argb∈amax​q\",{\"1\":{\"171\":1,\"176\":2}}],[\"arthur\",{\"1\":{\"170\":1,\"172\":1}}],[\"architectures\",{\"0\":{\"85\":1},\"1\":{\"91\":1,\"103\":2}}],[\"architecture\",{\"0\":{\"18\":1,\"40\":1,\"91\":1},\"1\":{\"38\":1}}],[\"arm\",{\"1\":{\"14\":1}}],[\"ak​=⎩⎨⎧​kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"16\":1}}],[\"ak​=\",{\"1\":{\"14\":1,\"15\":1}}],[\"ak​\",{\"1\":{\"14\":2}}],[\"advantage\",{\"1\":{\"171\":2,\"172\":2}}],[\"adversarial\",{\"1\":{\"66\":1,\"67\":1,\"82\":1}}],[\"adaptsegnet\",{\"1\":{\"200\":1}}],[\"adapt\",{\"1\":{\"82\":1,\"91\":1,\"148\":1}}],[\"adaptation\",{\"0\":{\"61\":1},\"1\":{\"82\":4,\"88\":1,\"136\":1,\"146\":1}}],[\"adaptations\",{\"1\":{\"49\":1,\"136\":1}}],[\"adaption\",{\"0\":{\"63\":1,\"71\":1},\"1\":{\"63\":2,\"67\":1,\"187\":1,\"196\":1},\"2\":{\"84\":1,\"105\":1,\"163\":1,\"206\":1}}],[\"adapting太多的問題\",{\"1\":{\"37\":1}}],[\"adapting\",{\"1\":{\"28\":1}}],[\"adaptive\",{\"0\":{\"13\":1,\"24\":1,\"85\":1,\"143\":1,\"185\":1},\"1\":{\"91\":1,\"103\":2,\"161\":2,\"204\":1}}],[\"adrià\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2}}],[\"asia\",{\"1\":{\"186\":1}}],[\"as\",{\"1\":{\"182\":1}}],[\"asynchronous\",{\"1\":{\"182\":2}}],[\"asb​\",{\"1\":{\"12\":3}}],[\"as​∣xs​\",{\"1\":{\"8\":2}}],[\"assignment\",{\"1\":{\"3\":1,\"10\":1,\"17\":2,\"196\":2}}],[\"a\",{\"0\":{\"13\":1,\"122\":1},\"1\":{\"8\":4,\"12\":5,\"14\":10,\"15\":8,\"16\":2,\"58\":1,\"68\":2,\"81\":1,\"110\":4,\"169\":5,\"171\":18,\"172\":3,\"176\":8,\"177\":2,\"182\":1,\"187\":1,\"192\":2}}],[\"a∣xt+1​\",{\"1\":{\"8\":2}}],[\"at+i​\",{\"1\":{\"172\":1,\"177\":2}}],[\"at+i​∣st+i​\",{\"1\":{\"172\":1,\"177\":2}}],[\"attention\",{\"1\":{\"91\":1,\"149\":1,\"151\":10,\"154\":1,\"159\":2,\"160\":1}}],[\"at\",{\"1\":{\"70\":2,\"71\":2,\"78\":2,\"79\":1}}],[\"at−1​\",{\"1\":{\"9\":1,\"19\":1}}],[\"at​=a\",{\"1\":{\"110\":1}}],[\"at​\",{\"1\":{\"7\":2,\"8\":8,\"114\":1}}],[\"atari\",{\"0\":{\"1\":1,\"106\":1},\"1\":{\"3\":5,\"25\":3,\"27\":1,\"28\":1,\"116\":1,\"179\":1}}],[\"agents\",{\"1\":{\"182\":1}}],[\"agent\",{\"0\":{\"9\":1},\"1\":{\"3\":1,\"5\":2,\"6\":3,\"15\":1,\"21\":1,\"22\":4,\"108\":1,\"114\":2,\"116\":1,\"117\":2,\"166\":1,\"168\":1,\"172\":1}}],[\"agent57\",{\"0\":{\"1\":1},\"1\":{\"10\":1,\"12\":2,\"13\":3,\"14\":1,\"16\":1,\"21\":1,\"22\":1,\"23\":1,\"25\":5,\"28\":2}}],[\"alternative\",{\"1\":{\"182\":1}}],[\"altering\",{\"0\":{\"48\":1},\"1\":{\"46\":1}}],[\"alogorithm\",{\"1\":{\"172\":1}}],[\"alpha\",{\"1\":{\"166\":1}}],[\"alan\",{\"1\":{\"161\":1}}],[\"alignment\",{\"0\":{\"66\":1},\"1\":{\"66\":1,\"201\":1}}],[\"alex\",{\"1\":{\"32\":1,\"123\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1}}],[\"alexnet將sigmoid替換成了relu\",{\"1\":{\"56\":1}}],[\"alexnet將dropout的機率p=0\",{\"1\":{\"49\":1}}],[\"alexnet的研究表明了一個大型的深度cnn是能夠透過只用supervised\",{\"1\":{\"55\":1}}],[\"alexnet的表現都非常出色\",{\"1\":{\"53\":1}}],[\"alexnet的架構如圖所示\",{\"1\":{\"40\":1}}],[\"alexnet共使用兩張gtx\",{\"1\":{\"50\":1}}],[\"alexnet使用stochastic\",{\"1\":{\"50\":1}}],[\"alexnet在fully\",{\"1\":{\"49\":1}}],[\"alexnet選擇讓s=2\",{\"1\":{\"44\":1}}],[\"alexnet\",{\"0\":{\"31\":1}}],[\"algorithms\",{\"1\":{\"58\":1,\"109\":1}}],[\"algorithm\",{\"0\":{\"14\":1},\"1\":{\"198\":1}}],[\"al\",{\"1\":{\"2\":1,\"6\":1,\"7\":1,\"9\":1,\"10\":2,\"12\":1,\"18\":1,\"19\":1,\"21\":1,\"22\":2,\"23\":2,\"24\":2,\"25\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2,\"70\":2,\"71\":2,\"73\":1,\"75\":1,\"76\":1,\"77\":1,\"78\":2,\"79\":1,\"107\":1,\"114\":1,\"117\":1,\"118\":1,\"128\":2,\"129\":1,\"132\":1,\"133\":5,\"134\":1,\"136\":2,\"137\":1,\"138\":1,\"165\":1,\"169\":1,\"171\":1,\"179\":3,\"180\":1}}]],\"serializationVersion\":2}";